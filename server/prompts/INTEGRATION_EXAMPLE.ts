/**
 * INTEGRATION EXAMPLE - How to use the prompt caching system
 *
 * This file shows the before/after of integrating the new caching system.
 * DO NOT import this file - it's just for reference.
 */

// ============================================================================
// BEFORE (OLD CODE) - From chatService.ts
// ============================================================================

/*
const PROMPT_PREFIX = `You are Kull's sales assistant, following a specific conversational sales script...`;
const PROMPT_SUFFIX = `</GITHUB_SOURCE_CODE>...`;

export async function getChatResponseStream(
  userMessage: string,
  history: ChatMessage[],
  // ... other params
): Promise<ReadableStream> {
  // Fetch repo content on EVERY request (slow, not cached)
  const repoContent = await fetchRepoContent();

  // Build static instructions by concatenating
  const staticInstructions = `${PROMPT_PREFIX}\n\n${repoContent}\n\n${PROMPT_SUFFIX}`;

  // Build messages
  const messages = [
    {
      role: 'system',
      content: staticInstructions,
    },
    // ... rest of messages
  ];

  // Call OpenAI
  const response = await openai.responses.create({ ... });
  return response;
}
*/

// ============================================================================
// AFTER (NEW CODE) - With prompt caching
// ============================================================================

import { MASTER_SALES_PROMPT } from './staticContent';
import { getStaticKnowledgeBase } from '../knowledge/repoCache';

export async function getChatResponseStream(
  userMessage: string,
  history: ChatMessage[],
  // ... other params
): Promise<ReadableStream> {
  const startTime = Date.now();

  // ========================================
  // Layer 1: Static sales prompt (CACHED)
  // ========================================

  // Get the master sales prompt and replace dynamic placeholders
  const salesPrompt = MASTER_SALES_PROMPT
    .replace('{{QUESTIONS_ALREADY_ASKED}}', questionListText)
    .replace('{{CURRENT_STEP}}', String(step))
    .replace('{{EXPECTED_NEXT_QUESTION}}', expectedNextQuestion);

  // ========================================
  // Layer 2: Static knowledge base (CACHED)
  // ========================================

  // Get knowledge base from cache (instant after first load)
  const knowledgeBase = await getStaticKnowledgeBase();

  // Combine layers 1 + 2 (both cacheable by OpenAI)
  const staticInstructions = `${salesPrompt}\n\n${knowledgeBase}`;

  const loadTime = Date.now() - startTime;
  console.log(`[Chat] Static content loaded in ${loadTime}ms`);
  statusCallback?.(`âœ… static content loaded (${Math.round(staticInstructions.length / 1000)}k chars)`, loadTime);

  // ========================================
  // Layer 3: Dynamic user context (NOT CACHED)
  // ========================================

  let dynamicContext = '';

  // User activity (changes per request)
  if (userActivityMarkdown) {
    dynamicContext += `\n\n## ðŸŽ¯ User Activity Context\n${userActivityMarkdown}`;
  }

  // Page visits (changes per request)
  if (pageVisits && pageVisits.length > 0) {
    dynamicContext += `\n\n## ðŸ§­ Page Visit History\n${JSON.stringify(pageVisits, null, 2)}`;
  }

  // Previous chat sessions (changes per user)
  if (allSessions && allSessions.length > 0) {
    dynamicContext += `\n\n## ðŸ’¬ Previous Chat Sessions\n`;
    allSessions.forEach((session, idx) => {
      dynamicContext += `\n### Session ${idx + 1}: ${session.title}\n`;
      // ... format messages
    });
  }

  // Validation feedback (changes per request)
  if (validationFeedback) {
    dynamicContext += `\n\n## âš ï¸ CRITICAL VALIDATION FEEDBACK\n\n${validationFeedback}`;
  }

  // ========================================
  // Build messages: Static first, then dynamic
  // ========================================

  const messages = [
    {
      role: 'system',
      content: staticInstructions, // CACHED: ~150k tokens (sales prompt + knowledge base)
    },
    // Add dynamic context as separate system message if present
    ...(dynamicContext ? [{
      role: 'system' as const,
      content: dynamicContext, // NOT CACHED: user-specific data (~5-10k tokens)
    }] : []),
    // Conversation history (changes per request)
    ...history.map(msg => ({
      role: msg.role === 'system' ? 'user' as const : msg.role,
      content: msg.content,
    })),
    {
      role: 'user' as const,
      content: userMessage,
    },
  ];

  // ========================================
  // Call OpenAI (now with prompt caching)
  // ========================================

  console.log(`[Chat] Sending ${Math.round(staticInstructions.length / 1000)}k chars (cached) + ${Math.round(dynamicContext.length / 1000)}k chars (dynamic)`);

  const response = await openai.responses.create({
    model,
    input: messages,
    // ... other params
  });

  return response;
}

// ============================================================================
// Server Startup - Initialize cache ONCE
// ============================================================================

// In server/index.ts:
/*
import { initializeKnowledgeBase } from './knowledge/repoCache';

async function startServer() {
  // ... existing setup ...

  // Initialize knowledge base cache (loads GitHub repo once)
  console.log('[Startup] Initializing knowledge base...');
  await initializeKnowledgeBase();
  console.log('[Startup] Knowledge base ready');

  // Start listening
  app.listen(port, () => {
    console.log(`Server running on port ${port}`);
  });
}

startServer();
*/

// ============================================================================
// Expected Performance
// ============================================================================

/*
BEFORE (no caching):
- GitHub API call: ~1000-2000ms per request
- Prompt tokens: ~150k every time
- Cost per conversation: $0.50
- First request: 2000ms
- Subsequent requests: 2000ms

AFTER (with caching):
- GitHub API call: ~1500ms once at startup, then 0ms
- Prompt tokens: ~150k first request, then ~10-15k (only dynamic context)
- Cost per conversation: $0.05-0.15 (70-90% cheaper)
- First request: 1500ms (no GitHub call)
- Subsequent requests: 50ms (cache hit)

Performance improvement: 40x faster, 10x cheaper
*/

// ============================================================================
// Cache Management
// ============================================================================

/*
// Check cache status
import { getCacheStatus } from './knowledge/repoCache';
const status = getCacheStatus();
console.log('Cache:', status);
// { isCached: true, sizeKB: 245, timestamp: '2025-11-20T02:12:00.000Z' }

// Force cache refresh (if needed)
import { invalidateKnowledgeBase } from './knowledge/repoCache';
invalidateKnowledgeBase();
// Next request will rebuild the cache

// Pre-warm cache at startup
import { initializeKnowledgeBase } from './knowledge/repoCache';
await initializeKnowledgeBase();
// Cache is now ready for first request
*/

export {}; // Make this a module
