Rate Limits - GroqDocs @keyframes go2264125279{from{transform:scale(0) rotate(45deg);opacity:0;}to{transform:scale(1) rotate(45deg);opacity:1;}}@keyframes go3020080000{from{transform:scale(0);opacity:0;}to{transform:scale(1);opacity:1;}}@keyframes go463499852{from{transform:scale(0) rotate(90deg);opacity:0;}to{transform:scale(1) rotate(90deg);opacity:1;}}@keyframes go1268368563{from{transform:rotate(0deg);}to{transform:rotate(360deg);}}@keyframes go1310225428{from{transform:scale(0) rotate(45deg);opacity:0;}to{transform:scale(1) rotate(45deg);opacity:1;}}@keyframes go651618207{0%{height:0;width:0;opacity:0;}40%{height:0;width:6px;opacity:1;}100%{opacity:1;height:10px;}}@keyframes go901347462{from{transform:scale(0.6);opacity:0.4;}to{transform:scale(1);opacity:1;}}.go4109123758{z-index:9999;}.go4109123758 > \*{pointer-events:auto;}:where(html\[dir="ltr"\]),:where(\[data-sonner-toaster\]\[dir="ltr"\]){--toast-icon-margin-start: -3px;--toast-icon-margin-end: 4px;--toast-svg-margin-start: -1px;--toast-svg-margin-end: 0px;--toast-button-margin-start: auto;--toast-button-margin-end: 0;--toast-close-button-start: 0;--toast-close-button-end: unset;--toast-close-button-transform: translate(-35%, -35%)}:where(html\[dir="rtl"\]),:where(\[data-sonner-toaster\]\[dir="rtl"\]){--toast-icon-margin-start: 4px;--toast-icon-margin-end: -3px;--toast-svg-margin-start: 0px;--toast-svg-margin-end: -1px;--toast-button-margin-start: 0;--toast-button-margin-end: auto;--toast-close-button-start: unset;--toast-close-button-end: 0;--toast-close-button-transform: translate(35%, -35%)}:where(\[data-sonner-toaster\]){position:fixed;width:var(--width);font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;--gray1: hsl(0, 0%, 99%);--gray2: hsl(0, 0%, 97.3%);--gray3: hsl(0, 0%, 95.1%);--gray4: hsl(0, 0%, 93%);--gray5: hsl(0, 0%, 90.9%);--gray6: hsl(0, 0%, 88.7%);--gray7: hsl(0, 0%, 85.8%);--gray8: hsl(0, 0%, 78%);--gray9: hsl(0, 0%, 56.1%);--gray10: hsl(0, 0%, 52.3%);--gray11: hsl(0, 0%, 43.5%);--gray12: hsl(0, 0%, 9%);--border-radius: 8px;box-sizing:border-box;padding:0;margin:0;list-style:none;outline:none;z-index:999999999;transition:transform .4s ease}:where(\[data-sonner-toaster\]\[data-lifted="true"\]){transform:translateY(-10px)}@media (hover: none) and (pointer: coarse){:where(\[data-sonner-toaster\]\[data-lifted="true"\]){transform:none}}:where(\[data-sonner-toaster\]\[data-x-position="right"\]){right:var(--offset-right)}:where(\[data-sonner-toaster\]\[data-x-position="left"\]){left:var(--offset-left)}:where(\[data-sonner-toaster\]\[data-x-position="center"\]){left:50%;transform:translate(-50%)}:where(\[data-sonner-toaster\]\[data-y-position="top"\]){top:var(--offset-top)}:where(\[data-sonner-toaster\]\[data-y-position="bottom"\]){bottom:var(--offset-bottom)}:where(\[data-sonner-toast\]){--y: translateY(100%);--lift-amount: calc(var(--lift) \* var(--gap));z-index:var(--z-index);position:absolute;opacity:0;transform:var(--y);filter:blur(0);touch-action:none;transition:transform .4s,opacity .4s,height .4s,box-shadow .2s;box-sizing:border-box;outline:none;overflow-wrap:anywhere}:where(\[data-sonner-toast\]\[data-styled="true"\]){padding:16px;background:var(--normal-bg);border:1px solid var(--normal-border);color:var(--normal-text);border-radius:var(--border-radius);box-shadow:0 4px 12px #0000001a;width:var(--width);font-size:13px;display:flex;align-items:center;gap:6px}:where(\[data-sonner-toast\]:focus-visible){box-shadow:0 4px 12px #0000001a,0 0 0 2px #0003}:where(\[data-sonner-toast\]\[data-y-position="top"\]){top:0;--y: translateY(-100%);--lift: 1;--lift-amount: calc(1 \* var(--gap))}:where(\[data-sonner-toast\]\[data-y-position="bottom"\]){bottom:0;--y: translateY(100%);--lift: -1;--lift-amount: calc(var(--lift) \* var(--gap))}:where(\[data-sonner-toast\]) :where(\[data-description\]){font-weight:400;line-height:1.4;color:inherit}:where(\[data-sonner-toast\]) :where(\[data-title\]){font-weight:500;line-height:1.5;color:inherit}:where(\[data-sonner-toast\]) :where(\[data-icon\]){display:flex;height:16px;width:16px;position:relative;justify-content:flex-start;align-items:center;flex-shrink:0;margin-left:var(--toast-icon-margin-start);margin-right:var(--toast-icon-margin-end)}:where(\[data-sonner-toast\]\[data-promise="true"\]) :where(\[data-icon\])>svg{opacity:0;transform:scale(.8);transform-origin:center;animation:sonner-fade-in .3s ease forwards}:where(\[data-sonner-toast\]) :where(\[data-icon\])>\*{flex-shrink:0}:where(\[data-sonner-toast\]) :where(\[data-icon\]) svg{margin-left:var(--toast-svg-margin-start);margin-right:var(--toast-svg-margin-end)}:where(\[data-sonner-toast\]) :where(\[data-content\]){display:flex;flex-direction:column;gap:2px}\[data-sonner-toast\]\[data-styled=true\] \[data-button\]{border-radius:4px;padding-left:8px;padding-right:8px;height:24px;font-size:12px;color:var(--normal-bg);background:var(--normal-text);margin-left:var(--toast-button-margin-start);margin-right:var(--toast-button-margin-end);border:none;cursor:pointer;outline:none;display:flex;align-items:center;flex-shrink:0;transition:opacity .4s,box-shadow .2s}:where(\[data-sonner-toast\]) :where(\[data-button\]):focus-visible{box-shadow:0 0 0 2px #0006}:where(\[data-sonner-toast\]) :where(\[data-button\]):first-of-type{margin-left:var(--toast-button-margin-start);margin-right:var(--toast-button-margin-end)}:where(\[data-sonner-toast\]) :where(\[data-cancel\]){color:var(--normal-text);background:rgba(0,0,0,.08)}:where(\[data-sonner-toast\]\[data-theme="dark"\]) :where(\[data-cancel\]){background:rgba(255,255,255,.3)}:where(\[data-sonner-toast\]) :where(\[data-close-button\]){position:absolute;left:var(--toast-close-button-start);right:var(--toast-close-button-end);top:0;height:20px;width:20px;display:flex;justify-content:center;align-items:center;padding:0;color:var(--gray12);border:1px solid var(--gray4);transform:var(--toast-close-button-transform);border-radius:50%;cursor:pointer;z-index:1;transition:opacity .1s,background .2s,border-color .2s}\[data-sonner-toast\] \[data-close-button\]{background:var(--gray1)}:where(\[data-sonner-toast\]) :where(\[data-close-button\]):focus-visible{box-shadow:0 4px 12px #0000001a,0 0 0 2px #0003}:where(\[data-sonner-toast\]) :where(\[data-disabled="true"\]){cursor:not-allowed}:where(\[data-sonner-toast\]):hover :where(\[data-close-button\]):hover{background:var(--gray2);border-color:var(--gray5)}:where(\[data-sonner-toast\]\[data-swiping="true"\]):before{content:"";position:absolute;left:-50%;right:-50%;height:100%;z-index:-1}:where(\[data-sonner-toast\]\[data-y-position="top"\]\[data-swiping="true"\]):before{bottom:50%;transform:scaleY(3) translateY(50%)}:where(\[data-sonner-toast\]\[data-y-position="bottom"\]\[data-swiping="true"\]):before{top:50%;transform:scaleY(3) translateY(-50%)}:where(\[data-sonner-toast\]\[data-swiping="false"\]\[data-removed="true"\]):before{content:"";position:absolute;inset:0;transform:scaleY(2)}:where(\[data-sonner-toast\]):after{content:"";position:absolute;left:0;height:calc(var(--gap) + 1px);bottom:100%;width:100%}:where(\[data-sonner-toast\]\[data-mounted="true"\]){--y: translateY(0);opacity:1}:where(\[data-sonner-toast\]\[data-expanded="false"\]\[data-front="false"\]){--scale: var(--toasts-before) \* .05 + 1;--y: translateY(calc(var(--lift-amount) \* var(--toasts-before))) scale(calc(-1 \* var(--scale)));height:var(--front-toast-height)}:where(\[data-sonner-toast\])>\*{transition:opacity .4s}:where(\[data-sonner-toast\]\[data-expanded="false"\]\[data-front="false"\]\[data-styled="true"\])>\*{opacity:0}:where(\[data-sonner-toast\]\[data-visible="false"\]){opacity:0;pointer-events:none}:where(\[data-sonner-toast\]\[data-mounted="true"\]\[data-expanded="true"\]){--y: translateY(calc(var(--lift) \* var(--offset)));height:var(--initial-height)}:where(\[data-sonner-toast\]\[data-removed="true"\]\[data-front="true"\]\[data-swipe-out="false"\]){--y: translateY(calc(var(--lift) \* -100%));opacity:0}:where(\[data-sonner-toast\]\[data-removed="true"\]\[data-front="false"\]\[data-swipe-out="false"\]\[data-expanded="true"\]){--y: translateY(calc(var(--lift) \* var(--offset) + var(--lift) \* -100%));opacity:0}:where(\[data-sonner-toast\]\[data-removed="true"\]\[data-front="false"\]\[data-swipe-out="false"\]\[data-expanded="false"\]){--y: translateY(40%);opacity:0;transition:transform .5s,opacity .2s}:where(\[data-sonner-toast\]\[data-removed="true"\]\[data-front="false"\]):before{height:calc(var(--initial-height) + 20%)}\[data-sonner-toast\]\[data-swiping=true\]{transform:var(--y) translateY(var(--swipe-amount-y, 0px)) translate(var(--swipe-amount-x, 0px));transition:none}\[data-sonner-toast\]\[data-swiped=true\]{user-select:none}\[data-sonner-toast\]\[data-swipe-out=true\]\[data-y-position=bottom\],\[data-sonner-toast\]\[data-swipe-out=true\]\[data-y-position=top\]{animation-duration:.2s;animation-timing-function:ease-out;animation-fill-mode:forwards}\[data-sonner-toast\]\[data-swipe-out=true\]\[data-swipe-direction=left\]{animation-name:swipe-out-left}\[data-sonner-toast\]\[data-swipe-out=true\]\[data-swipe-direction=right\]{animation-name:swipe-out-right}\[data-sonner-toast\]\[data-swipe-out=true\]\[data-swipe-direction=up\]{animation-name:swipe-out-up}\[data-sonner-toast\]\[data-swipe-out=true\]\[data-swipe-direction=down\]{animation-name:swipe-out-down}@keyframes swipe-out-left{0%{transform:var(--y) translate(var(--swipe-amount-x));opacity:1}to{transform:var(--y) translate(calc(var(--swipe-amount-x) - 100%));opacity:0}}@keyframes swipe-out-right{0%{transform:var(--y) translate(var(--swipe-amount-x));opacity:1}to{transform:var(--y) translate(calc(var(--swipe-amount-x) + 100%));opacity:0}}@keyframes swipe-out-up{0%{transform:var(--y) translateY(var(--swipe-amount-y));opacity:1}to{transform:var(--y) translateY(calc(var(--swipe-amount-y) - 100%));opacity:0}}@keyframes swipe-out-down{0%{transform:var(--y) translateY(var(--swipe-amount-y));opacity:1}to{transform:var(--y) translateY(calc(var(--swipe-amount-y) + 100%));opacity:0}}@media (max-width: 600px){\[data-sonner-toaster\]{position:fixed;right:var(--mobile-offset-right);left:var(--mobile-offset-left);width:100%}\[data-sonner-toaster\]\[dir=rtl\]{left:calc(var(--mobile-offset-left) \* -1)}\[data-sonner-toaster\] \[data-sonner-toast\]{left:0;right:0;width:calc(100% - var(--mobile-offset-left) \* 2)}\[data-sonner-toaster\]\[data-x-position=left\]{left:var(--mobile-offset-left)}\[data-sonner-toaster\]\[data-y-position=bottom\]{bottom:var(--mobile-offset-bottom)}\[data-sonner-toaster\]\[data-y-position=top\]{top:var(--mobile-offset-top)}\[data-sonner-toaster\]\[data-x-position=center\]{left:var(--mobile-offset-left);right:var(--mobile-offset-right);transform:none}}\[data-sonner-toaster\]\[data-theme=light\]{--normal-bg: #fff;--normal-border: var(--gray4);--normal-text: var(--gray12);--success-bg: hsl(143, 85%, 96%);--success-border: hsl(145, 92%, 91%);--success-text: hsl(140, 100%, 27%);--info-bg: hsl(208, 100%, 97%);--info-border: hsl(221, 91%, 91%);--info-text: hsl(210, 92%, 45%);--warning-bg: hsl(49, 100%, 97%);--warning-border: hsl(49, 91%, 91%);--warning-text: hsl(31, 92%, 45%);--error-bg: hsl(359, 100%, 97%);--error-border: hsl(359, 100%, 94%);--error-text: hsl(360, 100%, 45%)}\[data-sonner-toaster\]\[data-theme=light\] \[data-sonner-toast\]\[data-invert=true\]{--normal-bg: #000;--normal-border: hsl(0, 0%, 20%);--normal-text: var(--gray1)}\[data-sonner-toaster\]\[data-theme=dark\] \[data-sonner-toast\]\[data-invert=true\]{--normal-bg: #fff;--normal-border: var(--gray3);--normal-text: var(--gray12)}\[data-sonner-toaster\]\[data-theme=dark\]{--normal-bg: #000;--normal-bg-hover: hsl(0, 0%, 12%);--normal-border: hsl(0, 0%, 20%);--normal-border-hover: hsl(0, 0%, 25%);--normal-text: var(--gray1);--success-bg: hsl(150, 100%, 6%);--success-border: hsl(147, 100%, 12%);--success-text: hsl(150, 86%, 65%);--info-bg: hsl(215, 100%, 6%);--info-border: hsl(223, 100%, 12%);--info-text: hsl(216, 87%, 65%);--warning-bg: hsl(64, 100%, 6%);--warning-border: hsl(60, 100%, 12%);--warning-text: hsl(46, 87%, 65%);--error-bg: hsl(358, 76%, 10%);--error-border: hsl(357, 89%, 16%);--error-text: hsl(358, 100%, 81%)}\[data-sonner-toaster\]\[data-theme=dark\] \[data-sonner-toast\] \[data-close-button\]{background:var(--normal-bg);border-color:var(--normal-border);color:var(--normal-text)}\[data-sonner-toaster\]\[data-theme=dark\] \[data-sonner-toast\] \[data-close-button\]:hover{background:var(--normal-bg-hover);border-color:var(--normal-border-hover)}\[data-rich-colors=true\]\[data-sonner-toast\]\[data-type=success\],\[data-rich-colors=true\]\[data-sonner-toast\]\[data-type=success\] \[data-close-button\]{background:var(--success-bg);border-color:var(--success-border);color:var(--success-text)}\[data-rich-colors=true\]\[data-sonner-toast\]\[data-type=info\],\[data-rich-colors=true\]\[data-sonner-toast\]\[data-type=info\] \[data-close-button\]{background:var(--info-bg);border-color:var(--info-border);color:var(--info-text)}\[data-rich-colors=true\]\[data-sonner-toast\]\[data-type=warning\],\[data-rich-colors=true\]\[data-sonner-toast\]\[data-type=warning\] \[data-close-button\]{background:var(--warning-bg);border-color:var(--warning-border);color:var(--warning-text)}\[data-rich-colors=true\]\[data-sonner-toast\]\[data-type=error\],\[data-rich-colors=true\]\[data-sonner-toast\]\[data-type=error\] \[data-close-button\]{background:var(--error-bg);border-color:var(--error-border);color:var(--error-text)}.sonner-loading-wrapper{--size: 16px;height:var(--size);width:var(--size);position:absolute;inset:0;z-index:10}.sonner-loading-wrapper\[data-visible=false\]{transform-origin:center;animation:sonner-fade-out .2s ease forwards}.sonner-spinner{position:relative;top:50%;left:50%;height:var(--size);width:var(--size)}.sonner-loading-bar{animation:sonner-spin 1.2s linear infinite;background:var(--gray11);border-radius:6px;height:8%;left:-10%;position:absolute;top:-3.9%;width:24%}.sonner-loading-bar:nth-child(1){animation-delay:-1.2s;transform:rotate(.0001deg) translate(146%)}.sonner-loading-bar:nth-child(2){animation-delay:-1.1s;transform:rotate(30deg) translate(146%)}.sonner-loading-bar:nth-child(3){animation-delay:-1s;transform:rotate(60deg) translate(146%)}.sonner-loading-bar:nth-child(4){animation-delay:-.9s;transform:rotate(90deg) translate(146%)}.sonner-loading-bar:nth-child(5){animation-delay:-.8s;transform:rotate(120deg) translate(146%)}.sonner-loading-bar:nth-child(6){animation-delay:-.7s;transform:rotate(150deg) translate(146%)}.sonner-loading-bar:nth-child(7){animation-delay:-.6s;transform:rotate(180deg) translate(146%)}.sonner-loading-bar:nth-child(8){animation-delay:-.5s;transform:rotate(210deg) translate(146%)}.sonner-loading-bar:nth-child(9){animation-delay:-.4s;transform:rotate(240deg) translate(146%)}.sonner-loading-bar:nth-child(10){animation-delay:-.3s;transform:rotate(270deg) translate(146%)}.sonner-loading-bar:nth-child(11){animation-delay:-.2s;transform:rotate(300deg) translate(146%)}.sonner-loading-bar:nth-child(12){animation-delay:-.1s;transform:rotate(330deg) translate(146%)}@keyframes sonner-fade-in{0%{opacity:0;transform:scale(.8)}to{opacity:1;transform:scale(1)}}@keyframes sonner-fade-out{0%{opacity:1;transform:scale(1)}to{opacity:0;transform:scale(.8)}}@keyframes sonner-spin{0%{opacity:1}to{opacity:.15}}@media (prefers-reduced-motion){\[data-sonner-toast\],\[data-sonner-toast\]>\*,.sonner-loading-bar{transition:none!important;animation:none!important}}.sonner-loader{position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);transform-origin:center;transition:opacity .2s,transform .2s}.sonner-loader\[data-visible=false\]{opacity:0;transform:scale(.8) translate(-50%,-50%)} !function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()

[![Groq](/groq-logo.svg)](/home)

[Docs](/docs/overview)[Login](/home)

[Playground](/playground)

[API Keys](/keys)

[Dashboard](/dashboard)

[Docs](/docs)

[](/settings)

[Log In](/login)

## Documentation

[Docs](/docs/overview)[API Reference](/docs/api-reference)

Search⌘K

Rate Limits

## Docs

### Get Started

[Overview](/docs/overview)[Quickstart](/docs/quickstart)[OpenAI Compatibility](/docs/openai)[Responses API](/docs/responses-api)[Models](/docs/models)[Rate Limits](/docs/rate-limits)[Examples](/docs/examples)

### Features

[Text Generation](/docs/text-chat)[Speech to Text](/docs/speech-to-text)[Text to Speech](/docs/text-to-speech)[Images and Vision](/docs/vision)[Reasoning](/docs/reasoning)[Structured Outputs](/docs/structured-outputs)

### Built-In Tools

[Web Search](/docs/web-search)[Browser Search](/docs/browser-search)[Visit Website](/docs/visit-website)[Browser Automation](/docs/browser-automation)[Code Execution](/docs/code-execution)[Wolfram Alpha](/docs/wolfram-alpha)

### Compound

[Overview](/docs/compound)[Systems](/docs/compound/systems)[Compound](/docs/compound/systems/compound)[Compound Mini](/docs/compound/systems/compound-mini)[Built-In Tools](/docs/compound/built-in-tools)[Use Cases](/docs/compound/use-cases)

### Advanced Features

[Batch Processing](/docs/batch)[Flex Processing](/docs/flex-processing)[Content Moderation](/docs/content-moderation)[Prefilling](/docs/prefilling)[Tool Use](/docs/tool-use)[Remote MCP](/docs/mcp)[LoRA Inference](/docs/lora)

### Prompting Guide

[Prompt Basics](/docs/prompting)[Prompt Patterns](/docs/prompting/patterns)[Model Migration](/docs/prompting/model-migration)[Prompt Caching](/docs/prompt-caching)

### Production Readiness

[Optimizing Latency](/docs/production-readiness/optimizing-latency)[Production Checklist](/docs/production-readiness/production-ready-checklist)[Security Onboarding](/docs/production-readiness/security-onboarding)

### Developer Resources

[Groq Libraries](/docs/libraries)[Groq Badge](/docs/badge)[Integrations Catalog](/docs/integrations)

### Console

[Spend Limits](/docs/spend-limits)[Projects](/docs/projects)[Model Permissions](/docs/model-permissions)[Billing FAQs](/docs/billing-faqs)[Your Data](/docs/your-data)

### Support & Guidelines

[Developer Community](https://community.groq.com)[OpenBench](https://openbench.dev)[Errors](/docs/errors)[Changelog](/docs/changelog)[Compound Systems](/docs/changelog/compound)[Policies & Notices](/docs/legal)

Search⌘K

[Docs](/docs/overview)[API Reference](/docs/api-reference)

### Get Started

[Overview](/docs/overview)

[Quickstart](/docs/quickstart)

[OpenAI Compatibility](/docs/openai)

[Responses API](/docs/responses-api)

[Models](/docs/models)

[Rate Limits](/docs/rate-limits)

[Examples](/docs/examples)

### Features

[Text Generation](/docs/text-chat)

[Speech to Text](/docs/speech-to-text)

[Text to Speech](/docs/text-to-speech)

[Images and Vision](/docs/vision)

[Reasoning](/docs/reasoning)

[Structured Outputs](/docs/structured-outputs)

### Built-In Tools

[Web Search](/docs/web-search)

[Browser Search](/docs/browser-search)

[Visit Website](/docs/visit-website)

[Browser Automation](/docs/browser-automation)

[Code Execution](/docs/code-execution)

[Wolfram Alpha](/docs/wolfram-alpha)

### Compound

[Overview](/docs/compound)

[Systems](/docs/compound/systems)

[Built-In Tools](/docs/compound/built-in-tools)

[Use Cases](/docs/compound/use-cases)

### Advanced Features

[Batch Processing](/docs/batch)

[Flex Processing](/docs/flex-processing)

[Content Moderation](/docs/content-moderation)

[Prefilling](/docs/prefilling)

[Tool Use](/docs/tool-use)

[Remote MCP](/docs/mcp)

[LoRA Inference](/docs/lora)

### Prompting Guide

[Prompt Basics](/docs/prompting)

[Prompt Patterns](/docs/prompting/patterns)

[Model Migration](/docs/prompting/model-migration)

[Prompt Caching](/docs/prompt-caching)

### Production Readiness

[Optimizing Latency](/docs/production-readiness/optimizing-latency)

[Production Checklist](/docs/production-readiness/production-ready-checklist)

[Security Onboarding](/docs/production-readiness/security-onboarding)

### Developer Resources

[Groq Libraries](/docs/libraries)

[Groq Badge](/docs/badge)

[Integrations Catalog](/docs/integrations)

### Console

[Spend Limits](/docs/spend-limits)

[Projects](/docs/projects)

[Model Permissions](/docs/model-permissions)

[Billing FAQs](/docs/billing-faqs)

[Your Data](/docs/your-data)

### Support & Guidelines

[Developer Community](https://community.groq.com)

[OpenBench](https://openbench.dev)

[Errors](/docs/errors)

[Changelog](/docs/changelog)

[Policies & Notices](/docs/legal)

# Rate Limits

Copy page

Rate limits act as control measures to regulate how frequently users and applications can access our API within specified timeframes. These limits help ensure service stability, fair access, and protection against misuse so that we can serve reliable and fast inference for all.

## [Understanding Rate Limits](#understanding-rate-limits)

Rate limits are measured in:

*   **RPM:** Requests per minute
*   **RPD:** Requests per day
*   **TPM:** Tokens per minute
*   **TPD:** Tokens per day
*   **ASH:** Audio seconds per hour
*   **ASD:** Audio seconds per day

[Cached tokens](/docs/prompt-caching) do not count towards your rate limits.

Rate limits apply at the organization level, not individual users. You can hit any limit type depending on which threshold you reach first.

**Example:** Let's say your RPM = 50 and your TPM = 200K. If you were to send 50 requests with only 100 tokens within a minute, you would reach your limit even though you did not send 200K tokens within those 50 requests.

## [Rate Limits](#rate-limits)

The following is a high level summary and there may be exceptions to these limits. You can view the current, exact rate limits for your organization on the [limits page](/settings/limits) in your account settings.

FreeDeveloper

MODEL ID

RPM

RPD

TPM

TPD

ASH

ASD

allam-2-7b

30

7K

6K

500K

\-

\-

groq/compound

30

250

70K

\-

\-

\-

groq/compound-mini

30

250

70K

\-

\-

\-

llama-3.1-8b-instant

30

14.4K

6K

500K

\-

\-

llama-3.3-70b-versatile

30

1K

12K

100K

\-

\-

meta-llama/llama-4-maverick-17b-128e-instruct

30

1K

6K

500K

\-

\-

meta-llama/llama-4-scout-17b-16e-instruct

30

1K

30K

500K

\-

\-

meta-llama/llama-guard-4-12b

30

14.4K

15K

500K

\-

\-

meta-llama/llama-prompt-guard-2-22m

30

14.4K

15K

500K

\-

\-

meta-llama/llama-prompt-guard-2-86m

30

14.4K

15K

500K

\-

\-

moonshotai/kimi-k2-instruct

60

1K

10K

300K

\-

\-

moonshotai/kimi-k2-instruct-0905

60

1K

10K

300K

\-

\-

openai/gpt-oss-120b

30

1K

8K

200K

\-

\-

openai/gpt-oss-20b

30

1K

8K

200K

\-

\-

openai/gpt-oss-safeguard-20b

30

1K

8K

200K

\-

\-

playai-tts

10

100

1.2K

3.6K

\-

\-

playai-tts-arabic

10

100

1.2K

3.6K

\-

\-

qwen/qwen3-32b

60

1K

6K

500K

\-

\-

whisper-large-v3

20

2K

\-

\-

7.2K

28.8K

whisper-large-v3-turbo

20

2K

\-

\-

7.2K

28.8K

MODEL ID

RPM

RPD

TPM

TPD

ASH

ASD

allam-2-7b

300

60K

60K

\-

\-

\-

groq/compound

200

20K

200K

\-

\-

\-

groq/compound-mini

200

20K

200K

\-

\-

\-

llama-3.1-8b-instant

1K

500K

250K

\-

\-

\-

llama-3.3-70b-versatile

1K

500K

300K

\-

\-

\-

meta-llama/llama-4-maverick-17b-128e-instruct

1K

500K

300K

\-

\-

\-

meta-llama/llama-4-scout-17b-16e-instruct

1K

500K

300K

\-

\-

\-

meta-llama/llama-guard-4-12b

100

50K

30K

1M

\-

\-

meta-llama/llama-prompt-guard-2-22m

100

50K

30K

\-

\-

\-

meta-llama/llama-prompt-guard-2-86m

100

50K

30K

\-

\-

\-

moonshotai/kimi-k2-instruct

1K

500K

250K

\-

\-

\-

moonshotai/kimi-k2-instruct-0905

1K

500K

250K

\-

\-

\-

openai/gpt-oss-120b

1K

500K

250K

\-

\-

\-

openai/gpt-oss-20b

1K

500K

250K

\-

\-

\-

openai/gpt-oss-safeguard-20b

1K

500K

150K

\-

\-

\-

playai-tts

250

100K

50K

2M

\-

\-

playai-tts-arabic

250

100K

50K

2M

\-

\-

qwen/qwen3-32b

1K

500K

300K

\-

\-

\-

whisper-large-v3

300

200K

\-

\-

200K

4M

whisper-large-v3-turbo

400

200K

\-

\-

400K

4M

## [Rate Limit Headers](#rate-limit-headers)

In addition to viewing your limits on your account's [limits](https://console.groq.com/settings/limits) page, you can also view rate limit information such as remaining requests and tokens in HTTP response headers as follows:

The following headers are set (values are illustrative):

Header

Value

Notes

retry-after

2

In seconds

x-ratelimit-limit-requests

14400

Always refers to Requests Per Day (RPD)

x-ratelimit-limit-tokens

18000

Always refers to Tokens Per Minute (TPM)

x-ratelimit-remaining-requests

14370

Always refers to Requests Per Day (RPD)

x-ratelimit-remaining-tokens

17997

Always refers to Tokens Per Minute (TPM)

x-ratelimit-reset-requests

2m59.56s

Always refers to Requests Per Day (RPD)

x-ratelimit-reset-tokens

7.66s

Always refers to Tokens Per Minute (TPM)

## [Handling Rate Limits](#handling-rate-limits)

When you exceed rate limits, our API returns a `429 Too Many Requests` HTTP status code.

**Note**: `retry-after` is only set if you hit the rate limit and status code 429 is returned. The other headers are always included.

## [Need Higher Rate Limits?](#need-higher-rate-limits)

If you need higher rate limits, you can [request them here](https://groq.com/self-serve-support).

### Was this page helpful?

YesNoSuggest Edits

#### On this page

*   [Understanding Rate Limits](#understanding-rate-limits)
*   [Rate Limits](#rate-limits)
*   [Rate Limit Headers](#rate-limit-headers)
*   [Handling Rate Limits](#handling-rate-limits)
*   [Need Higher Rate Limits?](#need-higher-rate-limits)

(self.\_\_next\_f=self.\_\_next\_f||\[\]).push(\[0\])self.\_\_next\_f.push(\[1,"1:\\"$Sreact.fragment\\"\\n3:I\[53692,\[\],\\"ClientSegmentRoot\\"\]\\n4:I\[30352,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"1874\\",\\"static/chunks/1874-1af85b5fe730b1d1.js\\",\\"6603\\",\\"static/chunks/6603-5a9bea7a37afd382.js\\",\\"5193\\",\\"static/chunks/5193-5a50519070c43274.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"4215\\",\\"static/chunks/4215-210105fc188a4582.js\\",\\"8285\\",\\"static/chunks/8285-cf692dbe291dd0a9.js\\",\\"5674\\",\\"static/chunks/5674-883f5bd3e8483206.js\\",\\"3528\\",\\"static/chunks/3528-5e66c33acdac1843.js\\",\\"5789\\",\\"static/chunks/5789-d4359a1055e0aacd.js\\",\\"7860\\",\\"static/chunks/7860-a6cd4bd13b0a2513.js\\",\\"4209\\",\\"static/chunks/4209-19ded76058a94842.js\\",\\"9558\\",\\"static/chunks/9558-076fb64cebb3d532.js\\",\\"9870\\",\\"static/chunks/app/(console)/layout-77f984215e0a0987.js\\"\],\\"default\\"\]\\n5:I\[17421,\[\],\\"\\"\]\\n6:I\[47289,\[\],\\"\\"\]\\n9:I\[27666,\[\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"3148\\",\\"static/chunks/app/(console)/docs/(mdx-pages)/layout-11e495940bad72c6.js\\"\],\\"FeedbackCollector\\"\]\\na:I\[47864,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"1874\\",\\"static/chunks/1874-1af85b5fe730b1d1.js\\",\\"3354\\",\\"static/chunks/3354-1c66ad7116e53618.js\\",\\"6603\\",\\"static/chunks/6603-5a9bea7a37afd382.js\\",\\"1103\\",\\"static/chunks/1103-6ee8bf0e353f128c.js\\",\\"7764\\",\\"static/chunks/7764-9237aa9acd440644.js\\",\\"5193\\",\\"static/chunks/5193-5a50519070c43274.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"4215\\",\\"static/chunks/4215-210105fc188a4582.js\\",\\"5789\\",\\"static/chunks/5789-d4359a1055e0aacd.js\\",\\"4454\\",\\"static/chunks/4454-c3cbd02a8687a7b6.js\\",\\"7860\\",\\"static/ch"\])self.\_\_next\_f.push(\[1,"unks/7860-a6cd4bd13b0a2513.js\\",\\"1285\\",\\"static/chunks/app/(console)/docs/(mdx-pages)/rate-limits/page-b0be08a4163e38e4.js\\"\],\\"CopyAsMarkdownButton\\"\]\\nb:I\[26179,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"1874\\",\\"static/chunks/1874-1af85b5fe730b1d1.js\\",\\"3354\\",\\"static/chunks/3354-1c66ad7116e53618.js\\",\\"6603\\",\\"static/chunks/6603-5a9bea7a37afd382.js\\",\\"1103\\",\\"static/chunks/1103-6ee8bf0e353f128c.js\\",\\"7764\\",\\"static/chunks/7764-9237aa9acd440644.js\\",\\"5193\\",\\"static/chunks/5193-5a50519070c43274.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"4215\\",\\"static/chunks/4215-210105fc188a4582.js\\",\\"5789\\",\\"static/chunks/5789-d4359a1055e0aacd.js\\",\\"4454\\",\\"static/chunks/4454-c3cbd02a8687a7b6.js\\",\\"7860\\",\\"static/chunks/7860-a6cd4bd13b0a2513.js\\",\\"1285\\",\\"static/chunks/app/(console)/docs/(mdx-pages)/rate-limits/page-b0be08a4163e38e4.js\\"\],\\"Link\\"\]\\nd:I\[73783,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"1874\\",\\"static/chunks/1874-1af85b5fe730b1d1.js\\",\\"3354\\",\\"static/chunks/3354-1c66ad7116e53618.js\\",\\"6603\\",\\"static/chunks/6603-5a9bea7a37afd382.js\\",\\"1103\\",\\"static/chunks/1103-6ee8bf0e353f128c.js\\",\\"7764\\",\\"static/chunks/7764-9237aa9acd440644.js\\",\\"5193\\",\\"static/chunks/5193-5a50519070c43274.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"4215\\",\\"static/chunks/4215-210105fc188a4582.js\\",\\"5789\\",\\"static/chunks/5789-d4359a1055e0aacd.js\\",\\"4454\\",\\"static/chunks/4454-c3cbd02a8687a7b6.js\\",\\"7860\\",\\"static/chunks/7860-a6cd4bd13b0a2513.js\\",\\"1285\\",\\"static/chunks/app/(console)/docs/(mdx-pages)/rate-limits/page-b0be08a4163e38e4.js\\"\],\\"default\\"\]\\ne:I\[18195,\[\],\\"OutletBoundary\\"\]\\n11:I\[18195,\[\],\\"ViewportBoundary\\"\]\\n13:I\[18195,\[\],\\"MetadataBoundary\\"\]\\n15:I\[15065,\[\\"888\\",\\"static/chunks/888-"\])self.\_\_next\_f.push(\[1,"7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"4219\\",\\"static/chunks/app/global-error-008962f800b9c493.js\\"\],\\"default\\"\]\\n:HL\[\\"/\_next/static/media/17e5ee57c5ca5e5a-s.p.woff2\\",\\"font\\",{\\"crossOrigin\\":\\"\\",\\"type\\":\\"font/woff2\\"}\]\\n:HL\[\\"/\_next/static/media/36966cca54120369-s.p.woff2\\",\\"font\\",{\\"crossOrigin\\":\\"\\",\\"type\\":\\"font/woff2\\"}\]\\n:HL\[\\"/\_next/static/media/904be59b21bd51cb-s.p.woff2\\",\\"font\\",{\\"crossOrigin\\":\\"\\",\\"type\\":\\"font/woff2\\"}\]\\n:HL\[\\"/\_next/static/media/98e207f02528a563-s.p.woff2\\",\\"font\\",{\\"crossOrigin\\":\\"\\",\\"type\\":\\"font/woff2\\"}\]\\n:HL\[\\"/\_next/static/media/d3ebbfd689654d3a-s.p.woff2\\",\\"font\\",{\\"crossOrigin\\":\\"\\",\\"type\\":\\"font/woff2\\"}\]\\n:HL\[\\"/\_next/static/media/e4af272ccee01ff0-s.p.woff2\\",\\"font\\",{\\"crossOrigin\\":\\"\\",\\"type\\":\\"font/woff2\\"}\]\\n:HL\[\\"/\_next/static/media/f36144f235cd456e-s.p.woff2\\",\\"font\\",{\\"crossOrigin\\":\\"\\",\\"type\\":\\"font/woff2\\"}\]\\n:HL\[\\"/\_next/static/css/7bf466619b61d92d.css\\",\\"style\\"\]\\n:HL\[\\"/\_next/static/css/5f71e76d6445edb3.css\\",\\"style\\"\]\\n:HL\[\\"/\_next/static/css/c145a7ada77054e8.css\\",\\"style\\"\]\\n:HL\[\\"/\_next/static/css/0cec16494c374a65.css\\",\\"style\\"\]\\n:HL\[\\"/\_next/static/css/b1dbd2d25ec6d91f.css\\",\\"style\\"\]\\n"\])self.\_\_next\_f.push(\[1,"0:{\\"P\\":null,\\"b\\":\\"JaE-NAo4NUSHt0zfUAtYQ\\",\\"p\\":\\"\\",\\"c\\":\[\\"\\",\\"docs\\",\\"rate-limits\\"\],\\"i\\":false,\\"f\\":\[\[\[\\"\\",{\\"children\\":\[\\"(console)\\",{\\"children\\":\[\\"docs\\",{\\"children\\":\[\\"(mdx-pages)\\",{\\"children\\":\[\\"rate-limits\\",{\\"children\\":\[\\"\_\_PAGE\_\_\\",{}\]}\]}\]}\]}\]},\\"$undefined\\",\\"$undefined\\",true\],\[\\"\\",\[\\"$\\",\\"$1\\",\\"c\\",{\\"children\\":\[\[\[\\"$\\",\\"link\\",\\"0\\",{\\"rel\\":\\"stylesheet\\",\\"href\\":\\"/\_next/static/css/7bf466619b61d92d.css\\",\\"precedence\\":\\"next\\",\\"crossOrigin\\":\\"$undefined\\",\\"nonce\\":\\"$undefined\\"}\],\[\\"$\\",\\"link\\",\\"1\\",{\\"rel\\":\\"stylesheet\\",\\"href\\":\\"/\_next/static/css/5f71e76d6445edb3.css\\",\\"precedence\\":\\"next\\",\\"crossOrigin\\":\\"$undefined\\",\\"nonce\\":\\"$undefined\\"}\]\],\\"$L2\\"\]}\],{\\"children\\":\[\\"(console)\\",\[\\"$\\",\\"$1\\",\\"c\\",{\\"children\\":\[\[\[\\"$\\",\\"link\\",\\"0\\",{\\"rel\\":\\"stylesheet\\",\\"href\\":\\"/\_next/static/css/c145a7ada77054e8.css\\",\\"precedence\\":\\"next\\",\\"crossOrigin\\":\\"$undefined\\",\\"nonce\\":\\"$undefined\\"}\]\],\[\\"$\\",\\"$L3\\",null,{\\"Component\\":\\"$4\\",\\"slots\\":{\\"children\\":\[\\"$\\",\\"$L5\\",null,{\\"parallelRouterKey\\":\\"children\\",\\"error\\":\\"$undefined\\",\\"errorStyles\\":\\"$undefined\\",\\"errorScripts\\":\\"$undefined\\",\\"template\\":\[\\"$\\",\\"$L6\\",null,{}\],\\"templateStyles\\":\\"$undefined\\",\\"templateScripts\\":\\"$undefined\\",\\"notFound\\":\[\[\[\\"$\\",\\"title\\",null,{\\"children\\":\\"404: This page could not be found.\\"}\],\[\\"$\\",\\"div\\",null,{\\"style\\":{\\"fontFamily\\":\\"system-ui,\\\\\\"Segoe UI\\\\\\",Roboto,Helvetica,Arial,sans-serif,\\\\\\"Apple Color Emoji\\\\\\",\\\\\\"Segoe UI Emoji\\\\\\"\\",\\"height\\":\\"100vh\\",\\"textAlign\\":\\"center\\",\\"display\\":\\"flex\\",\\"flexDirection\\":\\"column\\",\\"alignItems\\":\\"center\\",\\"justifyContent\\":\\"center\\"},\\"children\\":\[\\"$\\",\\"div\\",null,{\\"children\\":\[\[\\"$\\",\\"style\\",null,{\\"dangerouslySetInnerHTML\\":{\\"\_\_html\\":\\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\\"}}\],\[\\"$\\",\\"h1\\",null,{\\"className\\":\\"next-error-h1\\",\\"style\\":{\\"display\\":\\"inline-block\\",\\"margin\\":\\"0 20px 0 0\\",\\"padding\\":\\"0 23px 0 0\\",\\"fontSize\\":24,\\"fontWeight\\":500,\\"verticalAlign\\":\\"top\\",\\"lineHeight\\":\\"49px\\"},\\"children\\":404}\],\[\\"$\\",\\"div\\",null,{\\"style\\":{\\"display\\":\\"inline-block\\"},\\"children\\":\[\\"$\\",\\"h2\\",null,{\\"style\\":{\\"fontSize\\":14,\\"fontWeight\\":400,\\"lineHeight\\":\\"49px\\",\\"margin\\":0},\\"children\\":\\"This page could not be found.\\"}\]}\]\]}\]}\]\],\[\]\],\\"forbidden\\":\\"$undefined\\",\\"unauthorized\\":\\"$undefined\\"}\]},\\"params\\":{},\\"promise\\":\\"$@7\\"}\]\]}\],{\\"children\\":\[\\"docs\\",\[\\"$\\",\\"$1\\",\\"c\\",{\\"children\\":\[\[\[\\"$\\",\\"link\\",\\"0\\",{\\"rel\\":\\"stylesheet\\",\\"href\\":\\"/\_next/static/css/0cec16494c374a65.css\\",\\"precedence\\":\\"next\\",\\"crossOrigin\\":\\"$undefined\\",\\"nonce\\":\\"$undefined\\"}\]\],\\"$L8\\"\]}\],{\\"children\\":\[\\"(mdx-pages)\\",\[\\"$\\",\\"$1\\",\\"c\\",{\\"children\\":\[null,\[\\"$\\",\\"div\\",null,{\\"children\\":\[\[\\"$\\",\\"$L5\\",null,{\\"parallelRouterKey\\":\\"children\\",\\"error\\":\\"$undefined\\",\\"errorStyles\\":\\"$undefined\\",\\"errorScripts\\":\\"$undefined\\",\\"template\\":\[\\"$\\",\\"$L6\\",null,{}\],\\"templateStyles\\":\\"$undefined\\",\\"templateScripts\\":\\"$undefined\\",\\"notFound\\":\\"$undefined\\",\\"forbidden\\":\\"$undefined\\",\\"unauthorized\\":\\"$undefined\\"}\],\[\\"$\\",\\"$L9\\",null,{}\]\]}\]\]}\],{\\"children\\":\[\\"rate-limits\\",\[\\"$\\",\\"$1\\",\\"c\\",{\\"children\\":\[null,\[\\"$\\",\\"$L5\\",null,{\\"parallelRouterKey\\":\\"children\\",\\"error\\":\\"$undefined\\",\\"errorStyles\\":\\"$undefined\\",\\"errorScripts\\":\\"$undefined\\",\\"template\\":\[\\"$\\",\\"$L6\\",null,{}\],\\"templateStyles\\":\\"$undefined\\",\\"templateScripts\\":\\"$undefined\\",\\"notFound\\":\\"$undefined\\",\\"forbidden\\":\\"$undefined\\",\\"unauthorized\\":\\"$undefined\\"}\]\]}\],{\\"children\\":\[\\"\_\_PAGE\_\_\\",\[\\"$\\",\\"$1\\",\\"c\\",{\\"children\\":\[\[\[\\"$\\",\\"div\\",null,{\\"className\\":\\"flex flex-col sm:flex-row sm:items-center gap-2 sm:gap-4 mb-3 justify-between\\",\\"children\\":\[\[\\"$\\",\\"h1\\",null,{\\"className\\":\\"text-2xl font-semibold font-header\\",\\"children\\":\\"Rate Limits\\"}\],\[\\"$\\",\\"$La\\",null,{}\]\]}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\\"Rate limits act as control measures to regulate how frequently users and applications can access our API within specified timeframes. These limits help ensure service stability, fair access, and protection\\\\nagainst misuse so that we can serve reliable and fast inference for all.\\"}\],\\"\\\\n\\",\[\\"$\\",\\"h2\\",null,{\\"id\\":\\"understanding-rate-limits\\",\\"className\\":\\"\[\\u0026:not(:first-child)\]:mt-12 mb-3 text-xl w-fit font-semibold font-header hover:underline\\",\\"children\\":\[\\"$\\",\\"a\\",null,{\\"href\\":\\"#understanding-rate-limits\\",\\"className\\":\\"anchor-link\\",\\"children\\":\[\\"Understanding Rate Limits\\"\]}\]}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\\"Rate limits are measured in:\\"}\],\\"\\\\n\\",\[\\"$\\",\\"ul\\",null,{\\"className\\":\\"list-disc my-1 text-sm\\",\\"children\\":\[\\"\\\\n\\",\[\\"$\\",\\"li\\",null,{\\"className\\":\\"ml-6 my-1.5 text-sm\\",\\"children\\":\[\[\\"$\\",\\"strong\\",null,{\\"children\\":\\"RPM:\\"}\],\\" Requests per minute\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"li\\",null,{\\"className\\":\\"ml-6 my-1.5 text-sm\\",\\"children\\":\[\[\\"$\\",\\"strong\\",null,{\\"children\\":\\"RPD:\\"}\],\\" Requests per day\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"li\\",null,{\\"className\\":\\"ml-6 my-1.5 text-sm\\",\\"children\\":\[\[\\"$\\",\\"strong\\",null,{\\"children\\":\\"TPM:\\"}\],\\" Tokens per minute\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"li\\",null,{\\"className\\":\\"ml-6 my-1.5 text-sm\\",\\"children\\":\[\[\\"$\\",\\"strong\\",null,{\\"children\\":\\"TPD:\\"}\],\\" Tokens per day\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"li\\",null,{\\"className\\":\\"ml-6 my-1.5 text-sm\\",\\"children\\":\[\[\\"$\\",\\"strong\\",null,{\\"children\\":\\"ASH:\\"}\],\\" Audio seconds per hour\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"li\\",null,{\\"className\\":\\"ml-6 my-1.5 text-sm\\",\\"children\\":\[\[\\"$\\",\\"strong\\",null,{\\"children\\":\\"ASD:\\"}\],\\" Audio seconds per day\\"\]}\],\\"\\\\n\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"div\\",null,{\\"ref\\":\\"$undefined\\",\\"role\\":\\"alert\\",\\"className\\":\\"relative w-full rounded-lg p-4 \[\\u0026\\u003esvg~\*\]:pl-7 \[\\u0026\\u003esvg\]:absolute \[\\u0026\\u003esvg\]:left-4 \[\\u0026\\u003esvg\]:top-4 \[\\u0026\\u003esvg\]:text-foreground bg-background-secondary text-sm px-3 pl-0 text-announcement-foreground\\",\\"children\\":\[\\"$\\",\\"div\\",null,{\\"className\\":\\"flex items-center space-x-2 p-2 border rounded-md bg-blue-100 border-blue-500 dark:bg-\[#0a2b4c\] dark:border-\[#14558d\]\\",\\"children\\":\[\[\\"$\\",\\"svg\\",null,{\\"ref\\":\\"$undefined\\",\\"xmlns\\":\\"http://www.w3.org/2000/svg\\",\\"width\\":16,\\"height\\":16,\\"viewBox\\":\\"0 0 24 24\\",\\"fill\\":\\"none\\",\\"stroke\\":\\"currentColor\\",\\"strokeWidth\\":2,\\"strokeLinecap\\":\\"round\\",\\"strokeLinejoin\\":\\"round\\",\\"className\\":\\"lucide lucide-info shrink-0 text-blue-500\\",\\"aria-hidden\\":\\"true\\",\\"children\\":\[\[\\"$\\",\\"circle\\",\\"1mglay\\",{\\"cx\\":\\"12\\",\\"cy\\":\\"12\\",\\"r\\":\\"10\\"}\],\[\\"$\\",\\"path\\",\\"1dtifu\\",{\\"d\\":\\"M12 16v-4\\"}\],\[\\"$\\",\\"path\\",\\"e9boi3\\",{\\"d\\":\\"M12 8h.01\\"}\],\\"$undefined\\"\]}\],\[\\"$\\",\\"div\\",null,{\\"className\\":\\"\\",\\"children\\":\[\[\\"$\\",\\"p\\",\\"p-0\\",{\\"children\\":\[\[\\"$\\",\\"$Lb\\",\\"a-0\\",{\\"href\\":\\"/docs/prompt-caching\\",\\"children\\":\\"Cached tokens\\"}\],\\" do not count towards your rate limits.\\"\]}\]\]}\]\]}\]}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\\"Rate limits apply at the organization level, not individual users. You can hit any limit type depending on which threshold you reach first.\\"}\],\\"\\\\n\\",\[\\"$\\",\\"div\\",null,{\\"className\\":\\"h-3\\"}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\[\[\\"$\\",\\"strong\\",null,{\\"children\\":\\"Example:\\"}\],\\" Let's say your RPM = 50 and your TPM = 200K. If you were to send 50 requests with only 100 tokens within a minute, you would reach your limit even though you did not send 200K tokens within those\\\\n50 requests.\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"h2\\",null,{\\"id\\":\\"rate-limits\\",\\"className\\":\\"\[\\u0026:not(:first-child)\]:mt-12 mb-3 text-xl w-fit font-semibold font-header hover:underline\\",\\"children\\":\[\\"$\\",\\"a\\",null,{\\"href\\":\\"#rate-limits\\",\\"className\\":\\"anchor-link\\",\\"children\\":\[\\"Rate Limits\\"\]}\]}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\[\\"The following is a high level summary and there may be exceptions to these limits. You can view the current, exact rate limits for your organization on the \\",\[\\"$\\",\\"$Lb\\",null,{\\"prefetch\\":true,\\"href\\":\\"/settings/limits\\",\\"children\\":\\"limits page\\"}\],\\" in your account settings.\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"div\\",null,{\\"className\\":\\"h-3\\"}\],\\"\\\\n\\",\\"$Lc\\",\\"\\\\n\\",\[\\"$\\",\\"h2\\",null,{\\"id\\":\\"rate-limit-headers\\",\\"className\\":\\"\[\\u0026:not(:first-child)\]:mt-12 mb-3 text-xl w-fit font-semibold font-header hover:underline\\",\\"children\\":\[\\"$\\",\\"a\\",null,{\\"href\\":\\"#rate-limit-headers\\",\\"className\\":\\"anchor-link\\",\\"children\\":\[\\"Rate Limit Headers\\"\]}\]}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\[\\"In addition to viewing your limits on your account's \\",\[\\"$\\",\\"$Lb\\",null,{\\"prefetch\\":true,\\"href\\":\\"https://console.groq.com/settings/limits\\",\\"children\\":\\"limits\\"}\],\\" page, you can also view rate limit information such as remaining requests and tokens in HTTP response\\\\nheaders as follows:\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"div\\",null,{\\"className\\":\\"h-3\\"}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\\"The following headers are set (values are illustrative):\\"}\],\\"\\\\n\\",\[\\"$\\",\\"div\\",null,{\\"className\\":\\"h-3\\"}\],\\"\\\\n\\",\[\\"$\\",\\"$Ld\\",null,{}\],\\"\\\\n\\",\[\\"$\\",\\"h2\\",null,{\\"id\\":\\"handling-rate-limits\\",\\"className\\":\\"\[\\u0026:not(:first-child)\]:mt-12 mb-3 text-xl w-fit font-semibold font-header hover:underline\\",\\"children\\":\[\\"$\\",\\"a\\",null,{\\"href\\":\\"#handling-rate-limits\\",\\"className\\":\\"anchor-link\\",\\"children\\":\[\\"Handling Rate Limits\\"\]}\]}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\[\\"When you exceed rate limits, our API returns a \\",\[\\"$\\",\\"code\\",null,{\\"className\\":\\"bg-gray-100 dark:bg-gray-800 py-0.5 px-\[0.5em\] rounded-\[4px\]\\",\\"children\\":\\"429 Too Many Requests\\"}\],\\" HTTP status code.\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"div\\",null,{\\"className\\":\\"h-3\\"}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\[\[\\"$\\",\\"strong\\",null,{\\"children\\":\\"Note\\"}\],\\": \\",\[\\"$\\",\\"code\\",null,{\\"className\\":\\"bg-gray-100 dark:bg-gray-800 py-0.5 px-\[0.5em\] rounded-\[4px\]\\",\\"children\\":\\"retry-after\\"}\],\\" is only set if you hit the rate limit and status code 429 is returned. The other headers are always included.\\"\]}\],\\"\\\\n\\",\[\\"$\\",\\"h2\\",null,{\\"id\\":\\"need-higher-rate-limits\\",\\"className\\":\\"\[\\u0026:not(:first-child)\]:mt-12 mb-3 text-xl w-fit font-semibold font-header hover:underline\\",\\"children\\":\[\\"$\\",\\"a\\",null,{\\"href\\":\\"#need-higher-rate-limits\\",\\"className\\":\\"anchor-link\\",\\"children\\":\[\\"Need Higher Rate Limits?\\"\]}\]}\],\\"\\\\n\\",\[\\"$\\",\\"p\\",null,{\\"className\\":\\"text-sm my-3\\",\\"children\\":\[\\"If you need higher rate limits, you can \\",\[\\"$\\",\\"$Lb\\",null,{\\"prefetch\\":true,\\"href\\":\\"https://groq.com/self-serve-support\\",\\"children\\":\\"request them here\\"}\],\\".\\"\]}\]\],\[\[\\"$\\",\\"link\\",\\"0\\",{\\"rel\\":\\"stylesheet\\",\\"href\\":\\"/\_next/static/css/b1dbd2d25ec6d91f.css\\",\\"precedence\\":\\"next\\",\\"crossOrigin\\":\\"$undefined\\",\\"nonce\\":\\"$undefined\\"}\]\],\[\\"$\\",\\"$Le\\",null,{\\"children\\":\[\\"$Lf\\",\\"$L10\\",null\]}\]\]}\],{},null,false\]},null,false\]},null,false\]},null,false\]},null,false\]},null,false\],\[\\"$\\",\\"$1\\",\\"h\\",{\\"children\\":\[null,\[\\"$\\",\\"$1\\",\\"jMv6wua-wSswNwDCq\_L8Tv\\",{\\"children\\":\[\[\\"$\\",\\"$L11\\",null,{\\"children\\":\\"$L12\\"}\],\[\\"$\\",\\"meta\\",null,{\\"name\\":\\"next-size-adjust\\",\\"content\\":\\"\\"}\]\]}\],\[\\"$\\",\\"$L13\\",null,{\\"children\\":\\"$L14\\"}\]\]}\],false\]\],\\"m\\":\\"$undefined\\",\\"G\\":\[\\"$15\\",\[\]\],\\"s\\":false,\\"S\\":true}\\n"\])self.\_\_next\_f.push(\[1,"18:I\[95037,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"7487\\",\\"static/chunks/b3b06311-fa77c49096c4cc97.js\\",\\"7539\\",\\"static/chunks/1f447996-8974f1b7e13f2cb9.js\\",\\"8455\\",\\"static/chunks/d611311c-7b5735a4c92c90d9.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"7213\\",\\"static/chunks/7213-443f51519115da15.js\\",\\"7177\\",\\"static/chunks/app/layout-a8184e653bbd672b.js\\"\],\\"GoogleAnalytics\\"\]\\n19:I\[43717,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"7487\\",\\"static/chunks/b3b06311-fa77c49096c4cc97.js\\",\\"7539\\",\\"static/chunks/1f447996-8974f1b7e13f2cb9.js\\",\\"8455\\",\\"static/chunks/d611311c-7b5735a4c92c90d9.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"7213\\",\\"static/chunks/7213-443f51519115da15.js\\",\\"7177\\",\\"static/chunks/app/layout-a8184e653bbd672b.js\\"\],\\"GoogleTagManager\\"\]\\n1a:I\[67986,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"7487\\",\\"static/chunks/b3b06311-fa77c49096c4cc97.js\\",\\"7539\\",\\"static/chunks/1f447996-8974f1b7e13f2cb9.js\\",\\"8455\\",\\"static/chunks/d611311c-7b5735a4c92c90d9.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"7213\\",\\"static/chunks/7213-443f51519115da15.js\\",\\"7177\\",\\"static/chunks/app/layout-a8184e653bbd672b.js\\"\],\\"SpeedInsights\\"\]\\n7:{}\\n16:T12b5,"\])self.\_\_next\_f.push(\[1,"/\* For our datagrail consent banner \*/\\n/\* https://docs.datagrail.io/docs/consent/banner/css-customization/ \*/\\n\\n:host(.dg-consent-banner) {\\n /\* Fonts - matching Groq Console system \*/\\n --dg-primary-font: Inter, -apple-system, BlinkMacSystemFont, \\"Segoe UI\\",\\n Roboto, \\"Helvetica Neue\\", Arial, sans-serif;\\n --dg-secondary-font: Montserrat, -apple-system, BlinkMacSystemFont, \\"Segoe UI\\",\\n Roboto, \\"Helvetica Neue\\", Arial, sans-serif;\\n\\n /\* General banner styling \*/\\n --dg-consent-background-color: rgb(255, 255, 255);\\n --dg-consent-background-border: rgb(231, 229, 228);\\n --consent-border-radius: 12px;\\n\\n /\* Body text styling \*/\\n --dg-body-font-size: 14px;\\n --dg-body-font-weight: 400;\\n --dg-body-font-color: rgb(118, 111, 107);\\n --dg-body-line-height: 1.5;\\n\\n /\* Heading styling \*/\\n --dg-heading-font-size: 18px;\\n --dg-heading-font-weight: 600;\\n --dg-heading-font-color: rgb(12, 10, 9);\\n --dg-heading-line-height: 1.4;\\n\\n /\* Title styling \*/\\n --dg-title-font-size: 16px;\\n --dg-title-font-weight: 500;\\n --dg-title-font-color: rgb(30, 30, 30);\\n --dg-title-line-height: 1.4;\\n\\n /\* Button styling \*/\\n --dg-button-border: rgb(231, 229, 228) 1px solid;\\n --dg-button-primary-background: rgb(30, 30, 30);\\n --dg-button-primary-color: rgb(248, 248, 247);\\n --dg-button-secondary-background: rgb(243, 243, 242);\\n --dg-button-secondary-color: rgb(28, 25, 23);\\n\\n /\* Category styling \*/\\n --dg-policy-option-heading-size: 15px;\\n --dg-policy-option-heading-weight: 500;\\n --dg-policy-option-heading-color: rgb(30, 30, 30);\\n --dg-policy-option-heading-enabled-color: rgb(245, 80, 54);\\n --dg-policy-option-chevron-size: 16;\\n\\n /\* Category description \*/\\n --dg-policy-option-description-font-size: 13px;\\n --dg-policy-option-description-font-weight: 400;\\n --dg-policy-option-description-font-color: rgb(118, 111, 107);\\n\\n /\* Essential categories \*/\\n --dg-policy-option-essential-label-font-size: 12px;\\n --dg-policy-option-essential-label-font-weight: 500;\\n --dg-policy-option-essential-label-font-color: rgb(118, 111, 107);\\n\\n /\* Slider styling - inspired by switch component \*/\\n --dg-slider-primary: rgb(231, 229, 228);\\n --dg-slider-secondary: rgb(255, 255, 255);\\n\\n --dg-slider-enabled-primary: rgb(30, 30, 30);\\n --dg-slider-enabled-secondary: rgb(255, 255, 255);\\n\\n /\* For Enabled Categories \*/\\n --dg-policy-option-heading-enabled-color: rgb(245, 80, 54);\\n}\\n\\n:host(.dg-consent-banner) strong {\\n font-weight: 500;\\n}\\n\\n/\* Slider styling for checked enabled categories \*/\\n:host(.dg-consent-banner)\\n input\[type=\\"checkbox\\"\]:not(:disabled):checked\\n + label\\n .dg-slider {\\n background: rgb(30, 30, 30) !important;\\n}\\n\\n/\* Advanced button styling to match Groq Console \*/\\n:host(.dg-consent-banner) .dg-button {\\n border-radius: 8px !important;\\n padding: 8px 16px !important;\\n font-weight: 500 !important;\\n font-size: 14px !important;\\n transition: all 0.2s ease !important;\\n}\\n\\n:host(.dg-consent-banner) .dg-button.accept\_all,\\n:host(.dg-consent-banner) .dg-button.accept\_some,\\n:host(.dg-consent-banner) .dg-button.reject\_all,\\n:host(.dg-consent-banner) .dg-button.open\_layer,\\n:host(.dg-consent-banner) .dg-button.custom {\\n background: rgb(243, 243, 242) !important;\\n color: rgb(28, 25, 23) !important;\\n border: 1px solid rgb(231, 229, 228) !important;\\n}\\n\\n:host(.dg-consent-banner) .dg-button.accept\_all:hover,\\n:host(.dg-consent-banner) .dg-button.accept\_some:hover,\\n:host(.dg-consent-banner) .dg-button.reject\_all:hover,\\n:host(.dg-consent-banner) .dg-button.open\_layer:hover,\\n:host(.dg-consent-banner) .dg-button.custom:hover {\\n background: rgb(231, 229, 228) !important;\\n}\\n\\n/\* Link styling to match Groq Console \*/\\n:host(.dg-consent-banner) .dg-link {\\n color: rgb(245, 80, 54) !important;\\n text-decoration: none !important;\\n font-weight: 500 !important;\\n}\\n\\n:host(.dg-consent-banner) .dg-link:hover {\\n text-decoration: underline !important;\\n}\\n\\n:host(.dg-consent-banner) .dg-main-content-policy-option-description p {\\n margin-top: 0 !important;\\n margin-bottom: 16px;\\n}\\n\\n/\* Dark mode support \*/\\n:host(.dg-consent-banner) .dark {\\n --dg-consent-background-color: rgb(18, 20, 24);\\n --dg-consent-background-border: rgba(153, 153, 153, 0.161);\\n --dg-body-font-color: rgb(165, 160, 156);\\n --dg-heading-font-color: rgb(248, 248, 247);\\n --dg-title-font-color: rgb(248, 248, 247);\\n --dg-policy-option-heading-color: rgb(248, 248, 247);\\n --dg-button-secondary-background: rgb(38, 38, 38);\\n --dg-button-secondary-color: rgb(248, 248, 247);\\n --dg-slider-primary: rgba(153, 153, 153, 0.35);\\n --dg-slider-background: rgb(107, 114, 128);\\n}\\n\\n/\* Overall banner container styling \*/\\n:host(.dg-consent-banner) .dg-app {\\n box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -2px\\n rgba(0, 0, 0, 0.1) !important;\\n border: 1px solid rgb(231, 229, 228) !important;\\n}\\n"\])self.\_\_next\_f.push(\[1,"2:\[\\"$\\",\\"html\\",null,{\\"lang\\":\\"en\\",\\"className\\":\\"\_\_variable\_f367f3 \_\_variable\_dd5b2f\\",\\"suppressHydrationWarning\\":true,\\"children\\":\[\[\\"$\\",\\"head\\",null,{\\"children\\":\[\[\\"$\\",\\"style\\",null,{\\"id\\":\\"dg-consent-custom-style\\",\\"dangerouslySetInnerHTML\\":{\\"\_\_html\\":\\"$16\\"}}\],\[\\"$\\",\\"link\\",null,{\\"rel\\":\\"icon\\",\\"href\\":\\"/favicon.ico?cache=rounded-bolt\\",\\"sizes\\":\\"32x32\\"}\]\]}\],\[\\"$\\",\\"script\\",null,{\\"async\\":true,\\"src\\":\\"/g.js\\"}\],\[\\"$\\",\\"script\\",null,{\\"async\\":true,\\"src\\":\\"https://js.stripe.com/v3/\\"}\],\[\\"$\\",\\"body\\",null,{\\"className\\":\\"font-inter\\",\\"children\\":\[\\"$L17\\",\[\\"$\\",\\"$L18\\",null,{\\"gaId\\":\\"G-CQ9K0VPEEQ\\"}\],\[\\"$\\",\\"$L19\\",null,{\\"gtmId\\":\\"GTM-WWK828JN\\"}\],\[\\"$\\",\\"$L1a\\",null,{\\"sampleRate\\":0.01}\]\]}\]\]}\]\\n"\])self.\_\_next\_f.push(\[1,"12:\[\[\\"$\\",\\"meta\\",\\"0\\",{\\"charSet\\":\\"utf-8\\"}\],\[\\"$\\",\\"meta\\",\\"1\\",{\\"name\\":\\"viewport\\",\\"content\\":\\"width=device-width, initial-scale=1, maximum-scale=1\\"}\]\]\\nf:null\\n"\])self.\_\_next\_f.push(\[1,"10:null\\n14:\[\[\\"$\\",\\"title\\",\\"0\\",{\\"children\\":\\"Rate Limits - GroqDocs\\"}\],\[\\"$\\",\\"meta\\",\\"1\\",{\\"name\\":\\"description\\",\\"content\\":\\"Understand Groq API rate limits, headers, and best practices for managing request and token quotas in your applications.\\"}\],\[\\"$\\",\\"meta\\",\\"2\\",{\\"property\\":\\"og:title\\",\\"content\\":\\"Rate Limits - GroqDocs\\"}\],\[\\"$\\",\\"meta\\",\\"3\\",{\\"property\\":\\"og:description\\",\\"content\\":\\"Understand Groq API rate limits, headers, and best practices for managing request and token quotas in your applications.\\"}\],\[\\"$\\",\\"meta\\",\\"4\\",{\\"property\\":\\"og:url\\",\\"content\\":\\"https://console.groq.com/docs\\"}\],\[\\"$\\",\\"meta\\",\\"5\\",{\\"property\\":\\"og:site\_name\\",\\"content\\":\\"GroqDocs\\"}\],\[\\"$\\",\\"meta\\",\\"6\\",{\\"property\\":\\"og:image\\",\\"content\\":\\"https://console.groq.com/og\_cloudv5.jpg\\"}\],\[\\"$\\",\\"meta\\",\\"7\\",{\\"property\\":\\"og:type\\",\\"content\\":\\"website\\"}\],\[\\"$\\",\\"meta\\",\\"8\\",{\\"name\\":\\"twitter:card\\",\\"content\\":\\"summary\_large\_image\\"}\],\[\\"$\\",\\"meta\\",\\"9\\",{\\"name\\":\\"twitter:title\\",\\"content\\":\\"Rate Limits - GroqDocs\\"}\],\[\\"$\\",\\"meta\\",\\"10\\",{\\"name\\":\\"twitter:description\\",\\"content\\":\\"Understand Groq API rate limits, headers, and best practices for managing request and token quotas in your applications.\\"}\],\[\\"$\\",\\"meta\\",\\"11\\",{\\"name\\":\\"twitter:image\\",\\"content\\":\\"https://console.groq.com/og\_cloudv5.jpg\\"}\],\[\\"$\\",\\"link\\",\\"12\\",{\\"rel\\":\\"icon\\",\\"href\\":\\"/favicon.ico?cache=rounded-bolt\\",\\"sizes\\":\\"32x32\\"}\]\]\\n"\])self.\_\_next\_f.push(\[1,"1b:I\[29135,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"1874\\",\\"static/chunks/1874-1af85b5fe730b1d1.js\\",\\"5193\\",\\"static/chunks/5193-5a50519070c43274.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"4215\\",\\"static/chunks/4215-210105fc188a4582.js\\",\\"8285\\",\\"static/chunks/8285-cf692dbe291dd0a9.js\\",\\"5674\\",\\"static/chunks/5674-883f5bd3e8483206.js\\",\\"8256\\",\\"static/chunks/8256-7af1a3383dec9c4d.js\\",\\"5789\\",\\"static/chunks/5789-d4359a1055e0aacd.js\\",\\"7860\\",\\"static/chunks/7860-a6cd4bd13b0a2513.js\\",\\"4209\\",\\"static/chunks/4209-19ded76058a94842.js\\",\\"7001\\",\\"static/chunks/app/(console)/docs/layout-d061ddcc54588b7f.js\\"\],\\"default\\"\]\\n1c:Tfb0,"\])self.\_\_next\_f.push(\[1,"{\\n \\"id\\": \\"chatcmpl-f51b2cd2-bef7-417e-964e-a08f0b513c22\\",\\n \\"object\\": \\"chat.completion\\",\\n \\"created\\": 1730241104,\\n \\"model\\": \\"openai/gpt-oss-20b\\",\\n \\"choices\\": \[\\n {\\n \\"index\\": 0,\\n \\"message\\": {\\n \\"role\\": \\"assistant\\",\\n \\"content\\": \\"Fast language models have gained significant attention in recent years due to their ability to process and generate human-like text quickly and efficiently. The importance of fast language models can be understood from their potential applications and benefits:\\\\n\\\\n1. \*\*Real-time Chatbots and Conversational Interfaces\*\*: Fast language models enable the development of chatbots and conversational interfaces that can respond promptly to user queries, making them more engaging and useful.\\\\n2. \*\*Sentiment Analysis and Opinion Mining\*\*: Fast language models can quickly analyze text data to identify sentiments, opinions, and emotions, allowing for improved customer service, market research, and opinion mining.\\\\n3. \*\*Language Translation and Localization\*\*: Fast language models can quickly translate text between languages, facilitating global communication and enabling businesses to reach a broader audience.\\\\n4. \*\*Text Summarization and Generation\*\*: Fast language models can summarize long documents or even generate new text on a given topic, improving information retrieval and processing efficiency.\\\\n5. \*\*Named Entity Recognition and Information Extraction\*\*: Fast language models can rapidly recognize and extract specific entities, such as names, locations, and organizations, from unstructured text data.\\\\n6. \*\*Recommendation Systems\*\*: Fast language models can analyze large amounts of text data to personalize product recommendations, improve customer experience, and increase sales.\\\\n7. \*\*Content Generation for Social Media\*\*: Fast language models can quickly generate engaging content for social media platforms, helping businesses maintain a consistent online presence and increasing their online visibility.\\\\n8. \*\*Sentiment Analysis for Stock Market Analysis\*\*: Fast language models can quickly analyze social media posts, news articles, and other text data to identify sentiment trends, enabling financial analysts to make more informed investment decisions.\\\\n9. \*\*Language Learning and Education\*\*: Fast language models can provide instant feedback and adaptive language learning, making language education more effective and engaging.\\\\n10. \*\*Domain-Specific Knowledge Extraction\*\*: Fast language models can quickly extract relevant information from vast amounts of text data, enabling domain experts to focus on high-level decision-making rather than manual information gathering.\\\\n\\\\nThe benefits of fast language models include:\\\\n\\\\n\* \*\*Increased Efficiency\*\*: Fast language models can process large amounts of text data quickly, reducing the time and effort required for tasks such as sentiment analysis, entity recognition, and text summarization.\\\\n\* \*\*Improved Accuracy\*\*: Fast language models can analyze and learn from large datasets, leading to more accurate results and more informed decision-making.\\\\n\* \*\*Enhanced User Experience\*\*: Fast language models can enable real-time interactions, personalized recommendations, and timely responses, improving the overall user experience.\\\\n\* \*\*Cost Savings\*\*: Fast language models can automate many tasks, reducing the need for manual labor and minimizing costs associated with data processing and analysis.\\\\n\\\\nIn summary, fast language models have the potential to transform various industries and applications by providing fast, accurate, and efficient language processing capabilities.\\"\\n },\\n \\"logprobs\\": null,\\n \\"finish\_reason\\": \\"stop\\"\\n }\\n \],\\n \\"usage\\": {\\n \\"queue\_time\\": 0.037493756,\\n \\"prompt\_tokens\\": 18,\\n \\"prompt\_time\\": 0.000680594,\\n \\"completion\_tokens\\": 556,\\n \\"completion\_time\\": 0.463333333,\\n \\"total\_tokens\\": 574,\\n \\"total\_time\\": 0.464013927\\n },\\n \\"system\_fingerprint\\": \\"fp\_179b0f92c9\\",\\n \\"x\_groq\\": { \\"id\\": \\"req\_01jbd6g2qdfw2adyrt2az8hz4w\\" }\\n}\\n"\])self.\_\_next\_f.push(\[1,"1d:T699,{\\n \\"object\\": \\"list\\",\\n \\"data\\": \[\\n {\\n \\"id\\": \\"gemma2-9b-it\\",\\n \\"object\\": \\"model\\",\\n \\"created\\": 1693721698,\\n \\"owned\_by\\": \\"Google\\",\\n \\"active\\": true,\\n \\"context\_window\\": 8192,\\n \\"public\_apps\\": null\\n },\\n {\\n \\"id\\": \\"llama3-8b-8192\\",\\n \\"object\\": \\"model\\",\\n \\"created\\": 1693721698,\\n \\"owned\_by\\": \\"Meta\\",\\n \\"active\\": true,\\n \\"context\_window\\": 8192,\\n \\"public\_apps\\": null\\n },\\n {\\n \\"id\\": \\"llama3-70b-8192\\",\\n \\"object\\": \\"model\\",\\n \\"created\\": 1693721698,\\n \\"owned\_by\\": \\"Meta\\",\\n \\"active\\": true,\\n \\"context\_window\\": 8192,\\n \\"public\_apps\\": null\\n },\\n {\\n \\"id\\": \\"whisper-large-v3-turbo\\",\\n \\"object\\": \\"model\\",\\n \\"created\\": 1728413088,\\n \\"owned\_by\\": \\"OpenAI\\",\\n \\"active\\": true,\\n \\"context\_window\\": 448,\\n \\"public\_apps\\": null\\n },\\n {\\n \\"id\\": \\"whisper-large-v3\\",\\n \\"object\\": \\"model\\",\\n \\"created\\": 1693721698,\\n \\"owned\_by\\": \\"OpenAI\\",\\n \\"active\\": true,\\n \\"context\_window\\": 448,\\n \\"public\_apps\\": null\\n },\\n {\\n \\"id\\": \\"llama-guard-3-8b\\",\\n \\"object\\": \\"model\\",\\n \\"created\\": 1693721698,\\n \\"owned\_by\\": \\"Meta\\",\\n \\"active\\": true,\\n \\"context\_window\\": 8192,\\n \\"public\_apps\\": null\\n },\\n {\\n \\"id\\": \\"distil-whisper-large-v3-en\\",\\n \\"object\\": \\"model\\",\\n \\"created\\": 1693721698,\\n \\"owned\_by\\": \\"Hugging Face\\",\\n \\"active\\": true,\\n \\"context\_window\\": 448,\\n \\"public\_apps\\": null\\n },\\n {\\n \\"id\\": \\"llama-3.1-8b-instant\\",\\n \\"object\\": \\"model\\",\\n \\"created\\": 1693721698,\\n \\"owned\_by\\": \\"Meta\\",\\n \\"active\\": true,\\n \\"context\_window\\": 131072,\\n \\"public\_apps\\": null\\n }\\n \]\\n}\\n1e:T5c5,{\\n \\"id\\": \\"resp\_01k1x6w9ane6d8rfxm05cb45yk\\",\\n \\"object\\": \\"response\\",\\n \\"status\\": \\"completed\\",\\n \\"created\_at\\": 1754400695,\\n \\"output\\": \[\\n {\\n \\"type\\": \\"message\\",\\n \\"id\\": \\"msg\_01k1x6w9ane6eb0650crhawwyy\\",\\n \\"status\\": \\"completed\\",\\n \\"role\\": \\"assistant\\",\\n \\"content\\": \[\\n {\\n \\"type\\": \\"output\_text\\",\\n \\"t"\])self.\_\_next\_f.push(\[1,"ext\\": \\"When the stars blinked awake, Luna the unicorn curled her mane and whispered wishes to the sleeping pine trees. She galloped through a field of moonlit daisies, gathering dew like tiny silver pearls. With a gentle sigh, she tucked her hooves beneath a silver cloud so the world slept softly, dreaming of her gentle hooves until the morning.\\",\\n \\"annotations\\": \[\]\\n }\\n \]\\n }\\n \],\\n \\"previous\_response\_id\\": null,\\n \\"model\\": \\"llama-3.3-70b-versatile\\",\\n \\"reasoning\\": {\\n \\"effort\\": null,\\n \\"summary\\": null\\n },\\n \\"max\_output\_tokens\\": null,\\n \\"instructions\\": null,\\n \\"text\\": {\\n \\"format\\": {\\n \\"type\\": \\"text\\"\\n }\\n },\\n \\"tools\\": \[\],\\n \\"tool\_choice\\": \\"auto\\",\\n \\"truncation\\": \\"disabled\\",\\n \\"metadata\\": {},\\n \\"temperature\\": 1,\\n \\"top\_p\\": 1,\\n \\"user\\": null,\\n \\"service\_tier\\": \\"default\\",\\n \\"error\\": null,\\n \\"incomplete\_details\\": null,\\n \\"usage\\": {\\n \\"input\_tokens\\": 82,\\n \\"input\_tokens\_details\\": {\\n \\"cached\_tokens\\": 0\\n },\\n \\"output\_tokens\\": 266,\\n \\"output\_tokens\_details\\": {\\n \\"reasoning\_tokens\\": 0\\n },\\n \\"total\_tokens\\": 348\\n },\\n \\"parallel\_tool\_calls\\": true,\\n \\"store\\": false\\n}\\n"\])self.\_\_next\_f.push(\[1,"8:\[\\"$\\",\\"$L1b\\",null,{\\"openapiSpec\\":{\\"components\\":{\\"schemas\\":{\\"Annotation\\":{\\"description\\":\\"An annotation that provides citations or references for content in a message.\\",\\"properties\\":{\\"document\_citation\\":{\\"$ref\\":\\"#/components/schemas/DocumentCitation\\"},\\"function\_citation\\":{\\"$ref\\":\\"#/components/schemas/FunctionCitation\\"},\\"type\\":{\\"description\\":\\"The type of annotation.\\",\\"enum\\":\[\\"document\_citation\\",\\"function\_citation\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\"\],\\"type\\":\\"object\\"},\\"Batch\\":{\\"properties\\":{\\"cancelled\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch was cancelled.\\",\\"type\\":\\"integer\\"},\\"cancelling\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch started cancelling.\\",\\"type\\":\\"integer\\"},\\"completed\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch was completed.\\",\\"type\\":\\"integer\\"},\\"completion\_window\\":{\\"description\\":\\"The time frame within which the batch should be processed.\\",\\"type\\":\\"string\\"},\\"created\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch was created.\\",\\"type\\":\\"integer\\"},\\"endpoint\\":{\\"description\\":\\"The API endpoint used by the batch.\\",\\"type\\":\\"string\\"},\\"error\_file\_id\\":{\\"description\\":\\"The ID of the file containing the outputs of requests with errors.\\",\\"type\\":\\"string\\"},\\"errors\\":{\\"properties\\":{\\"data\\":{\\"items\\":{\\"properties\\":{\\"code\\":{\\"description\\":\\"An error code identifying the error type.\\",\\"type\\":\\"string\\"},\\"line\\":{\\"description\\":\\"The line number of the input file where the error occurred, if applicable.\\",\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"message\\":{\\"description\\":\\"A human-readable message providing more details about the error.\\",\\"type\\":\\"string\\"},\\"param\\":{\\"description\\":\\"The name of the parameter that caused the error, if applicable.\\",\\"nullable\\":true,\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"type\\":\\"array\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \`list\`.\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"expired\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch expired.\\",\\"type\\":\\"integer\\"},\\"expires\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch will expire.\\",\\"type\\":\\"integer\\"},\\"failed\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch failed.\\",\\"type\\":\\"integer\\"},\\"finalizing\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch started finalizing.\\",\\"type\\":\\"integer\\"},\\"id\\":{\\"type\\":\\"string\\"},\\"in\_progress\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the batch started processing.\\",\\"type\\":\\"integer\\"},\\"input\_file\_id\\":{\\"description\\":\\"The ID of the input file for the batch.\\",\\"type\\":\\"string\\"},\\"metadata\\":{\\"description\\":\\"Set of key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"object\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \`batch\`.\\",\\"enum\\":\[\\"batch\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true},\\"output\_file\_id\\":{\\"description\\":\\"The ID of the file containing the outputs of successfully executed requests.\\",\\"type\\":\\"string\\"},\\"request\_counts\\":{\\"description\\":\\"The request counts for different statuses within the batch.\\",\\"properties\\":{\\"completed\\":{\\"description\\":\\"Number of requests that have been completed successfully.\\",\\"type\\":\\"integer\\"},\\"failed\\":{\\"description\\":\\"Number of requests that have failed.\\",\\"type\\":\\"integer\\"},\\"total\\":{\\"description\\":\\"Total number of requests in the batch.\\",\\"type\\":\\"integer\\"}},\\"required\\":\[\\"total\\",\\"completed\\",\\"failed\\"\],\\"type\\":\\"object\\"},\\"status\\":{\\"description\\":\\"The current status of the batch.\\",\\"enum\\":\[\\"validating\\",\\"failed\\",\\"in\_progress\\",\\"finalizing\\",\\"completed\\",\\"expired\\",\\"cancelling\\",\\"cancelled\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"id\\",\\"object\\",\\"endpoint\\",\\"input\_file\_id\\",\\"completion\_window\\",\\"status\\",\\"created\_at\\"\],\\"type\\":\\"object\\"},\\"BatchRequestInput\\":{\\"description\\":\\"The per-line object of the batch input file\\",\\"properties\\":{\\"custom\_id\\":{\\"description\\":\\"A developer-provided per-request id that will be used to match outputs to inputs. Must be unique for each request in a batch.\\",\\"type\\":\\"string\\"},\\"method\\":{\\"description\\":\\"The HTTP method to be used for the request. Currently only \`POST\` is supported.\\",\\"enum\\":\[\\"POST\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true},\\"url\\":{\\"description\\":\\"The OpenAI API relative URL to be used for the request. Currently \`/v1/chat/completions\` is supported.\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"BatchRequestOutput\\":{\\"description\\":\\"The per-line object of the batch output and error files\\",\\"properties\\":{\\"custom\_id\\":{\\"description\\":\\"A developer-provided per-request id that will be used to match outputs to inputs.\\",\\"type\\":\\"string\\"},\\"error\\":{\\"description\\":\\"For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure.\\",\\"nullable\\":true,\\"properties\\":{\\"code\\":{\\"description\\":\\"A machine-readable error code.\\",\\"type\\":\\"string\\"},\\"message\\":{\\"description\\":\\"A human-readable error message.\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"id\\":{\\"type\\":\\"string\\"},\\"response\\":{\\"nullable\\":true,\\"properties\\":{\\"body\\":{\\"description\\":\\"The JSON body of the response\\",\\"type\\":\\"object\\"},\\"request\_id\\":{\\"description\\":\\"An unique identifier for the OpenAI API request. Please include this request ID when contacting support.\\",\\"type\\":\\"string\\"},\\"status\_code\\":{\\"description\\":\\"The HTTP status code of the response\\",\\"type\\":\\"integer\\"}},\\"type\\":\\"object\\"}},\\"type\\":\\"object\\"},\\"BrowserResult\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"content\\":{\\"description\\":\\"The content of the browser result\\",\\"type\\":\\"string\\"},\\"live\_view\_url\\":{\\"description\\":\\"The live view URL for the browser window\\",\\"type\\":\\"string\\"},\\"title\\":{\\"description\\":\\"The title of the browser window\\",\\"type\\":\\"string\\"},\\"url\\":{\\"description\\":\\"The URL of the browser window\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"url\\",\\"title\\"\],\\"type\\":\\"object\\"},\\"Chart\\":{\\"properties\\":{\\"elements\\":{\\"description\\":\\"The chart elements (data series, points, etc.)\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChartElement\\"},\\"type\\":\\"array\\"},\\"title\\":{\\"description\\":\\"The title of the chart\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of chart\\",\\"enum\\":\[\\"bar\\",\\"box\_and\_whisker\\",\\"line\\",\\"pie\\",\\"scatter\\",\\"superchart\\",\\"unknown\\"\],\\"type\\":\\"string\\"},\\"x\_label\\":{\\"description\\":\\"The label for the x-axis\\",\\"type\\":\\"string\\"},\\"x\_scale\\":{\\"description\\":\\"The scale type for the x-axis\\",\\"type\\":\\"string\\"},\\"x\_tick\_labels\\":{\\"description\\":\\"The labels for the x-axis ticks\\",\\"items\\":{\\"type\\":\\"string\\"},\\"type\\":\\"array\\"},\\"x\_ticks\\":{\\"description\\":\\"The tick values for the x-axis\\",\\"items\\":{\\"type\\":\\"number\\"},\\"type\\":\\"array\\"},\\"x\_unit\\":{\\"description\\":\\"The unit for the x-axis\\",\\"type\\":\\"string\\"},\\"y\_label\\":{\\"description\\":\\"The label for the y-axis\\",\\"type\\":\\"string\\"},\\"y\_scale\\":{\\"description\\":\\"The scale type for the y-axis\\",\\"type\\":\\"string\\"},\\"y\_tick\_labels\\":{\\"description\\":\\"The labels for the y-axis ticks\\",\\"items\\":{\\"type\\":\\"string\\"},\\"type\\":\\"array\\"},\\"y\_ticks\\":{\\"description\\":\\"The tick values for the y-axis\\",\\"items\\":{\\"type\\":\\"number\\"},\\"type\\":\\"array\\"},\\"y\_unit\\":{\\"description\\":\\"The unit for the y-axis\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"elements\\"\],\\"type\\":\\"object\\"},\\"ChartElement\\":{\\"properties\\":{\\"angle\\":{\\"description\\":\\"The angle for this element\\",\\"type\\":\\"number\\"},\\"first\_quartile\\":{\\"description\\":\\"The first quartile value for this element\\",\\"type\\":\\"number\\"},\\"group\\":{\\"description\\":\\"The group this element belongs to\\",\\"type\\":\\"string\\"},\\"label\\":{\\"description\\":\\"The label for this chart element\\",\\"type\\":\\"string\\"},\\"max\\":{\\"type\\":\\"number\\"},\\"median\\":{\\"description\\":\\"The median value for this element\\",\\"type\\":\\"number\\"},\\"min\\":{\\"description\\":\\"The minimum value for this element\\",\\"type\\":\\"number\\"},\\"outliers\\":{\\"description\\":\\"The outliers for this element\\",\\"items\\":{\\"type\\":\\"number\\"},\\"type\\":\\"array\\"},\\"points\\":{\\"description\\":\\"The points for this element\\",\\"items\\":{\\"items\\":{\\"type\\":\\"number\\"},\\"type\\":\\"array\\"},\\"type\\":\\"array\\"},\\"radius\\":{\\"description\\":\\"The radius for this element\\",\\"type\\":\\"number\\"},\\"third\_quartile\\":{\\"description\\":\\"The third quartile value for this element\\",\\"type\\":\\"number\\"},\\"value\\":{\\"description\\":\\"The value for this element\\",\\"type\\":\\"number\\"}},\\"required\\":\[\\"label\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionDocument\\":{\\"additionalProperties\\":false,\\"description\\":\\"A document that can be referenced by the model while generating responses.\\",\\"properties\\":{\\"id\\":{\\"description\\":\\"Optional unique identifier that can be used for citations in responses.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"source\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionDocumentSource\\"}},\\"required\\":\[\\"source\\"\],\\"title\\":\\"Document\\",\\"type\\":\\"object\\"},\\"ChatCompletionDocumentSource\\":{\\"description\\":\\"The source of the document. Only text and JSON sources are currently supported.\\",\\"discriminator\\":{\\"mapping\\":{\\"json\\":\\"#/components/schemas/ChatCompletionDocumentSourceJSON\\",\\"text\\":\\"#/components/schemas/ChatCompletionDocumentSourceText\\"},\\"propertyName\\":\\"type\\"},\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ChatCompletionDocumentSourceText\\"},{\\"$ref\\":\\"#/components/schemas/ChatCompletionDocumentSourceJSON\\"}\],\\"title\\":\\"Document source\\"},\\"ChatCompletionDocumentSourceJSON\\":{\\"additionalProperties\\":false,\\"description\\":\\"A document whose contents are provided inline as JSON data.\\",\\"properties\\":{\\"data\\":{\\"additionalProperties\\":true,\\"description\\":\\"The JSON payload associated with the document.\\",\\"type\\":\\"object\\"},\\"type\\":{\\"description\\":\\"Identifies this document source as JSON data.\\",\\"enum\\":\[\\"json\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"data\\"\],\\"title\\":\\"JSON document source\\",\\"type\\":\\"object\\"},\\"ChatCompletionDocumentSourceText\\":{\\"additionalProperties\\":false,\\"description\\":\\"A document whose contents are provided inline as text.\\",\\"properties\\":{\\"text\\":{\\"description\\":\\"The document contents.\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"Identifies this document source as inline text.\\",\\"enum\\":\[\\"text\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"text\\"\],\\"title\\":\\"Text document source\\",\\"type\\":\\"object\\"},\\"ChatCompletionFunctionCallOption\\":{\\"description\\":\\"Specifying a particular function via \`{\\\\\\"name\\\\\\": \\\\\\"my\_function\\\\\\"}\` forces the model to call that function.\\\\n\\",\\"properties\\":{\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"name\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionFunctions\\":{\\"deprecated\\":true,\\"properties\\":{\\"description\\":{\\"description\\":\\"A description of what the function does, used by the model to choose when and how to call the function.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\\",\\"type\\":\\"string\\"},\\"parameters\\":{\\"$ref\\":\\"#/components/schemas/FunctionParameters\\"}},\\"required\\":\[\\"name\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionMessageExecutedTools\\":{\\"description\\":\\"A list of tools that were executed during the chat completion for compound AI systems.\\",\\"items\\":{\\"properties\\":{\\"arguments\\":{\\"description\\":\\"The arguments passed to the tool in JSON format.\\",\\"type\\":\\"string\\"},\\"browser\_results\\":{\\"description\\":\\"Array of browser results\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/BrowserResult\\"},\\"type\\":\\"array\\"},\\"code\_results\\":{\\"description\\":\\"Array of code execution results\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/CodeExecutionResult\\"},\\"type\\":\\"array\\"},\\"index\\":{\\"description\\":\\"The index of the executed tool.\\",\\"type\\":\\"integer\\"},\\"output\\":{\\"description\\":\\"The output returned by the tool.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"search\_results\\":{\\"description\\":\\"The search results returned by the tool, if applicable.\\",\\"nullable\\":true,\\"properties\\":{\\"images\\":{\\"description\\":\\"List of image URLs returned by the search\\",\\"items\\":{\\"type\\":\\"string\\"},\\"type\\":\\"array\\"},\\"results\\":{\\"description\\":\\"List of search results\\",\\"items\\":{\\"properties\\":{\\"content\\":{\\"description\\":\\"The content of the search result\\",\\"type\\":\\"string\\"},\\"score\\":{\\"description\\":\\"The relevance score of the search result\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"title\\":{\\"description\\":\\"The title of the search result\\",\\"type\\":\\"string\\"},\\"url\\":{\\"description\\":\\"The URL of the search result\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"type\\":\\"array\\"}},\\"type\\":\\"object\\"},\\"type\\":{\\"description\\":\\"The type of tool that was executed.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"index\\",\\"type\\",\\"arguments\\"\],\\"type\\":\\"object\\"},\\"type\\":\\"array\\"},\\"ChatCompletionMessageToolCall\\":{\\"properties\\":{\\"function\\":{\\"description\\":\\"The function that the model called.\\",\\"properties\\":{\\"arguments\\":{\\"description\\":\\"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"name\\",\\"arguments\\"\],\\"type\\":\\"object\\"},\\"id\\":{\\"description\\":\\"The ID of the tool call.\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the tool. Currently, only \`function\` is supported.\\",\\"enum\\":\[\\"function\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"id\\",\\"type\\",\\"function\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionMessageToolCallChunk\\":{\\"properties\\":{\\"function\\":{\\"properties\\":{\\"arguments\\":{\\"description\\":\\"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"id\\":{\\"description\\":\\"The ID of the tool call.\\",\\"type\\":\\"string\\"},\\"index\\":{\\"type\\":\\"integer\\"},\\"type\\":{\\"description\\":\\"The type of the tool. Currently, only \`function\` is supported.\\",\\"enum\\":\[\\"function\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"index\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionMessageToolCalls\\":{\\"description\\":\\"The tool calls generated by the model, such as function calls.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionMessageToolCall\\"},\\"type\\":\\"array\\"},\\"ChatCompletionNamedToolChoice\\":{\\"description\\":\\"Specifies a tool the model should use. Use to force the model to call a specific function.\\",\\"properties\\":{\\"function\\":{\\"properties\\":{\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"name\\"\],\\"type\\":\\"object\\"},\\"type\\":{\\"description\\":\\"The type of the tool. Currently, only \`function\` is supported.\\",\\"enum\\":\[\\"function\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"function\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionRequestAssistantMessage\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"content\\":{\\"description\\":\\"The contents of the assistant message. Required unless \`tool\_calls\` or \`function\_call\` is specified.\\\\n\\",\\"nullable\\":true,\\"oneOf\\":\[{\\"description\\":\\"The text contents of the message.\\",\\"title\\":\\"Text content\\",\\"type\\":\\"string\\"},{\\"description\\":\\"An array of content parts with a defined type, only \`text\` is supported for this message type.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestMessageContentPartText\\"},\\"title\\":\\"Array of content parts\\",\\"type\\":\\"array\\"}\],\\"title\\":\\"Assistant message content\\"},\\"function\_call\\":{\\"deprecated\\":true,\\"description\\":\\"Deprecated and replaced by \`tool\_calls\`. The name and arguments of a function that should be called, as generated by the model.\\",\\"properties\\":{\\"arguments\\":{\\"description\\":\\"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"name\\":{\\"description\\":\\"An optional name for the participant. Provides the model information to differentiate between participants of the same role.\\",\\"type\\":\\"string\\"},\\"reasoning\\":{\\"description\\":\\"The reasoning output by the assistant if reasoning\_format was set to 'parsed'.\\\\nThis field is only useable with qwen3 models.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"role\\":{\\"description\\":\\"The role of the messages author, in this case \`assistant\`.\\",\\"enum\\":\[\\"assistant\\"\],\\"type\\":\\"string\\"},\\"tool\_call\_id\\":{\\"description\\":\\"DO NOT USE. This field is present because OpenAI allows it and users send it.\\",\\"nullable\\":true,\\"required\\":\[\\"arguments\\",\\"name\\"\],\\"type\\":\\"string\\",\\"x-groq-meta\\":{\\"hidden\\":true}},\\"tool\_calls\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionMessageToolCalls\\"}},\\"required\\":\[\\"role\\"\],\\"title\\":\\"Assistant message\\",\\"type\\":\\"object\\"},\\"ChatCompletionRequestFunctionMessage\\":{\\"additionalProperties\\":false,\\"deprecated\\":true,\\"properties\\":{\\"content\\":{\\"description\\":\\"The contents of the function message.\\",\\"nullable\\":true,\\"title\\":\\"Function message content\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"},\\"role\\":{\\"description\\":\\"The role of the messages author, in this case \`function\`.\\",\\"enum\\":\[\\"function\\"\],\\"type\\":\\"string\\"},\\"tool\_call\_id\\":{\\"description\\":\\"DO NOT USE. This field is present because OpenAI allows it and users send it.\\",\\"nullable\\":true,\\"type\\":\\"string\\",\\"x-groq-meta\\":{\\"hidden\\":true}}},\\"required\\":\[\\"role\\",\\"content\\",\\"name\\"\],\\"title\\":\\"Function message\\",\\"type\\":\\"object\\"},\\"ChatCompletionRequestMessage\\":{\\"discriminator\\":{\\"mapping\\":{\\"assistant\\":\\"#/components/schemas/ChatCompletionRequestAssistantMessage\\",\\"developer\\":\\"#/components/schemas/ChatCompletionRequestSystemMessage\\",\\"function\\":\\"#/components/schemas/ChatCompletionRequestFunctionMessage\\",\\"system\\":\\"#/components/schemas/ChatCompletionRequestSystemMessage\\",\\"tool\\":\\"#/components/schemas/ChatCompletionRequestToolMessage\\",\\"user\\":\\"#/components/schemas/ChatCompletionRequestUserMessage\\"},\\"propertyName\\":\\"role\\"},\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestSystemMessage\\"},{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestUserMessage\\"},{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestAssistantMessage\\"},{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestToolMessage\\"},{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestFunctionMessage\\"}\]},\\"ChatCompletionRequestMessageContentPart\\":{\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestMessageContentPartText\\"},{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestMessageContentPartImage\\"}\]},\\"ChatCompletionRequestMessageContentPartImage\\":{\\"properties\\":{\\"image\_url\\":{\\"properties\\":{\\"detail\\":{\\"default\\":\\"auto\\",\\"description\\":\\"Specifies the detail level of the image.\\",\\"enum\\":\[\\"auto\\",\\"low\\",\\"high\\"\],\\"type\\":\\"string\\"},\\"url\\":{\\"description\\":\\"Either a URL of the image or the base64 encoded image data.\\",\\"format\\":\\"uri\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"url\\"\],\\"type\\":\\"object\\"},\\"type\\":{\\"description\\":\\"The type of the content part.\\",\\"enum\\":\[\\"image\_url\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"image\_url\\"\],\\"title\\":\\"Image content part\\",\\"type\\":\\"object\\"},\\"ChatCompletionRequestMessageContentPartText\\":{\\"properties\\":{\\"text\\":{\\"description\\":\\"The text content.\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the content part.\\",\\"enum\\":\[\\"text\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"text\\"\],\\"title\\":\\"Text content part\\",\\"type\\":\\"object\\"},\\"ChatCompletionRequestSystemMessage\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"content\\":{\\"description\\":\\"The contents of the system message.\\",\\"oneOf\\":\[{\\"description\\":\\"The text contents of the message.\\",\\"title\\":\\"Text content\\",\\"type\\":\\"string\\"},{\\"description\\":\\"An array of content parts with a defined type, only \`text\` is supported for this message type.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestMessageContentPartText\\"},\\"minItems\\":1,\\"title\\":\\"Array of content parts\\",\\"type\\":\\"array\\"}\],\\"title\\":\\"System message content\\"},\\"name\\":{\\"description\\":\\"An optional name for the participant. Provides the model information to differentiate between participants of the same role.\\",\\"type\\":\\"string\\"},\\"role\\":{\\"description\\":\\"The role of the messages author, in this case \`system\`.\\",\\"enum\\":\[\\"system\\",\\"developer\\"\],\\"type\\":\\"string\\"},\\"tool\_call\_id\\":{\\"description\\":\\"DO NOT USE. This field is present because OpenAI allows it and users send it.\\",\\"nullable\\":true,\\"type\\":\\"string\\",\\"x-groq-meta\\":{\\"hidden\\":true}}},\\"required\\":\[\\"content\\",\\"role\\"\],\\"title\\":\\"System message\\",\\"type\\":\\"object\\"},\\"ChatCompletionRequestToolMessage\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"content\\":{\\"description\\":\\"The contents of the tool message.\\",\\"oneOf\\":\[{\\"description\\":\\"The text contents of the message.\\",\\"title\\":\\"Text content\\",\\"type\\":\\"string\\"},{\\"description\\":\\"An array of content parts with a defined type, each can be of type \`text\` or \`image\_url\` when passing in images. You can pass multiple images by adding multiple \`image\_url\` content parts. Image input is only supported when using the \`gpt-4-visual-preview\` model.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestMessageContentPart\\"},\\"minItems\\":1,\\"title\\":\\"Array of content parts\\",\\"type\\":\\"array\\"}\],\\"title\\":\\"Tool message content\\"},\\"name\\":{\\"description\\":\\"DO NOT USE. This field is present because OpenAI allows it and users send it.\\",\\"type\\":\\"string\\",\\"x-groq-meta\\":{\\"hidden\\":true}},\\"role\\":{\\"description\\":\\"The role of the messages author, in this case \`tool\`.\\",\\"enum\\":\[\\"tool\\"\],\\"type\\":\\"string\\"},\\"tool\_call\_id\\":{\\"description\\":\\"Tool call that this message is responding to.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"role\\",\\"content\\",\\"tool\_call\_id\\"\],\\"title\\":\\"Tool message\\",\\"type\\":\\"object\\"},\\"ChatCompletionRequestUserMessage\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"content\\":{\\"description\\":\\"The contents of the user message.\\\\n\\",\\"oneOf\\":\[{\\"description\\":\\"The text contents of the message.\\",\\"title\\":\\"Text content\\",\\"type\\":\\"string\\"},{\\"description\\":\\"An array of content parts with a defined type, each can be of type \`text\` or \`image\_url\` when passing in images. You can pass multiple images by adding multiple \`image\_url\` content parts. Image input is only supported when using the \`gpt-4-visual-preview\` model.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestMessageContentPart\\"},\\"minItems\\":1,\\"title\\":\\"Array of content parts\\",\\"type\\":\\"array\\"}\],\\"title\\":\\"User message content\\"},\\"name\\":{\\"description\\":\\"An optional name for the participant. Provides the model information to differentiate between participants of the same role.\\",\\"type\\":\\"string\\"},\\"role\\":{\\"description\\":\\"The role of the messages author, in this case \`user\`.\\",\\"enum\\":\[\\"user\\"\],\\"type\\":\\"string\\"},\\"tool\_call\_id\\":{\\"description\\":\\"DO NOT USE. This field is present because OpenAI allows it and users send it.\\",\\"nullable\\":true,\\"type\\":\\"string\\",\\"x-groq-meta\\":{\\"hidden\\":true}}},\\"required\\":\[\\"content\\",\\"role\\"\],\\"title\\":\\"User message\\",\\"type\\":\\"object\\"},\\"ChatCompletionResponseMessage\\":{\\"description\\":\\"A chat completion message generated by the model.\\",\\"properties\\":{\\"annotations\\":{\\"description\\":\\"A list of annotations providing citations and references for the content in the message.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/Annotation\\"},\\"type\\":\\"array\\"},\\"content\\":{\\"description\\":\\"The contents of the message.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"executed\_tools\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionMessageExecutedTools\\"},\\"function\_call\\":{\\"deprecated\\":true,\\"description\\":\\"Deprecated and replaced by \`tool\_calls\`. The name and arguments of a function that should be called, as generated by the model.\\",\\"properties\\":{\\"arguments\\":{\\"description\\":\\"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"name\\",\\"arguments\\"\],\\"type\\":\\"object\\"},\\"reasoning\\":{\\"description\\":\\"The model's reasoning for a response. Only available for reasoning models when requests parameter reasoning\_format has value \`parsed.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"role\\":{\\"description\\":\\"The role of the author of this message.\\",\\"enum\\":\[\\"assistant\\"\],\\"type\\":\\"string\\"},\\"tool\_calls\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionMessageToolCalls\\"}},\\"required\\":\[\\"role\\",\\"content\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionRole\\":{\\"description\\":\\"The role of the author of a message\\",\\"enum\\":\[\\"system\\",\\"user\\",\\"assistant\\",\\"tool\\",\\"function\\"\],\\"type\\":\\"string\\"},\\"ChatCompletionStreamOptions\\":{\\"description\\":\\"Options for streaming response. Only set this when you set \`stream: true\`.\\\\n\\",\\"nullable\\":true,\\"properties\\":{\\"include\_usage\\":{\\"description\\":\\"If set, an additional chunk will be streamed before the \`data: \[DONE\]\` message. The \`usage\` field on this chunk shows the token usage statistics for the entire request, and the \`choices\` field will always be an empty array. All other chunks will also include a \`usage\` field, but with a null value.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"}},\\"type\\":\\"object\\"},\\"ChatCompletionStreamResponseDelta\\":{\\"description\\":\\"A chat completion delta generated by streamed model responses.\\",\\"properties\\":{\\"annotations\\":{\\"description\\":\\"A list of annotations providing citations and references for the content in the message.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/Annotation\\"},\\"type\\":\\"array\\"},\\"content\\":{\\"description\\":\\"The contents of the chunk message.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"executed\_tools\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionMessageExecutedTools\\"},\\"function\_call\\":{\\"deprecated\\":true,\\"description\\":\\"Deprecated and replaced by \`tool\_calls\`. The name and arguments of a function that should be called, as generated by the model.\\",\\"properties\\":{\\"arguments\\":{\\"description\\":\\"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"reasoning\\":{\\"description\\":\\"The model's reasoning for a response. Only available for reasoning models when requests parameter reasoning\_format has value \`parsed.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"role\\":{\\"description\\":\\"The role of the author of this message.\\",\\"enum\\":\[\\"system\\",\\"user\\",\\"assistant\\",\\"tool\\"\],\\"type\\":\\"string\\"},\\"tool\_calls\\":{\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionMessageToolCallChunk\\"},\\"type\\":\\"array\\"}},\\"type\\":\\"object\\"},\\"ChatCompletionTokenLogprob\\":{\\"properties\\":{\\"bytes\\":{\\"description\\":\\"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be \`null\` if there is no bytes representation for the token.\\",\\"items\\":{\\"type\\":\\"integer\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"logprob\\":{\\"description\\":\\"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value \`-9999.0\` is used to signify that the token is very unlikely.\\",\\"type\\":\\"number\\"},\\"token\\":{\\"description\\":\\"The token.\\",\\"type\\":\\"string\\"},\\"top\_logprobs\\":{\\"description\\":\\"List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested \`top\_logprobs\` returned.\\",\\"items\\":{\\"properties\\":{\\"bytes\\":{\\"description\\":\\"A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be \`null\` if there is no bytes representation for the token.\\",\\"items\\":{\\"type\\":\\"integer\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"logprob\\":{\\"description\\":\\"The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value \`-9999.0\` is used to signify that the token is very unlikely.\\",\\"type\\":\\"number\\"},\\"token\\":{\\"description\\":\\"The token.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"token\\",\\"logprob\\",\\"bytes\\"\],\\"type\\":\\"object\\"},\\"type\\":\\"array\\"}},\\"required\\":\[\\"token\\",\\"logprob\\",\\"bytes\\",\\"top\_logprobs\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionTool\\":{\\"properties\\":{\\"allowed\_tools\\":{\\"description\\":\\"A list of tool names to allow from the MCP server. If specified, only these tools will be exposed to the model. If empty or not specified, all discovered tools will be available.\\",\\"items\\":{\\"type\\":\\"string\\"},\\"type\\":\\"array\\",\\"x-groq-meta\\":{\\"hidden\\":true}},\\"function\\":{\\"$ref\\":\\"#/components/schemas/FunctionObject\\"},\\"headers\\":{\\"additionalProperties\\":{\\"type\\":\\"string\\"},\\"description\\":\\"HTTP headers to send with requests to the MCP server (optional for MCP tools).\\",\\"type\\":\\"object\\",\\"x-groq-meta\\":{\\"hidden\\":true}},\\"server\_label\\":{\\"description\\":\\"A human-readable label for the MCP server (optional for MCP tools).\\",\\"type\\":\\"string\\",\\"x-groq-meta\\":{\\"hidden\\":true}},\\"server\_url\\":{\\"description\\":\\"The URL of the MCP server to connect to (required for MCP tools).\\",\\"type\\":\\"string\\",\\"x-groq-meta\\":{\\"hidden\\":true}},\\"type\\":{\\"anyOf\\":\[{\\"description\\":\\"The type of the tool. \`function\`, \`browser\_search\`, and \`code\_interpreter\` are supported.\\",\\"enum\\":\[\\"function\\",\\"browser\_search\\",\\"code\_interpreter\\"\],\\"type\\":\\"string\\"},{\\"type\\":\\"string\\"}\]}},\\"required\\":\[\\"type\\"\],\\"type\\":\\"object\\"},\\"ChatCompletionToolChoiceOption\\":{\\"description\\":\\"Controls which (if any) tool is called by the model.\\\\n\`none\` means the model will not call any tool and instead generates a message.\\\\n\`auto\` means the model can pick between generating a message or calling one or more tools.\\\\n\`required\` means the model must call one or more tools.\\\\nSpecifying a particular tool via \`{\\\\\\"type\\\\\\": \\\\\\"function\\\\\\", \\\\\\"function\\\\\\": {\\\\\\"name\\\\\\": \\\\\\"my\_function\\\\\\"}}\` forces the model to call that tool.\\\\n\\\\n\`none\` is the default when no tools are present. \`auto\` is the default if tools are present.\\\\n\\",\\"nullable\\":true,\\"oneOf\\":\[{\\"description\\":\\"\`none\` means the model will not call any tool and instead generates a message. \`auto\` means the model can pick between generating a message or calling one or more tools.\\\\n\\",\\"enum\\":\[\\"none\\",\\"auto\\",\\"required\\"\],\\"type\\":\\"string\\"},{\\"$ref\\":\\"#/components/schemas/ChatCompletionNamedToolChoice\\"}\],\\"x-groq-meta\\":{\\"validator\\":\\"ChatCompletionToolChoiceOption\\"}},\\"ChatCompletionUsageBreakdown\\":{\\"description\\":\\"Usage statistics for compound AI completion requests.\\",\\"properties\\":{\\"models\\":{\\"description\\":\\"List of models used in the request and their individual usage statistics\\",\\"items\\":{\\"properties\\":{\\"model\\":{\\"description\\":\\"The name/identifier of the model used\\",\\"type\\":\\"string\\"},\\"usage\\":{\\"$ref\\":\\"#/components/schemas/CompletionUsage\\"}},\\"required\\":\[\\"model\\",\\"usage\\"\],\\"type\\":\\"object\\"},\\"type\\":\\"array\\"}},\\"required\\":\[\\"models\\"\],\\"type\\":\\"object\\"},\\"CodeExecutionOutput\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"results\\":{\\"description\\":\\"Array of code execution results\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/CodeExecutionResult\\"},\\"type\\":\\"array\\"},\\"stdout\\":{\\"description\\":\\"Standard output from code execution\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"stdout\\"\],\\"type\\":\\"object\\"},\\"CodeExecutionResult\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"chart\\":{\\"$ref\\":\\"#/components/schemas/Chart\\"},\\"charts\\":{\\"description\\":\\"Array of charts from a superchart\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/Chart\\"},\\"type\\":\\"array\\"},\\"png\\":{\\"description\\":\\"Base64 encoded PNG image output from code execution\\",\\"type\\":\\"string\\"},\\"text\\":{\\"description\\":\\"The text version of the code execution result\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"CompletionUsage\\":{\\"description\\":\\"Usage statistics for the completion request.\\",\\"properties\\":{\\"completion\_time\\":{\\"description\\":\\"Time spent generating tokens\\",\\"type\\":\\"number\\"},\\"completion\_tokens\\":{\\"description\\":\\"Number of tokens in the generated completion.\\",\\"type\\":\\"integer\\"},\\"prompt\_time\\":{\\"description\\":\\"Time spent processing input tokens\\",\\"type\\":\\"number\\"},\\"prompt\_tokens\\":{\\"description\\":\\"Number of tokens in the prompt.\\",\\"type\\":\\"integer\\"},\\"queue\_time\\":{\\"description\\":\\"Time the requests was spent queued\\",\\"type\\":\\"number\\"},\\"total\_time\\":{\\"description\\":\\"completion time and prompt time combined\\",\\"type\\":\\"number\\"},\\"total\_tokens\\":{\\"description\\":\\"Total number of tokens used in the request (prompt + completion).\\",\\"type\\":\\"integer\\"}},\\"required\\":\[\\"prompt\_tokens\\",\\"completion\_tokens\\",\\"total\_tokens\\"\],\\"type\\":\\"object\\"},\\"CreateChatCompletionRequest\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"citation\_options\\":{\\"default\\":\\"enabled\\",\\"description\\":\\"Whether to enable citations in the response. When enabled, the model will include citations for information retrieved from provided documents or web searches.\\",\\"enum\\":\[\\"enabled\\",\\"disabled\\"\],\\"nullable\\":true,\\"type\\":\\"string\\"},\\"compound\_custom\\":{\\"description\\":\\"Custom configuration of models and tools for Compound.\\",\\"nullable\\":true,\\"properties\\":{\\"models\\":{\\"nullable\\":true,\\"properties\\":{\\"answering\_model\\":{\\"description\\":\\"Custom model to use for answering.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"reasoning\_model\\":{\\"description\\":\\"Custom model to use for reasoning.\\",\\"nullable\\":true,\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"tools\\":{\\"description\\":\\"Configuration options for tools available to Compound.\\",\\"nullable\\":true,\\"properties\\":{\\"enabled\_tools\\":{\\"description\\":\\"A list of tool names that are enabled for the request.\\",\\"items\\":{\\"type\\":\\"string\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"wolfram\_settings\\":{\\"description\\":\\"Configuration for the Wolfram tool integration.\\",\\"nullable\\":true,\\"properties\\":{\\"authorization\\":{\\"description\\":\\"API key used to authorize requests to Wolfram services.\\",\\"nullable\\":true,\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"}},\\"type\\":\\"object\\"}},\\"type\\":\\"object\\"},\\"disable\_tool\_validation\\":{\\"default\\":false,\\"description\\":\\"If set to true, groq will return called tools without validating that the tool is present in request.tools. tool\_choice=required/none will still be enforced, but the request cannot require a specific tool be used.\\\\n\\",\\"type\\":\\"boolean\\"},\\"documents\\":{\\"description\\":\\"A list of documents to provide context for the conversation. Each document contains text that can be referenced by the model.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionDocument\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"exclude\_domains\\":{\\"deprecated\\":true,\\"description\\":\\"Deprecated: Use search\_settings.exclude\_domains instead.\\\\nA list of domains to exclude from the search results when the model uses a web search tool.\\\\n\\",\\"items\\":{\\"type\\":\\"string\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"exclude\_instance\_ids\\":{\\"description\\":\\"For internal use only\\\\n\\",\\"items\\":{\\"type\\":\\"string\\"},\\"nullable\\":true,\\"type\\":\\"array\\",\\"x-groq-meta\\":{\\"hidden\\":true}},\\"frequency\_penalty\\":{\\"default\\":0,\\"description\\":\\"This is not yet supported by any of our models. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\\",\\"maximum\\":2,\\"minimum\\":-2,\\"nullable\\":true,\\"type\\":\\"number\\"},\\"function\_call\\":{\\"deprecated\\":true,\\"description\\":\\"Deprecated in favor of \`tool\_choice\`.\\\\n\\\\nControls which (if any) function is called by the model.\\\\n\`none\` means the model will not call a function and instead generates a message.\\\\n\`auto\` means the model can pick between generating a message or calling a function.\\\\nSpecifying a particular function via \`{\\\\\\"name\\\\\\": \\\\\\"my\_function\\\\\\"}\` forces the model to call that function.\\\\n\\\\n\`none\` is the default when no functions are present. \`auto\` is the default if functions are present.\\\\n\\",\\"nullable\\":true,\\"oneOf\\":\[{\\"description\\":\\"\`none\` means the model will not call a function and instead generates a message. \`auto\` means the model can pick between generating a message or calling a function.\\\\n\\",\\"enum\\":\[\\"none\\",\\"auto\\",\\"required\\"\],\\"type\\":\\"string\\"},{\\"$ref\\":\\"#/components/schemas/ChatCompletionFunctionCallOption\\"}\]},\\"functions\\":{\\"deprecated\\":true,\\"description\\":\\"Deprecated in favor of \`tools\`.\\\\n\\\\nA list of functions the model may generate JSON inputs for.\\\\n\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionFunctions\\"},\\"maxItems\\":128,\\"nullable\\":true,\\"type\\":\\"array\\"},\\"include\_domains\\":{\\"deprecated\\":true,\\"description\\":\\"Deprecated: Use search\_settings.include\_domains instead.\\\\nA list of domains to include in the search results when the model uses a web search tool.\\\\n\\",\\"items\\":{\\"type\\":\\"string\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"include\_reasoning\\":{\\"description\\":\\"Whether to include reasoning in the response. If true, the response will include a \`reasoning\` field. If false, the model's reasoning will not be included in the response.\\\\nThis field is mutually exclusive with \`reasoning\_format\`.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"logit\_bias\\":{\\"additionalProperties\\":{\\"type\\":\\"integer\\"},\\"description\\":\\"This is not yet supported by any of our models.\\\\nModify the likelihood of specified tokens appearing in the completion.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"object\\"},\\"logprobs\\":{\\"default\\":false,\\"description\\":\\"This is not yet supported by any of our models.\\\\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the \`content\` of \`message\`.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"max\_completion\_tokens\\":{\\"description\\":\\"The maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.\\",\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"max\_tokens\\":{\\"deprecated\\":true,\\"description\\":\\"Deprecated in favor of \`max\_completion\_tokens\`.\\\\nThe maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"messages\\":{\\"description\\":\\"A list of messages comprising the conversation so far.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionRequestMessage\\"},\\"minItems\\":1,\\"type\\":\\"array\\"},\\"metadata\\":{\\"additionalProperties\\":{\\"type\\":\\"string\\"},\\"description\\":\\"This parameter is not currently supported.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"object\\"},\\"model\\":{\\"anyOf\\":\[{\\"type\\":\\"string\\"},{\\"enum\\":\[\\"compound-beta\\",\\"compound-beta-mini\\",\\"gemma2-9b-it\\",\\"llama-3.1-8b-instant\\",\\"llama-3.3-70b-versatile\\",\\"meta-llama/llama-4-maverick-17b-128e-instruct\\",\\"meta-llama/llama-4-scout-17b-16e-instruct\\",\\"meta-llama/llama-guard-4-12b\\",\\"moonshotai/kimi-k2-instruct\\",\\"openai/gpt-oss-120b\\",\\"openai/gpt-oss-20b\\",\\"qwen/qwen3-32b\\"\],\\"type\\":\\"string\\"}\],\\"description\\":\\"ID of the model to use. For details on which models are compatible with the Chat API, see available \[models\](https://console.groq.com/docs/models)\\",\\"example\\":\\"meta-llama/llama-4-scout-17b-16e-instruct\\"},\\"n\\":{\\"default\\":1,\\"description\\":\\"How many chat completion choices to generate for each input message. Note that the current moment, only n=1 is supported. Other values will result in a 400 response.\\",\\"example\\":1,\\"maximum\\":1,\\"minimum\\":1,\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"parallel\_tool\_calls\\":{\\"default\\":true,\\"description\\":\\"Whether to enable parallel function calling during tool use.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"presence\_penalty\\":{\\"default\\":0,\\"description\\":\\"This is not yet supported by any of our models. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\\",\\"maximum\\":2,\\"minimum\\":-2,\\"nullable\\":true,\\"type\\":\\"number\\"},\\"raw\\":{\\"default\\":false,\\"description\\":\\"Skip all post processing such as stop sequences, reasoning parsing, tool parsing, json validation, etc\\",\\"nullable\\":true,\\"type\\":\\"boolean\\",\\"x-groq-meta\\":{\\"hidden\\":true}},\\"reasoning\_effort\\":{\\"description\\":\\"qwen3 models support the following values\\\\nSet to 'none' to disable reasoning.\\\\nSet to 'default' or null to let Qwen reason.\\\\n\\\\nopenai/gpt-oss-20b and openai/gpt-oss-120b support 'low', 'medium', or 'high'.\\\\n'medium' is the default value.\\\\n\\",\\"enum\\":\[\\"none\\",\\"default\\",\\"low\\",\\"medium\\",\\"high\\"\],\\"nullable\\":true,\\"type\\":\\"string\\"},\\"reasoning\_format\\":{\\"description\\":\\"Specifies how to output reasoning tokens\\\\nThis field is mutually exclusive with \`include\_reasoning\`.\\\\n\\",\\"enum\\":\[\\"hidden\\",\\"raw\\",\\"parsed\\"\],\\"nullable\\":true,\\"type\\":\\"string\\"},\\"response\_format\\":{\\"description\\":\\"An object specifying the format that the model must output. Setting to \`{ \\\\\\"type\\\\\\": \\\\\\"json\_schema\\\\\\", \\\\\\"json\_schema\\\\\\": {...} }\` enables Structured Outputs which ensures the model will match your supplied JSON schema. \`json\_schema\` response format is only available on \[supported models\](https://console.groq.com/docs/structured-outputs#supported-models). Setting to \`{ \\\\\\"type\\\\\\": \\\\\\"json\_object\\\\\\" }\` enables the older JSON mode, which ensures the message the model generates is valid JSON. Using \`json\_schema\` is preferred for models that support it.\\\\n\\",\\"nullable\\":true,\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ResponseFormatText\\"},{\\"$ref\\":\\"#/components/schemas/ResponseFormatJsonSchema\\"},{\\"$ref\\":\\"#/components/schemas/ResponseFormatJsonObject\\"}\]},\\"search\_settings\\":{\\"description\\":\\"Settings for web search functionality when the model uses a web search tool.\\\\n\\",\\"nullable\\":true,\\"properties\\":{\\"country\\":{\\"description\\":\\"Name of country to prioritize search results from (e.g., \\\\\\"united states\\\\\\", \\\\\\"germany\\\\\\", \\\\\\"france\\\\\\").\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"exclude\_domains\\":{\\"description\\":\\"A list of domains to exclude from the search results.\\",\\"items\\":{\\"type\\":\\"string\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"include\_domains\\":{\\"description\\":\\"A list of domains to include in the search results.\\",\\"items\\":{\\"type\\":\\"string\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"include\_images\\":{\\"description\\":\\"Whether to include images in the search results.\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"}},\\"type\\":\\"object\\"},\\"seed\\":{\\"description\\":\\"If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same \`seed\` and parameters should return the same result.\\\\nDeterminism is not guaranteed, and you should refer to the \`system\_fingerprint\` response parameter to monitor changes in the backend.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"service\_tier\\":{\\"description\\":\\"The service tier to use for the request. Defaults to \`on\_demand\`.\\\\n- \`auto\` will automatically select the highest tier available within the rate limits of your organization.\\\\n- \`flex\` uses the flex tier, which will succeed or fail quickly.\\\\n\\",\\"enum\\":\[\\"auto\\",\\"on\_demand\\",\\"flex\\",\\"performance\\",null\],\\"nullable\\":true,\\"type\\":\\"string\\"},\\"stop\\":{\\"description\\":\\"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\\\\n\\",\\"nullable\\":true,\\"oneOf\\":\[{\\"example\\":\\"\\\\n\\",\\"nullable\\":true,\\"type\\":\\"string\\"},{\\"items\\":{\\"example\\":\\"\[\\\\\\"\\\\\\\\n\\\\\\"\]\\",\\"type\\":\\"string\\"},\\"maxItems\\":4,\\"type\\":\\"array\\"}\]},\\"store\\":{\\"description\\":\\"This parameter is not currently supported.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"stream\\":{\\"default\\":false,\\"description\\":\\"If set, partial message deltas will be sent. Tokens will be sent as data-only \[server-sent events\](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent\_events/Using\_server-sent\_events#Event\_stream\_format) as they become available, with the stream terminated by a \`data: \[DONE\]\` message. \[Example code\](/docs/text-chat#streaming-a-chat-completion).\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"stream\_options\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionStreamOptions\\"},\\"temperature\\":{\\"default\\":1,\\"description\\":\\"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top\_p but not both.\\",\\"example\\":1,\\"maximum\\":2,\\"minimum\\":0,\\"nullable\\":true,\\"type\\":\\"number\\"},\\"tool\_choice\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionToolChoiceOption\\"},\\"tools\\":{\\"description\\":\\"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\\\\n\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionTool\\"},\\"maxItems\\":128,\\"nullable\\":true,\\"type\\":\\"array\\"},\\"top\_logprobs\\":{\\"description\\":\\"This is not yet supported by any of our models.\\\\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. \`logprobs\` must be set to \`true\` if this parameter is used.\\\\n\\",\\"maximum\\":20,\\"minimum\\":0,\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"top\_p\\":{\\"default\\":1,\\"description\\":\\"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.\\",\\"example\\":1,\\"maximum\\":1,\\"minimum\\":0,\\"nullable\\":true,\\"type\\":\\"number\\"},\\"user\\":{\\"description\\":\\"A unique identifier representing your end-user, which can help us monitor and detect abuse.\\",\\"nullable\\":true,\\"type\\":\\"string\\"}},\\"required\\":\[\\"model\\",\\"messages\\"\],\\"type\\":\\"object\\"},\\"CreateChatCompletionResponse\\":{\\"description\\":\\"Represents a chat completion response returned by model, based on the provided input.\\",\\"properties\\":{\\"choices\\":{\\"description\\":\\"A list of chat completion choices. Can be more than one if \`n\` is greater than 1.\\",\\"items\\":{\\"properties\\":{\\"finish\_reason\\":{\\"description\\":\\"The reason the model stopped generating tokens. This will be \`stop\` if the model hit a natural stop point or a provided stop sequence,\\\\n\`length\` if the maximum number of tokens specified in the request was reached,\\\\n\`tool\_calls\` if the model called a tool, or \`function\_call\` (deprecated) if the model called a function.\\\\n\\",\\"enum\\":\[\\"stop\\",\\"length\\",\\"tool\_calls\\",\\"function\_call\\"\],\\"type\\":\\"string\\"},\\"index\\":{\\"description\\":\\"The index of the choice in the list of choices.\\",\\"type\\":\\"integer\\"},\\"logprobs\\":{\\"description\\":\\"Log probability information for the choice.\\",\\"nullable\\":true,\\"properties\\":{\\"content\\":{\\"description\\":\\"A list of message content tokens with log probability information.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionTokenLogprob\\"},\\"nullable\\":true,\\"type\\":\\"array\\"}},\\"required\\":\[\\"content\\"\],\\"type\\":\\"object\\"},\\"message\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionResponseMessage\\"}},\\"required\\":\[\\"finish\_reason\\",\\"index\\",\\"message\\",\\"logprobs\\"\],\\"type\\":\\"object\\"},\\"type\\":\\"array\\"},\\"created\\":{\\"description\\":\\"The Unix timestamp (in seconds) of when the chat completion was created.\\",\\"type\\":\\"integer\\"},\\"id\\":{\\"description\\":\\"A unique identifier for the chat completion.\\",\\"type\\":\\"string\\"},\\"model\\":{\\"description\\":\\"The model used for the chat completion.\\",\\"type\\":\\"string\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \`chat.completion\`.\\",\\"enum\\":\[\\"chat.completion\\"\],\\"type\\":\\"string\\"},\\"system\_fingerprint\\":{\\"description\\":\\"This fingerprint represents the backend configuration that the model runs with.\\\\n\\\\nCan be used in conjunction with the \`seed\` request parameter to understand when backend changes have been made that might impact determinism.\\\\n\\",\\"type\\":\\"string\\"},\\"usage\\":{\\"$ref\\":\\"#/components/schemas/CompletionUsage\\"},\\"usage\_breakdown\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionUsageBreakdown\\"}},\\"required\\":\[\\"choices\\",\\"created\\",\\"id\\",\\"model\\",\\"object\\"\],\\"type\\":\\"object\\"},\\"CreateChatCompletionStreamResponse\\":{\\"description\\":\\"Represents a streamed chunk of a chat completion response returned by model, based on the provided input.\\",\\"properties\\":{\\"choices\\":{\\"description\\":\\"A list of chat completion choices. Can contain more than one elements if \`n\` is greater than 1.\\\\n\\",\\"items\\":{\\"properties\\":{\\"delta\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionStreamResponseDelta\\"},\\"finish\_reason\\":{\\"description\\":\\"The reason the model stopped generating tokens. This will be \`stop\` if the model hit a natural stop point or a provided stop sequence,\\\\n\`length\` if the maximum number of tokens specified in the request was reached,\\\\n\`tool\_calls\` if the model called a tool, or \`function\_call\` (deprecated) if the model called a function.\\\\n\\",\\"enum\\":\[\\"stop\\",\\"length\\",\\"tool\_calls\\",\\"function\_call\\"\],\\"nullable\\":true,\\"type\\":\\"string\\"},\\"index\\":{\\"description\\":\\"The index of the choice in the list of choices.\\",\\"type\\":\\"integer\\"},\\"logprobs\\":{\\"description\\":\\"Log probability information for the choice.\\",\\"nullable\\":true,\\"properties\\":{\\"content\\":{\\"description\\":\\"A list of message content tokens with log probability information.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionTokenLogprob\\"},\\"nullable\\":true,\\"type\\":\\"array\\"}},\\"required\\":\[\\"content\\"\],\\"type\\":\\"object\\"}},\\"required\\":\[\\"delta\\",\\"finish\_reason\\",\\"index\\"\],\\"type\\":\\"object\\"},\\"type\\":\\"array\\"},\\"created\\":{\\"description\\":\\"The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.\\",\\"type\\":\\"integer\\"},\\"id\\":{\\"description\\":\\"A unique identifier for the chat completion. Each chunk has the same ID.\\",\\"type\\":\\"string\\"},\\"model\\":{\\"description\\":\\"The model to generate the completion.\\",\\"type\\":\\"string\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \`chat.completion.chunk\`.\\",\\"enum\\":\[\\"chat.completion.chunk\\"\],\\"type\\":\\"string\\"},\\"system\_fingerprint\\":{\\"description\\":\\"This fingerprint represents the backend configuration that the model runs with.\\\\nCan be used in conjunction with the \`seed\` request parameter to understand when backend changes have been made that might impact determinism.\\\\n\\",\\"type\\":\\"string\\"},\\"x\_groq\\":{\\"$ref\\":\\"#/components/schemas/XGroq\\"}},\\"required\\":\[\\"choices\\",\\"created\\",\\"id\\",\\"model\\",\\"object\\"\],\\"type\\":\\"object\\"},\\"CreateEmbeddingRequest\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"encoding\_format\\":{\\"default\\":\\"float\\",\\"description\\":\\"The format to return the embeddings in. Can only be \`float\` or \`base64\`.\\",\\"enum\\":\[\\"float\\",\\"base64\\"\],\\"example\\":\\"float\\",\\"type\\":\\"string\\"},\\"input\\":{\\"description\\":\\"Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model, cannot be an empty string, and any array must be 2048 dimensions or less.\\\\n\\",\\"example\\":\\"The quick brown fox jumped over the lazy dog\\",\\"oneOf\\":\[{\\"default\\":\\"\\",\\"description\\":\\"The string that will be turned into an embedding.\\",\\"example\\":\\"This is a test.\\",\\"title\\":\\"string\\",\\"type\\":\\"string\\"},{\\"description\\":\\"The array of strings that will be turned into an embeddings.\\",\\"items\\":{\\"default\\":\\"\\",\\"example\\":\\"\['This is a test.'\]\\",\\"type\\":\\"string\\"},\\"maxItems\\":2048,\\"minItems\\":1,\\"title\\":\\"array\\",\\"type\\":\\"array\\"}\],\\"x-groq-meta\\":{\\"validator\\":\\"EmbeddingInput\\"}},\\"model\\":{\\"anyOf\\":\[{\\"type\\":\\"string\\"},{\\"enum\\":\[\\"nomic-embed-text-v1\_5\\"\],\\"type\\":\\"string\\"}\],\\"description\\":\\"ID of the model to use.\\\\n\\",\\"example\\":\\"nomic-embed-text-v1\_5\\"},\\"user\\":{\\"description\\":\\"A unique identifier representing your end-user, which can help us monitor and detect abuse.\\",\\"nullable\\":true,\\"type\\":\\"string\\"}},\\"required\\":\[\\"model\\",\\"input\\"\],\\"type\\":\\"object\\"},\\"CreateEmbeddingResponse\\":{\\"properties\\":{\\"data\\":{\\"description\\":\\"The list of embeddings generated by the model.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/Embedding\\"},\\"type\\":\\"array\\"},\\"model\\":{\\"description\\":\\"The name of the model used to generate the embedding.\\",\\"type\\":\\"string\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \\\\\\"list\\\\\\".\\",\\"enum\\":\[\\"list\\"\],\\"type\\":\\"string\\"},\\"usage\\":{\\"description\\":\\"The usage information for the request.\\",\\"properties\\":{\\"prompt\_tokens\\":{\\"description\\":\\"The number of tokens used by the prompt.\\",\\"type\\":\\"integer\\"},\\"total\_tokens\\":{\\"description\\":\\"The total number of tokens used by the request.\\",\\"type\\":\\"integer\\"}},\\"required\\":\[\\"prompt\_tokens\\",\\"total\_tokens\\"\],\\"type\\":\\"object\\"}},\\"required\\":\[\\"object\\",\\"model\\",\\"data\\",\\"usage\\"\],\\"type\\":\\"object\\"},\\"CreateFileRequest\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"file\\":{\\"description\\":\\"The File object (not file name) to be uploaded.\\\\n\\",\\"format\\":\\"binary\\",\\"type\\":\\"string\\"},\\"purpose\\":{\\"description\\":\\"The intended purpose of the uploaded file.\\\\nUse \\\\\\"batch\\\\\\" for \[Batch API\](/docs/api-reference#batches).\\\\n\\",\\"enum\\":\[\\"batch\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"file\\",\\"purpose\\"\],\\"type\\":\\"object\\"},\\"CreateFineTuningRequest\\":{\\"properties\\":{\\"base\_model\\":{\\"description\\":\\"BaseModel is the model that the fine tune was originally trained on.\\\\n\\",\\"type\\":\\"string\\"},\\"input\_file\_id\\":{\\"description\\":\\"InputFileID is the id of the file that was uploaded via the /files api.\\\\n\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"Name is the given name to a fine tuned model.\\\\n\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"Type is the type of fine tuning format such as \\\\\\"lora\\\\\\".\\\\n\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"CreateResponseRequest\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"input\\":{\\"description\\":\\"Text input to the model, used to generate a response.\\\\n\\",\\"oneOf\\":\[{\\"description\\":\\"A text input to the model, equivalent to a text input with the \`user\` role.\\",\\"title\\":\\"Text input\\",\\"type\\":\\"string\\"},{\\"description\\":\\"A list of one or many input items to the model, containing different content types.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ResponseInputItem\\"},\\"title\\":\\"Input item list\\",\\"type\\":\\"array\\"}\]},\\"instructions\\":{\\"description\\":\\"Inserts a system (or developer) message as the first item in the model's context.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"max\_output\_tokens\\":{\\"description\\":\\"An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"metadata\\":{\\"additionalProperties\\":{\\"type\\":\\"string\\"},\\"description\\":\\"Custom key-value pairs for storing additional information. Maximum of 16 pairs.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"object\\"},\\"model\\":{\\"anyOf\\":\[{\\"type\\":\\"string\\"},{\\"enum\\":\[\\"gemma2-9b-it\\",\\"llama-3.3-70b-versatile\\",\\"llama-3.1-8b-instant\\",\\"llama-guard-3-8b\\",\\"llama3-70b-8192\\",\\"llama3-8b-8192\\"\],\\"type\\":\\"string\\"}\],\\"description\\":\\"ID of the model to use. For details on which models are compatible with the Responses API, see available \[models\](https://console.groq.com/docs/models)\\",\\"example\\":\\"llama-3.3-70b-versatile\\"},\\"parallel\_tool\_calls\\":{\\"default\\":true,\\"description\\":\\"Enable parallel execution of multiple tool calls.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"reasoning\\":{\\"description\\":\\"Configuration for reasoning capabilities when using compatible models.\\\\n\\",\\"nullable\\":true,\\"properties\\":{\\"effort\\":{\\"default\\":\\"medium\\",\\"description\\":\\"Level of reasoning effort. Supported values: \`low\`, \`medium\`, \`high\`. Lower values provide faster responses with less reasoning depth.\\\\n\\",\\"enum\\":\[\\"low\\",\\"medium\\",\\"high\\"\],\\"nullable\\":true,\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"service\_tier\\":{\\"default\\":\\"auto\\",\\"description\\":\\"Specifies the latency tier to use for processing the request.\\\\n\\",\\"enum\\":\[\\"auto\\",\\"default\\",\\"flex\\"\],\\"nullable\\":true,\\"type\\":\\"string\\"},\\"store\\":{\\"default\\":false,\\"description\\":\\"Response storage flag. Note: Currently only supports false or null values.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"stream\\":{\\"default\\":false,\\"description\\":\\"Enable streaming mode to receive response data as server-sent events.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"temperature\\":{\\"default\\":1,\\"description\\":\\"Controls randomness in the response generation. Range: 0 to 2. Lower values produce more deterministic outputs, higher values increase variety and creativity.\\\\n\\",\\"example\\":1,\\"maximum\\":2,\\"minimum\\":0,\\"nullable\\":true,\\"type\\":\\"number\\"},\\"text\\":{\\"description\\":\\"Response format configuration. Supports plain text or structured JSON output.\\\\n\\",\\"properties\\":{\\"format\\":{\\"$ref\\":\\"#/components/schemas/ResponseFormatConfiguration\\"}},\\"type\\":\\"object\\"},\\"tool\_choice\\":{\\"$ref\\":\\"#/components/schemas/ResponseToolChoiceOption\\"},\\"tools\\":{\\"description\\":\\"List of tools available to the model. Currently supports function definitions only. Maximum of 128 functions.\\\\n\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ResponseTool\\"},\\"maxItems\\":128,\\"nullable\\":true,\\"type\\":\\"array\\"},\\"top\_p\\":{\\"default\\":1,\\"description\\":\\"Nucleus sampling parameter that controls the cumulative probability cutoff. Range: 0 to 1. A value of 0.1 restricts sampling to tokens within the top 10% probability mass.\\\\n\\",\\"example\\":1,\\"maximum\\":1,\\"minimum\\":0,\\"nullable\\":true,\\"type\\":\\"number\\"},\\"truncation\\":{\\"default\\":\\"disabled\\",\\"description\\":\\"Context truncation strategy. Supported values: \`auto\` or \`disabled\`.\\\\n\\",\\"enum\\":\[\\"auto\\",\\"disabled\\"\],\\"nullable\\":true,\\"type\\":\\"string\\"},\\"user\\":{\\"description\\":\\"Optional identifier for tracking end-user requests. Useful for usage monitoring and compliance.\\\\n\\",\\"example\\":\\"user-1234\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"model\\",\\"input\\"\],\\"type\\":\\"object\\"},\\"CreateResponseResponse\\":{\\"description\\":\\"Represents a response returned by model, based on the provided input.\\",\\"properties\\":{\\"background\\":{\\"default\\":false,\\"description\\":\\"Whether the response was generated in the background.\\",\\"type\\":\\"boolean\\"},\\"created\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) of when the response was created.\\",\\"type\\":\\"integer\\"},\\"error\\":{\\"description\\":\\"An error object if the response failed.\\",\\"nullable\\":true,\\"properties\\":{\\"code\\":{\\"description\\":\\"The error code.\\",\\"type\\":\\"string\\"},\\"message\\":{\\"description\\":\\"A human-readable error message.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"code\\",\\"message\\"\],\\"type\\":\\"object\\"},\\"id\\":{\\"description\\":\\"A unique identifier for the response.\\",\\"type\\":\\"string\\"},\\"incomplete\_details\\":{\\"description\\":\\"Details about why the response is incomplete.\\",\\"nullable\\":true,\\"properties\\":{\\"reason\\":{\\"description\\":\\"The reason why the response is incomplete.\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"instructions\\":{\\"description\\":\\"The system instructions used for the response.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"max\_output\_tokens\\":{\\"description\\":\\"The maximum number of tokens configured for the response.\\",\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"max\_tool\_calls\\":{\\"description\\":\\"The maximum number of tool calls allowed.\\",\\"nullable\\":true,\\"type\\":\\"integer\\"},\\"metadata\\":{\\"additionalProperties\\":{\\"type\\":\\"string\\"},\\"description\\":\\"Metadata attached to the response.\\",\\"nullable\\":true,\\"type\\":\\"object\\"},\\"model\\":{\\"description\\":\\"The model used for the response.\\",\\"type\\":\\"string\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \`response\`.\\",\\"enum\\":\[\\"response\\"\],\\"type\\":\\"string\\"},\\"output\\":{\\"description\\":\\"An array of content items generated by the model.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ResponseOutputItem\\"},\\"type\\":\\"array\\"},\\"parallel\_tool\_calls\\":{\\"description\\":\\"Whether the model can run tool calls in parallel.\\",\\"type\\":\\"boolean\\"},\\"previous\_response\_id\\":{\\"description\\":\\"Not supported. Always null.\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"reasoning\\":{\\"description\\":\\"Configuration options for reasoning models.\\",\\"nullable\\":true,\\"properties\\":{\\"effort\\":{\\"description\\":\\"The reasoning effort level used.\\",\\"enum\\":\[\\"low\\",\\"medium\\",\\"high\\"\],\\"nullable\\":true,\\"type\\":\\"string\\"},\\"summary\\":{\\"description\\":\\"Not supported. Always null.\\",\\"nullable\\":true,\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"service\_tier\\":{\\"description\\":\\"The service tier used for processing.\\",\\"enum\\":\[\\"auto\\",\\"default\\",\\"flex\\"\],\\"type\\":\\"string\\"},\\"status\\":{\\"description\\":\\"The status of the response generation. One of \`completed\`, \`failed\`, \`in\_progress\`, or \`incomplete\`.\\\\n\\",\\"enum\\":\[\\"completed\\",\\"failed\\",\\"in\_progress\\",\\"incomplete\\"\],\\"type\\":\\"string\\"},\\"store\\":{\\"description\\":\\"Whether the response was stored.\\",\\"type\\":\\"boolean\\"},\\"temperature\\":{\\"description\\":\\"The sampling temperature used.\\",\\"type\\":\\"number\\"},\\"text\\":{\\"description\\":\\"Text format configuration used for the response.\\",\\"properties\\":{\\"format\\":{\\"$ref\\":\\"#/components/schemas/ResponseFormatConfiguration\\"}},\\"type\\":\\"object\\"},\\"tool\_choice\\":{\\"$ref\\":\\"#/components/schemas/ResponseToolChoiceOption\\"},\\"tools\\":{\\"description\\":\\"The tools that were available to the model.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ResponseTool\\"},\\"type\\":\\"array\\"},\\"top\_logprobs\\":{\\"default\\":0,\\"description\\":\\"The number of top log probabilities returned.\\",\\"type\\":\\"integer\\"},\\"top\_p\\":{\\"description\\":\\"The nucleus sampling parameter used.\\",\\"type\\":\\"number\\"},\\"truncation\\":{\\"description\\":\\"The truncation strategy used.\\",\\"enum\\":\[\\"auto\\",\\"disabled\\"\],\\"type\\":\\"string\\"},\\"usage\\":{\\"$ref\\":\\"#/components/schemas/ResponseUsage\\"},\\"user\\":{\\"description\\":\\"The user identifier.\\",\\"nullable\\":true,\\"type\\":\\"string\\"}},\\"required\\":\[\\"id\\",\\"object\\",\\"status\\",\\"created\_at\\",\\"output\\",\\"model\\",\\"tools\\",\\"tool\_choice\\",\\"truncation\\",\\"metadata\\",\\"temperature\\",\\"top\_p\\",\\"service\_tier\\",\\"error\\",\\"incomplete\_details\\",\\"parallel\_tool\_calls\\",\\"store\\"\],\\"type\\":\\"object\\"},\\"CreateSpeechRequest\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"input\\":{\\"description\\":\\"The text to generate audio for.\\",\\"example\\":\\"The quick brown fox jumped over the lazy dog\\",\\"type\\":\\"string\\"},\\"model\\":{\\"anyOf\\":\[{\\"type\\":\\"string\\"},{\\"enum\\":\[\\"playai-tts\\",\\"playai-tts-arabic\\"\],\\"type\\":\\"string\\"}\],\\"description\\":\\"One of the \[available TTS models\](/docs/text-to-speech).\\\\n\\",\\"example\\":\\"playai-tts\\"},\\"response\_format\\":{\\"default\\":\\"mp3\\",\\"description\\":\\"The format of the generated audio. Supported formats are \`flac, mp3, mulaw, ogg, wav\`.\\",\\"enum\\":\[\\"flac\\",\\"mp3\\",\\"mulaw\\",\\"ogg\\",\\"wav\\"\],\\"type\\":\\"string\\"},\\"sample\_rate\\":{\\"default\\":48000,\\"description\\":\\"The sample rate for generated audio\\",\\"enum\\":\[8000,16000,22050,24000,32000,44100,48000\],\\"example\\":48000,\\"type\\":\\"integer\\"},\\"speed\\":{\\"default\\":1,\\"description\\":\\"The speed of the generated audio.\\",\\"example\\":1,\\"maximum\\":5,\\"minimum\\":0.5,\\"type\\":\\"number\\"},\\"voice\\":{\\"description\\":\\"The voice to use when generating the audio. List of voices can be found \[here\](/docs/text-to-speech).\\",\\"example\\":\\"Fritz-PlayAI\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"model\\",\\"input\\",\\"voice\\"\],\\"type\\":\\"object\\"},\\"CreateTranscriptionRequest\\":{\\"additionalProperties\\":false,\\"oneOf\\":\[{\\"required\\":\[\\"file\\"\]},{\\"required\\":\[\\"url\\"\]}\],\\"properties\\":{\\"file\\":{\\"description\\":\\"The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\\\\nEither a file or a URL must be provided. Note that the file field is not supported in Batch API requests.\\\\n\\",\\"format\\":\\"binary\\",\\"type\\":\\"string\\"},\\"language\\":{\\"anyOf\\":\[{\\"type\\":\\"string\\"},{\\"enum\\":\[\\"en\\",\\"zh\\",\\"de\\",\\"es\\",\\"ru\\",\\"ko\\",\\"fr\\",\\"ja\\",\\"pt\\",\\"tr\\",\\"pl\\",\\"ca\\",\\"nl\\",\\"ar\\",\\"sv\\",\\"it\\",\\"id\\",\\"hi\\",\\"fi\\",\\"vi\\",\\"he\\",\\"uk\\",\\"el\\",\\"ms\\",\\"cs\\",\\"ro\\",\\"da\\",\\"hu\\",\\"ta\\",\\"no\\",\\"th\\",\\"ur\\",\\"hr\\",\\"bg\\",\\"lt\\",\\"la\\",\\"mi\\",\\"ml\\",\\"cy\\",\\"sk\\",\\"te\\",\\"fa\\",\\"lv\\",\\"bn\\",\\"sr\\",\\"az\\",\\"sl\\",\\"kn\\",\\"et\\",\\"mk\\",\\"br\\",\\"eu\\",\\"is\\",\\"hy\\",\\"ne\\",\\"mn\\",\\"bs\\",\\"kk\\",\\"sq\\",\\"sw\\",\\"gl\\",\\"mr\\",\\"pa\\",\\"si\\",\\"km\\",\\"sn\\",\\"yo\\",\\"so\\",\\"af\\",\\"oc\\",\\"ka\\",\\"be\\",\\"tg\\",\\"sd\\",\\"gu\\",\\"am\\",\\"yi\\",\\"lo\\",\\"uz\\",\\"fo\\",\\"ht\\",\\"ps\\",\\"tk\\",\\"nn\\",\\"mt\\",\\"sa\\",\\"lb\\",\\"my\\",\\"bo\\",\\"tl\\",\\"mg\\",\\"as\\",\\"tt\\",\\"haw\\",\\"ln\\",\\"ha\\",\\"ba\\",\\"jv\\",\\"su\\",\\"yue\\"\],\\"type\\":\\"string\\"}\],\\"description\\":\\"The language of the input audio. Supplying the input language in \[ISO-639-1\](https://en.wikipedia.org/wiki/List\_of\_ISO\_639-1\_codes) format will improve accuracy and latency.\\\\n\\"},\\"model\\":{\\"anyOf\\":\[{\\"type\\":\\"string\\"},{\\"enum\\":\[\\"whisper-large-v3\\",\\"whisper-large-v3-turbo\\"\],\\"type\\":\\"string\\"}\],\\"description\\":\\"ID of the model to use. \`whisper-large-v3\` and \`whisper-large-v3-turbo\` are currently available.\\\\n\\",\\"example\\":\\"whisper-large-v3-turbo\\"},\\"prompt\\":{\\"description\\":\\"An optional text to guide the model's style or continue a previous audio segment. The \[prompt\](/docs/speech-text) should match the audio language.\\\\n\\",\\"type\\":\\"string\\"},\\"response\_format\\":{\\"default\\":\\"json\\",\\"description\\":\\"The format of the transcript output, in one of these options: \`json\`, \`text\`, or \`verbose\_json\`.\\\\n\\",\\"enum\\":\[\\"json\\",\\"text\\",\\"verbose\_json\\"\],\\"type\\":\\"string\\"},\\"temperature\\":{\\"default\\":0,\\"description\\":\\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use \[log probability\](https://en.wikipedia.org/wiki/Log\_probability) to automatically increase the temperature until certain thresholds are hit.\\\\n\\",\\"type\\":\\"number\\"},\\"timestamp\_granularities\[\]\\":{\\"default\\":\[\\"segment\\"\],\\"description\\":\\"The timestamp granularities to populate for this transcription. \`response\_format\` must be set \`verbose\_json\` to use timestamp granularities. Either or both of these options are supported: \`word\`, or \`segment\`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.\\\\n\\",\\"items\\":{\\"enum\\":\[\\"word\\",\\"segment\\"\],\\"type\\":\\"string\\"},\\"type\\":\\"array\\"},\\"url\\":{\\"description\\":\\"The audio URL to translate/transcribe (supports Base64URL).\\\\nEither a file or a URL must be provided. For Batch API requests, the URL field is required since the file field is not supported.\\\\n\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"model\\"\],\\"type\\":\\"object\\"},\\"CreateTranscriptionResponseJson\\":{\\"description\\":\\"Represents a transcription response returned by model, based on the provided input.\\",\\"properties\\":{\\"text\\":{\\"description\\":\\"The transcribed text.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"text\\"\],\\"type\\":\\"object\\"},\\"CreateTranscriptionResponseVerboseJson\\":{\\"description\\":\\"Represents a verbose json transcription response returned by model, based on the provided input.\\",\\"properties\\":{\\"duration\\":{\\"description\\":\\"The duration of the input audio.\\",\\"type\\":\\"string\\"},\\"language\\":{\\"description\\":\\"The language of the input audio.\\",\\"type\\":\\"string\\"},\\"segments\\":{\\"description\\":\\"Segments of the transcribed text and their corresponding details.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/TranscriptionSegment\\"},\\"type\\":\\"array\\"},\\"text\\":{\\"description\\":\\"The transcribed text.\\",\\"type\\":\\"string\\"},\\"words\\":{\\"description\\":\\"Extracted words and their corresponding timestamps.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/TranscriptionWord\\"},\\"type\\":\\"array\\"}},\\"required\\":\[\\"language\\",\\"duration\\",\\"text\\"\],\\"type\\":\\"object\\"},\\"CreateTranslationRequest\\":{\\"additionalProperties\\":false,\\"oneOf\\":\[{\\"required\\":\[\\"file\\"\]},{\\"required\\":\[\\"url\\"\]}\],\\"properties\\":{\\"file\\":{\\"description\\":\\"The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\\\\n\\",\\"format\\":\\"binary\\",\\"type\\":\\"string\\"},\\"model\\":{\\"anyOf\\":\[{\\"type\\":\\"string\\"},{\\"enum\\":\[\\"whisper-large-v3\\",\\"whisper-large-v3-turbo\\"\],\\"type\\":\\"string\\"}\],\\"description\\":\\"ID of the model to use. \`whisper-large-v3\` and \`whisper-large-v3-turbo\` are currently available.\\\\n\\",\\"example\\":\\"whisper-large-v3-turbo\\"},\\"prompt\\":{\\"description\\":\\"An optional text to guide the model's style or continue a previous audio segment. The \[prompt\](/docs/guides/speech-to-text/prompting) should be in English.\\\\n\\",\\"type\\":\\"string\\"},\\"response\_format\\":{\\"default\\":\\"json\\",\\"description\\":\\"The format of the transcript output, in one of these options: \`json\`, \`text\`, or \`verbose\_json\`.\\\\n\\",\\"enum\\":\[\\"json\\",\\"text\\",\\"verbose\_json\\"\],\\"type\\":\\"string\\"},\\"temperature\\":{\\"default\\":0,\\"description\\":\\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use \[log probability\](https://en.wikipedia.org/wiki/Log\_probability) to automatically increase the temperature until certain thresholds are hit.\\\\n\\",\\"type\\":\\"number\\"},\\"url\\":{\\"description\\":\\"The audio URL to translate/transcribe (supports Base64URL). Either file or url must be provided.\\\\nWhen using the Batch API only url is supported.\\\\n\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"model\\"\],\\"type\\":\\"object\\"},\\"CreateTranslationResponseJson\\":{\\"properties\\":{\\"text\\":{\\"type\\":\\"string\\"}},\\"required\\":\[\\"text\\"\],\\"type\\":\\"object\\"},\\"CreateTranslationResponseVerboseJson\\":{\\"properties\\":{\\"duration\\":{\\"description\\":\\"The duration of the input audio.\\",\\"type\\":\\"string\\"},\\"language\\":{\\"description\\":\\"The language of the output translation (always \`english\`).\\",\\"type\\":\\"string\\"},\\"segments\\":{\\"description\\":\\"Segments of the translated text and their corresponding details.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/TranscriptionSegment\\"},\\"type\\":\\"array\\"},\\"text\\":{\\"description\\":\\"The translated text.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"language\\",\\"duration\\",\\"text\\"\],\\"type\\":\\"object\\"},\\"DeleteFileResponse\\":{\\"properties\\":{\\"deleted\\":{\\"type\\":\\"boolean\\"},\\"id\\":{\\"type\\":\\"string\\"},\\"object\\":{\\"enum\\":\[\\"file\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"id\\",\\"object\\",\\"deleted\\"\],\\"type\\":\\"object\\"},\\"DeleteFineTuningResponse\\":{\\"properties\\":{\\"deleted\\":{\\"type\\":\\"boolean\\"},\\"id\\":{\\"type\\":\\"string\\"},\\"object\\":{\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"DeleteModelResponse\\":{\\"properties\\":{\\"deleted\\":{\\"type\\":\\"boolean\\"},\\"id\\":{\\"type\\":\\"string\\"},\\"object\\":{\\"type\\":\\"string\\"}},\\"required\\":\[\\"id\\",\\"object\\",\\"deleted\\"\],\\"type\\":\\"object\\"},\\"DocumentCitation\\":{\\"additionalProperties\\":false,\\"description\\":\\"A citation referencing a specific document that was provided in the request.\\",\\"properties\\":{\\"document\_id\\":{\\"description\\":\\"The ID of the document being cited, corresponding to a document provided in the request.\\",\\"type\\":\\"string\\"},\\"end\_index\\":{\\"description\\":\\"The character index in the message content where this citation ends.\\",\\"type\\":\\"integer\\"},\\"start\_index\\":{\\"description\\":\\"The character index in the message content where this citation begins.\\",\\"type\\":\\"integer\\"}},\\"required\\":\[\\"start\_index\\",\\"end\_index\\",\\"document\_id\\"\],\\"type\\":\\"object\\"},\\"Embedding\\":{\\"description\\":\\"Represents an embedding vector returned by embedding endpoint.\\\\n\\",\\"properties\\":{\\"embedding\\":{\\"oneOf\\":\[{\\"description\\":\\"The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the \[embedding guide\](/docs/guides/embeddings).\\\\n\\",\\"items\\":{\\"type\\":\\"number\\"},\\"type\\":\\"array\\"},{\\"description\\":\\"The embedding vector, which is a base64 encoded string. The length of vector depends on the model as listed in the \[embedding guide\](/docs/guides/embeddings).\\\\n\\",\\"type\\":\\"string\\"}\]},\\"index\\":{\\"description\\":\\"The index of the embedding in the list of embeddings.\\",\\"type\\":\\"integer\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \\\\\\"embedding\\\\\\".\\",\\"enum\\":\[\\"embedding\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"index\\",\\"object\\",\\"embedding\\"\],\\"type\\":\\"object\\"},\\"Error\\":{\\"properties\\":{\\"code\\":{\\"nullable\\":true,\\"type\\":\\"string\\"},\\"message\\":{\\"type\\":\\"string\\"},\\"param\\":{\\"nullable\\":true,\\"type\\":\\"string\\"},\\"type\\":{\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"message\\",\\"param\\",\\"code\\"\],\\"type\\":\\"object\\"},\\"ErrorResponse\\":{\\"properties\\":{\\"error\\":{\\"$ref\\":\\"#/components/schemas/Error\\"}},\\"required\\":\[\\"error\\"\],\\"type\\":\\"object\\"},\\"File\\":{\\"description\\":\\"The \`File\` object represents a document that has been uploaded.\\",\\"properties\\":{\\"bytes\\":{\\"description\\":\\"The size of the file, in bytes.\\",\\"type\\":\\"integer\\"},\\"created\_at\\":{\\"description\\":\\"The Unix timestamp (in seconds) for when the file was created.\\",\\"type\\":\\"integer\\"},\\"filename\\":{\\"description\\":\\"The name of the file.\\",\\"type\\":\\"string\\"},\\"id\\":{\\"description\\":\\"The file identifier, which can be referenced in the API endpoints.\\",\\"type\\":\\"string\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \`file\`.\\",\\"enum\\":\[\\"file\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true},\\"purpose\\":{\\"description\\":\\"The intended purpose of the file. Supported values are \`batch\`, and \`batch\_output\`.\\",\\"enum\\":\[\\"batch\\",\\"batch\_output\\"\],\\"type\\":\\"string\\"}},\\"title\\":\\"File\\"},\\"FunctionCitation\\":{\\"additionalProperties\\":false,\\"description\\":\\"A citation referencing the result of a function or tool call.\\",\\"properties\\":{\\"end\_index\\":{\\"description\\":\\"The character index in the message content where this citation ends.\\",\\"type\\":\\"integer\\"},\\"start\_index\\":{\\"description\\":\\"The character index in the message content where this citation begins.\\",\\"type\\":\\"integer\\"},\\"tool\_call\_id\\":{\\"description\\":\\"The ID of the tool call being cited, corresponding to a tool call made during the conversation.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"start\_index\\",\\"end\_index\\",\\"tool\_call\_id\\"\],\\"type\\":\\"object\\"},\\"FunctionObject\\":{\\"properties\\":{\\"description\\":{\\"description\\":\\"A description of what the function does, used by the model to choose when and how to call the function.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\\",\\"type\\":\\"string\\"},\\"parameters\\":{\\"$ref\\":\\"#/components/schemas/FunctionParameters\\"},\\"strict\\":{\\"default\\":false,\\"description\\":\\"Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the \`schema\` field. Only a subset of JSON Schema is supported when \`strict\` is \`true\`.\\\\n\\",\\"type\\":\\"boolean\\"}},\\"required\\":\[\\"name\\"\],\\"type\\":\\"object\\"},\\"FunctionParameters\\":{\\"additionalProperties\\":true,\\"description\\":\\"Function parameters defined as a JSON Schema object. Refer to https://json-schema.org/understanding-json-schema/ for schema documentation.\\",\\"type\\":\\"object\\"},\\"ListBatchesResponse\\":{\\"properties\\":{\\"data\\":{\\"items\\":{\\"$ref\\":\\"#/components/schemas/Batch\\"},\\"type\\":\\"array\\"},\\"object\\":{\\"enum\\":\[\\"list\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"object\\",\\"data\\"\],\\"type\\":\\"object\\"},\\"ListFilesResponse\\":{\\"properties\\":{\\"data\\":{\\"items\\":{\\"$ref\\":\\"#/components/schemas/File\\"},\\"type\\":\\"array\\"},\\"object\\":{\\"enum\\":\[\\"list\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"object\\",\\"data\\"\],\\"type\\":\\"object\\"},\\"ListFineTuningsResponse\\":{\\"properties\\":{\\"data\\":{\\"items\\":{\\"properties\\":{\\"base\_model\\":{\\"description\\":\\"BaseModel is the model that the fine tune was originally trained on.\\\\n\\",\\"type\\":\\"string\\"},\\"created\_at\\":{\\"description\\":\\"CreatedAt is the timestamp of when the fine tuned model was created.\\\\n\\",\\"type\\":\\"number\\"},\\"fine\_tuned\_model\\":{\\"description\\":\\"FineTunedModel is the final name of the fine tuned model.\\\\n\\",\\"type\\":\\"string\\"},\\"id\\":{\\"description\\":\\"ID is the unique identifier of a fine tune.\\\\n\\",\\"type\\":\\"string\\"},\\"input\_file\_id\\":{\\"description\\":\\"InputFileID is the id of the file that was uploaded via the /files api.\\\\n\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"Name is the given name to a fine tuned model.\\\\n\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"Type is the type of fine tuning format such as \\\\\\"lora\\\\\\".\\\\n\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"type\\":\\"array\\"},\\"object\\":{\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"ListModelsResponse\\":{\\"properties\\":{\\"data\\":{\\"items\\":{\\"$ref\\":\\"#/components/schemas/Model\\"},\\"type\\":\\"array\\"},\\"object\\":{\\"enum\\":\[\\"list\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"object\\",\\"data\\"\],\\"type\\":\\"object\\"},\\"Model\\":{\\"description\\":\\"Describes an OpenAI model offering that can be used with the API.\\",\\"properties\\":{\\"created\\":{\\"description\\":\\"The Unix timestamp (in seconds) when the model was created.\\",\\"type\\":\\"integer\\"},\\"id\\":{\\"description\\":\\"The model identifier, which can be referenced in the API endpoints.\\",\\"type\\":\\"string\\"},\\"object\\":{\\"description\\":\\"The object type, which is always \\\\\\"model\\\\\\".\\",\\"enum\\":\[\\"model\\"\],\\"type\\":\\"string\\"},\\"owned\_by\\":{\\"description\\":\\"The organization that owns the model.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"id\\",\\"object\\",\\"created\\",\\"owned\_by\\"\],\\"title\\":\\"Model\\"},\\"ReadFineTuningResponse\\":{\\"properties\\":{\\"data\\":{\\"properties\\":{\\"base\_model\\":{\\"description\\":\\"BaseModel is the model that the fine tune was originally trained on.\\\\n\\",\\"type\\":\\"string\\"},\\"created\_at\\":{\\"description\\":\\"CreatedAt is the timestamp of when the fine tuned model was created.\\\\n\\",\\"type\\":\\"number\\"},\\"fine\_tuned\_model\\":{\\"description\\":\\"FineTunedModel is the final name of the fine tuned model.\\\\n\\",\\"type\\":\\"string\\"},\\"id\\":{\\"description\\":\\"ID is the unique identifier of a fine tune.\\\\n\\",\\"type\\":\\"string\\"},\\"input\_file\_id\\":{\\"description\\":\\"InputFileID is the id of the file that was uploaded via the /files api.\\\\n\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"Name is the given name to a fine tuned model.\\\\n\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"Type is the type of fine tuning format such as \\\\\\"lora\\\\\\".\\\\n\\",\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"id\\":{\\"type\\":\\"string\\"},\\"object\\":{\\"type\\":\\"string\\"}},\\"type\\":\\"object\\"},\\"RerankingRequest\\":{\\"additionalProperties\\":false,\\"properties\\":{\\"docs\\":{\\"description\\":\\"An array of documents to rank. Each document is a string containing the text content.\\\\nMaximum of 100 documents per request.\\\\n\\",\\"example\\":\[\\"Machine learning is a subset of artificial intelligence\\",\\"The weather forecast predicts rain tomorrow\\",\\"Deep learning uses neural networks with multiple layers\\"\],\\"items\\":{\\"minLength\\":1,\\"type\\":\\"string\\"},\\"maxItems\\":100,\\"minItems\\":1,\\"type\\":\\"array\\"},\\"instruction\\":{\\"description\\":\\"Optional instruction to guide the reranking process. If not provided, \\\\na default instruction will be used.\\\\n\\",\\"example\\":\\"Find the most relevant document about AI research\\",\\"nullable\\":true,\\"type\\":\\"string\\"},\\"model\\":{\\"description\\":\\"ID of the reranking model to use.\\\\n\\",\\"example\\":\\"qwen3-reranker-4b\\",\\"type\\":\\"string\\"},\\"query\\":{\\"description\\":\\"The search query to rank documents against.\\\\n\\",\\"example\\":\\"artificial intelligence research\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"model\\",\\"query\\",\\"docs\\"\],\\"type\\":\\"object\\"},\\"RerankingResponse\\":{\\"properties\\":{\\"results\\":{\\"description\\":\\"List of documents sorted by relevance score in descending order.\\\\nEach result contains the original document text and its relevance score.\\\\n\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/RerankingResult\\"},\\"type\\":\\"array\\"}},\\"required\\":\[\\"results\\"\],\\"type\\":\\"object\\"},\\"RerankingResult\\":{\\"properties\\":{\\"doc\\":{\\"description\\":\\"The original document text.\\",\\"example\\":\\"Machine learning is a subset of artificial intelligence\\",\\"type\\":\\"string\\"},\\"score\\":{\\"description\\":\\"Relevance score between 0.0 and 1.0, where higher scores indicate \\\\ngreater relevance to the query.\\\\n\\",\\"example\\":0.92,\\"format\\":\\"float\\",\\"maximum\\":1,\\"minimum\\":0,\\"type\\":\\"number\\"}},\\"required\\":\[\\"doc\\",\\"score\\"\],\\"type\\":\\"object\\"},\\"ResponseAnnotation\\":{\\"discriminator\\":{\\"mapping\\":{\\"file\_citation\\":\\"#/components/schemas/ResponseFileCitation\\",\\"url\_citation\\":\\"#/components/schemas/ResponseUrlCitation\\"},\\"propertyName\\":\\"type\\"},\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ResponseFileCitation\\"},{\\"$ref\\":\\"#/components/schemas/ResponseUrlCitation\\"}\]},\\"ResponseEasyInputMessage\\":{\\"description\\":\\"A message input to the model with a role indicating instruction following hierarchy.\\\\n\\",\\"properties\\":{\\"content\\":{\\"description\\":\\"Text input to the model.\\\\n\\",\\"oneOf\\":\[{\\"description\\":\\"A text input to the model.\\",\\"title\\":\\"Text input\\",\\"type\\":\\"string\\"},{\\"description\\":\\"An array of content parts.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ResponseInputContent\\"},\\"title\\":\\"Content array\\",\\"type\\":\\"array\\"}\]},\\"role\\":{\\"description\\":\\"The role of the message input. One of \`user\`, \`assistant\`, \`system\`, or \`developer\`.\\\\n\\",\\"enum\\":\[\\"user\\",\\"assistant\\",\\"system\\",\\"developer\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"role\\",\\"content\\"\],\\"title\\":\\"Easy input message\\",\\"type\\":\\"object\\"},\\"ResponseFileCitation\\":{\\"description\\":\\"A citation to a file.\\",\\"properties\\":{\\"file\_id\\":{\\"description\\":\\"The ID of the file.\\",\\"type\\":\\"string\\"},\\"index\\":{\\"description\\":\\"The index of the citation in the text.\\",\\"type\\":\\"integer\\"},\\"type\\":{\\"description\\":\\"The type of the annotation. Always \`file\_citation\`.\\",\\"enum\\":\[\\"file\_citation\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"file\_id\\"\],\\"title\\":\\"File citation\\",\\"type\\":\\"object\\"},\\"ResponseFormatConfiguration\\":{\\"description\\":\\"An object specifying the format that the model must output.\\\\n\\",\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ResponseFormatText\\"},{\\"$ref\\":\\"#/components/schemas/ResponseFormatJsonObject\\"},{\\"$ref\\":\\"#/components/schemas/TextResponseFormatJsonSchema\\"}\]},\\"ResponseFormatJsonObject\\":{\\"description\\":\\"JSON object response format. An older method of generating JSON responses. Using \`json\_schema\` is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.\\\\n\\",\\"properties\\":{\\"type\\":{\\"description\\":\\"The type of response format being defined. Always \`json\_object\`.\\",\\"enum\\":\[\\"json\_object\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\"\],\\"title\\":\\"JSON object\\",\\"type\\":\\"object\\"},\\"ResponseFormatJsonSchema\\":{\\"description\\":\\"JSON Schema response format. Used to generate structured JSON responses.\\\\n\\",\\"properties\\":{\\"json\_schema\\":{\\"description\\":\\"Structured Outputs configuration options, including a JSON Schema.\\\\n\\",\\"properties\\":{\\"description\\":{\\"description\\":\\"A description of what the response format is for, used by the model to determine how to respond in the format.\\\\n\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\\\\n\\",\\"type\\":\\"string\\"},\\"schema\\":{\\"$ref\\":\\"#/components/schemas/ResponseFormatJsonSchemaSchema\\"},\\"strict\\":{\\"default\\":false,\\"description\\":\\"Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the \`schema\` field. Only a subset of JSON Schema is supported when \`strict\` is \`true\`.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"}},\\"required\\":\[\\"name\\"\],\\"title\\":\\"JSON schema\\",\\"type\\":\\"object\\"},\\"type\\":{\\"description\\":\\"The type of response format being defined. Always \`json\_schema\`.\\",\\"enum\\":\[\\"json\_schema\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"json\_schema\\"\],\\"title\\":\\"JSON schema\\",\\"type\\":\\"object\\"},\\"ResponseFormatJsonSchemaSchema\\":{\\"additionalProperties\\":true,\\"description\\":\\"The schema for the response format, described as a JSON Schema object.\\\\nLearn how to build JSON schemas \[here\](https://json-schema.org/).\\\\n\\",\\"title\\":\\"JSON schema\\",\\"type\\":\\"object\\"},\\"ResponseFormatText\\":{\\"description\\":\\"Default response format. Used to generate text responses.\\\\n\\",\\"properties\\":{\\"type\\":{\\"description\\":\\"The type of response format being defined. Always \`text\`.\\",\\"enum\\":\[\\"text\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\"\],\\"title\\":\\"Text\\",\\"type\\":\\"object\\"},\\"ResponseFunctionCall\\":{\\"description\\":\\"A function call generated by the model.\\",\\"properties\\":{\\"arguments\\":{\\"description\\":\\"A JSON string of the arguments to pass to the function.\\",\\"type\\":\\"string\\"},\\"call\_id\\":{\\"description\\":\\"The unique ID of the function tool call generated by the model.\\",\\"type\\":\\"string\\"},\\"id\\":{\\"description\\":\\"The unique ID of the function tool call.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"},\\"status\\":{\\"description\\":\\"The status of the item.\\",\\"enum\\":\[\\"in\_progress\\",\\"completed\\",\\"incomplete\\"\],\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the function call. Always \`function\_call\`.\\",\\"enum\\":\[\\"function\_call\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"call\_id\\",\\"name\\",\\"arguments\\"\],\\"title\\":\\"Function call\\",\\"type\\":\\"object\\"},\\"ResponseFunctionCallOutput\\":{\\"description\\":\\"The output of a function tool call.\\",\\"properties\\":{\\"call\_id\\":{\\"description\\":\\"The unique ID of the function tool call generated by the model.\\",\\"type\\":\\"string\\"},\\"id\\":{\\"description\\":\\"The unique ID of the function tool call output.\\",\\"type\\":\\"string\\"},\\"output\\":{\\"description\\":\\"A JSON string of the output of the function tool call.\\",\\"type\\":\\"string\\"},\\"status\\":{\\"description\\":\\"The status of the item.\\",\\"enum\\":\[\\"in\_progress\\",\\"completed\\",\\"incomplete\\"\],\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the function tool call output. Always \`function\_call\_output\`.\\",\\"enum\\":\[\\"function\_call\_output\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"call\_id\\",\\"output\\"\],\\"title\\":\\"Function call output\\",\\"type\\":\\"object\\"},\\"ResponseInputContent\\":{\\"discriminator\\":{\\"mapping\\":{\\"input\_text\\":\\"#/components/schemas/ResponseInputTextContent\\"},\\"propertyName\\":\\"type\\"},\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ResponseInputTextContent\\"}\]},\\"ResponseInputItem\\":{\\"discriminator\\":{\\"mapping\\":{\\"function\_call\\":\\"#/components/schemas/ResponseFunctionCall\\",\\"function\_call\_output\\":\\"#/components/schemas/ResponseFunctionCallOutput\\",\\"item\_reference\\":\\"#/components/schemas/ResponseItemReference\\",\\"message\\":\\"#/components/schemas/ResponseInputMessage\\"},\\"propertyName\\":\\"type\\"},\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ResponseEasyInputMessage\\"},{\\"$ref\\":\\"#/components/schemas/ResponseInputMessage\\"},{\\"$ref\\":\\"#/components/schemas/ResponseItemReference\\"},{\\"$ref\\":\\"#/components/schemas/ResponseFunctionCall\\"},{\\"$ref\\":\\"#/components/schemas/ResponseFunctionCallOutput\\"}\]},\\"ResponseInputMessage\\":{\\"description\\":\\"A message input to the model with explicit type field.\\\\n\\",\\"properties\\":{\\"content\\":{\\"description\\":\\"A list of one or many input content items.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ResponseInputContent\\"},\\"type\\":\\"array\\"},\\"role\\":{\\"description\\":\\"The role of the message input. One of \`user\`, \`system\`, or \`developer\`. Note: assistant role is not supported with explicit type.\\\\n\\",\\"enum\\":\[\\"user\\",\\"system\\",\\"developer\\"\],\\"type\\":\\"string\\"},\\"status\\":{\\"description\\":\\"The status of item. Populated when items are returned via API.\\",\\"enum\\":\[\\"in\_progress\\",\\"completed\\",\\"incomplete\\"\],\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the message input. Always set to \`message\`.\\",\\"enum\\":\[\\"message\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"role\\",\\"content\\"\],\\"title\\":\\"Input message\\",\\"type\\":\\"object\\"},\\"ResponseInputTextContent\\":{\\"description\\":\\"A text input to the model.\\",\\"properties\\":{\\"text\\":{\\"description\\":\\"The text input to the model.\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the input item. Always \`input\_text\`.\\",\\"enum\\":\[\\"input\_text\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"text\\"\],\\"title\\":\\"Input text\\",\\"type\\":\\"object\\"},\\"ResponseItemReference\\":{\\"description\\":\\"An internal identifier for an item to reference.\\",\\"properties\\":{\\"id\\":{\\"description\\":\\"The ID of the item to reference.\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of item to reference. Always \`item\_reference\`.\\",\\"enum\\":\[\\"item\_reference\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"id\\"\],\\"title\\":\\"Item reference\\",\\"type\\":\\"object\\"},\\"ResponseNamedToolChoice\\":{\\"description\\":\\"Specifies a tool the model should use. Use to force the model to call a specific function.\\",\\"properties\\":{\\"function\\":{\\"properties\\":{\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"name\\"\],\\"type\\":\\"object\\"},\\"type\\":{\\"description\\":\\"The type of the tool. Currently, only \`function\` is supported.\\",\\"enum\\":\[\\"function\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"function\\"\],\\"type\\":\\"object\\"},\\"ResponseOutputContent\\":{\\"discriminator\\":{\\"mapping\\":{\\"output\_text\\":\\"#/components/schemas/ResponseOutputTextContent\\"},\\"propertyName\\":\\"type\\"},\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ResponseOutputTextContent\\"}\]},\\"ResponseOutputFunctionCall\\":{\\"description\\":\\"A function call generated by the model.\\",\\"properties\\":{\\"arguments\\":{\\"description\\":\\"A JSON string of the arguments to pass to the function.\\",\\"type\\":\\"string\\"},\\"call\_id\\":{\\"description\\":\\"The unique ID of the function tool call generated by the model.\\",\\"type\\":\\"string\\"},\\"id\\":{\\"description\\":\\"The unique ID of the function tool call.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to call.\\",\\"type\\":\\"string\\"},\\"status\\":{\\"description\\":\\"The status of the function call.\\",\\"enum\\":\[\\"in\_progress\\",\\"completed\\",\\"incomplete\\"\],\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the function call. Always \`function\_call\`.\\",\\"enum\\":\[\\"function\_call\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"id\\",\\"call\_id\\",\\"name\\",\\"arguments\\"\],\\"title\\":\\"Function call\\",\\"type\\":\\"object\\"},\\"ResponseOutputItem\\":{\\"discriminator\\":{\\"mapping\\":{\\"function\_call\\":\\"#/components/schemas/ResponseOutputFunctionCall\\",\\"message\\":\\"#/components/schemas/ResponseOutputMessage\\",\\"reasoning\\":\\"#/components/schemas/ResponseOutputReasoning\\"},\\"propertyName\\":\\"type\\"},\\"oneOf\\":\[{\\"$ref\\":\\"#/components/schemas/ResponseOutputMessage\\"},{\\"$ref\\":\\"#/components/schemas/ResponseOutputFunctionCall\\"},{\\"$ref\\":\\"#/components/schemas/ResponseOutputReasoning\\"}\]},\\"ResponseOutputMessage\\":{\\"description\\":\\"An output message from the model.\\",\\"properties\\":{\\"content\\":{\\"description\\":\\"The content of the output message.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ResponseOutputContent\\"},\\"type\\":\\"array\\"},\\"id\\":{\\"description\\":\\"The unique ID of the output message.\\",\\"type\\":\\"string\\"},\\"role\\":{\\"description\\":\\"The role of the output message. Always \`assistant\`.\\",\\"enum\\":\[\\"assistant\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true},\\"status\\":{\\"description\\":\\"The status of the message.\\",\\"enum\\":\[\\"in\_progress\\",\\"completed\\",\\"incomplete\\"\],\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the output message. Always \`message\`.\\",\\"enum\\":\[\\"message\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"id\\",\\"role\\",\\"content\\"\],\\"title\\":\\"Output message\\",\\"type\\":\\"object\\"},\\"ResponseOutputReasoning\\":{\\"description\\":\\"A reasoning output from the model.\\",\\"properties\\":{\\"id\\":{\\"description\\":\\"The unique ID of the reasoning output.\\",\\"type\\":\\"string\\"},\\"summary\\":{\\"description\\":\\"Summary items (currently empty).\\",\\"items\\":{\\"type\\":\\"object\\"},\\"type\\":\\"array\\"},\\"type\\":{\\"description\\":\\"The type of the reasoning output. Always \`reasoning\`.\\",\\"enum\\":\[\\"reasoning\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"id\\",\\"summary\\"\],\\"title\\":\\"Reasoning\\",\\"type\\":\\"object\\"},\\"ResponseOutputTextContent\\":{\\"description\\":\\"A text output from the model.\\",\\"properties\\":{\\"annotations\\":{\\"description\\":\\"The annotations of the text output.\\",\\"items\\":{\\"$ref\\":\\"#/components/schemas/ResponseAnnotation\\"},\\"type\\":\\"array\\"},\\"logprobs\\":{\\"description\\":\\"Log probability information for the output.\\",\\"items\\":{\\"type\\":\\"string\\"},\\"nullable\\":true,\\"type\\":\\"array\\"},\\"text\\":{\\"description\\":\\"The text output from the model.\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the output text. Always \`output\_text\`.\\",\\"enum\\":\[\\"output\_text\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"text\\",\\"annotations\\"\],\\"title\\":\\"Output text\\",\\"type\\":\\"object\\"},\\"ResponseTool\\":{\\"properties\\":{\\"description\\":{\\"description\\":\\"Describes the function's purpose. The model uses this to determine when to invoke the function.\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\\",\\"type\\":\\"string\\"},\\"parameters\\":{\\"$ref\\":\\"#/components/schemas/FunctionParameters\\"},\\"strict\\":{\\"description\\":\\"Whether to enable strict schema adherence when generating the function call.\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"type\\":{\\"description\\":\\"The type of the tool. Currently, only \`function\` is supported.\\",\\"enum\\":\[\\"function\\"\],\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"name\\"\],\\"type\\":\\"object\\"},\\"ResponseToolChoiceOption\\":{\\"description\\":\\"Controls which (if any) tool is called by the model.\\\\n\`none\` means the model will not call any tool and instead generates a message.\\\\n\`auto\` means the model can pick between generating a message or calling one or more tools.\\\\n\`required\` means the model must call one or more tools.\\\\nSpecifying a particular tool via \`{\\\\\\"type\\\\\\": \\\\\\"function\\\\\\", \\\\\\"function\\\\\\": {\\\\\\"name\\\\\\": \\\\\\"my\_function\\\\\\"}}\` forces the model to call that tool.\\\\n\\\\n\`none\` is the default when no tools are present. \`auto\` is the default if tools are present.\\\\n\\",\\"nullable\\":true,\\"oneOf\\":\[{\\"description\\":\\"\`none\` means the model will not call any tool and instead generates a message. \`auto\` means the model can pick between generating a message or calling one or more tools. \`required\` means the model must call one or more tools.\\\\n\\",\\"enum\\":\[\\"none\\",\\"auto\\",\\"required\\"\],\\"type\\":\\"string\\"},{\\"$ref\\":\\"#/components/schemas/ResponseNamedToolChoice\\"}\]},\\"ResponseUrlCitation\\":{\\"description\\":\\"A citation for a web resource.\\",\\"properties\\":{\\"end\_index\\":{\\"description\\":\\"The index of the last character of the URL citation in the message.\\",\\"type\\":\\"integer\\"},\\"start\_index\\":{\\"description\\":\\"The index of the first character of the URL citation in the message.\\",\\"type\\":\\"integer\\"},\\"title\\":{\\"description\\":\\"The title of the web resource.\\",\\"type\\":\\"string\\"},\\"type\\":{\\"description\\":\\"The type of the annotation. Always \`url\_citation\`.\\",\\"enum\\":\[\\"url\_citation\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true},\\"url\\":{\\"description\\":\\"The URL of the web resource.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"type\\",\\"url\\"\],\\"title\\":\\"URL citation\\",\\"type\\":\\"object\\"},\\"ResponseUsage\\":{\\"description\\":\\"Usage statistics for the response request.\\",\\"properties\\":{\\"input\_tokens\\":{\\"description\\":\\"Number of tokens in the input.\\",\\"type\\":\\"integer\\"},\\"input\_tokens\_details\\":{\\"description\\":\\"Breakdown of input tokens.\\",\\"properties\\":{\\"cached\_tokens\\":{\\"description\\":\\"Number of cached tokens.\\",\\"type\\":\\"integer\\"},\\"reasoning\_tokens\\":{\\"description\\":\\"Number of reasoning tokens.\\",\\"type\\":\\"integer\\"}},\\"required\\":\[\\"cached\_tokens\\"\],\\"type\\":\\"object\\"},\\"output\_tokens\\":{\\"description\\":\\"Number of tokens in the generated output.\\",\\"type\\":\\"integer\\"},\\"output\_tokens\_details\\":{\\"description\\":\\"Breakdown of output tokens.\\",\\"properties\\":{\\"cached\_tokens\\":{\\"description\\":\\"Number of cached tokens.\\",\\"type\\":\\"integer\\"},\\"reasoning\_tokens\\":{\\"description\\":\\"Number of reasoning tokens.\\",\\"type\\":\\"integer\\"}},\\"required\\":\[\\"cached\_tokens\\",\\"reasoning\_tokens\\"\],\\"type\\":\\"object\\"},\\"total\_tokens\\":{\\"description\\":\\"Total number of tokens used in the request (input + output).\\",\\"type\\":\\"integer\\"}},\\"required\\":\[\\"input\_tokens\\",\\"input\_tokens\_details\\",\\"output\_tokens\\",\\"output\_tokens\_details\\",\\"total\_tokens\\"\],\\"type\\":\\"object\\"},\\"TextResponseFormatJsonSchema\\":{\\"description\\":\\"JSON Schema response format. Used to generate structured JSON responses.\\\\n\\",\\"properties\\":{\\"description\\":{\\"description\\":\\"A description of what the response format is for, used by the model to determine how to respond in the format.\\\\n\\",\\"type\\":\\"string\\"},\\"name\\":{\\"description\\":\\"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\\\\n\\",\\"type\\":\\"string\\"},\\"schema\\":{\\"additionalProperties\\":true,\\"description\\":\\"The schema for the response format, described as a JSON Schema object.\\\\n\\",\\"type\\":\\"object\\"},\\"strict\\":{\\"default\\":false,\\"description\\":\\"Whether to enable strict schema adherence when generating the output.\\\\n\\",\\"nullable\\":true,\\"type\\":\\"boolean\\"},\\"type\\":{\\"description\\":\\"The type of response format being defined. Always \`json\_schema\`.\\",\\"enum\\":\[\\"json\_schema\\"\],\\"type\\":\\"string\\",\\"x-stainless-const\\":true}},\\"required\\":\[\\"type\\",\\"name\\",\\"schema\\"\],\\"title\\":\\"JSON schema\\",\\"type\\":\\"object\\"},\\"TranscriptionSegment\\":{\\"properties\\":{\\"avg\_logprob\\":{\\"description\\":\\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed.\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"compression\_ratio\\":{\\"description\\":\\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed.\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"end\\":{\\"description\\":\\"End time of the segment in seconds.\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"id\\":{\\"description\\":\\"Unique identifier of the segment.\\",\\"type\\":\\"integer\\"},\\"no\_speech\_prob\\":{\\"description\\":\\"Probability of no speech in the segment. If the value is higher than 1.0 and the \`avg\_logprob\` is below -1, consider this segment silent.\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"seek\\":{\\"description\\":\\"Seek offset of the segment.\\",\\"type\\":\\"integer\\"},\\"start\\":{\\"description\\":\\"Start time of the segment in seconds.\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"temperature\\":{\\"description\\":\\"Temperature parameter used for generating the segment.\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"text\\":{\\"description\\":\\"Text content of the segment.\\",\\"type\\":\\"string\\"},\\"tokens\\":{\\"description\\":\\"Array of token IDs for the text content.\\",\\"items\\":{\\"type\\":\\"integer\\"},\\"type\\":\\"array\\"}},\\"required\\":\[\\"id\\",\\"seek\\",\\"start\\",\\"end\\",\\"text\\",\\"tokens\\",\\"temperature\\",\\"avg\_logprob\\",\\"compression\_ratio\\",\\"no\_speech\_prob\\"\],\\"type\\":\\"object\\"},\\"TranscriptionWord\\":{\\"properties\\":{\\"end\\":{\\"description\\":\\"End time of the word in seconds.\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"start\\":{\\"description\\":\\"Start time of the word in seconds.\\",\\"format\\":\\"float\\",\\"type\\":\\"number\\"},\\"word\\":{\\"description\\":\\"The text content of the word.\\",\\"type\\":\\"string\\"}},\\"required\\":\[\\"word\\",\\"start\\",\\"end\\"\],\\"type\\":\\"object\\"},\\"XGroq\\":{\\"properties\\":{\\"error\\":{\\"description\\":\\"An error string indicating why a stream was stopped early\\",\\"type\\":\\"string\\"},\\"id\\":{\\"description\\":\\"A groq request ID which can be used by to refer to a specific request to groq support\\\\nOnly sent with the first chunk\\\\n\\",\\"type\\":\\"string\\"},\\"usage\\":{\\"$ref\\":\\"#/components/schemas/CompletionUsage\\"},\\"usage\_breakdown\\":{\\"$ref\\":\\"#/components/schemas/ChatCompletionUsageBreakdown\\"}},\\"type\\":\\"object\\"}},\\"securitySchemes\\":{\\"api\_key\\":{\\"bearerFormat\\":\\"apiKey\\",\\"scheme\\":\\"bearer\\",\\"type\\":\\"http\\"}}},\\"info\\":{\\"contact\\":{\\"email\\":\\"support@groq.com\\",\\"name\\":\\"Groq Support\\"},\\"description\\":\\"Specification of the Groq cloud API\\",\\"termsOfService\\":\\"https://groq.com/terms-of-use/\\",\\"title\\":\\"GroqCloud API\\",\\"version\\":\\"2.1\\"},\\"openapi\\":\\"3.0.1\\",\\"paths\\":{\\"/openai/v1/audio/speech\\":{\\"post\\":{\\"operationId\\":\\"createSpeech\\",\\"requestBody\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateSpeechRequest\\"}}},\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"audio/wav\\":{\\"schema\\":{\\"format\\":\\"binary\\",\\"type\\":\\"string\\"}}},\\"description\\":\\"OK\\",\\"headers\\":{\\"Transfer-Encoding\\":{\\"description\\":\\"chunked\\",\\"schema\\":{\\"type\\":\\"string\\"}}}}},\\"summary\\":\\"Generates audio from the input text.\\",\\"tags\\":\[\\"Audio\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/audio/speech \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n -d '{\\\\n \\\\\\"model\\\\\\": \\\\\\"playai-tts\\\\\\",\\\\n \\\\\\"input\\\\\\": \\\\\\"I love building and shipping new features for our users!\\\\\\",\\\\n \\\\\\"voice\\\\\\": \\\\\\"Fritz-PlayAI\\\\\\",\\\\n \\\\\\"response\_format\\\\\\": \\\\\\"wav\\\\\\"\\\\n }'\\\\n\\",\\"js\\":\\"import fs from \\\\\\"fs\\\\\\";\\\\nimport path from \\\\\\"path\\\\\\";\\\\nimport Groq from 'groq-sdk';\\\\n\\\\nconst groq = new Groq({\\\\n apiKey: process.env.GROQ\_API\_KEY\\\\n});\\\\n\\\\nconst speechFilePath = \\\\\\"speech.wav\\\\\\";\\\\nconst model = \\\\\\"playai-tts\\\\\\";\\\\nconst voice = \\\\\\"Fritz-PlayAI\\\\\\";\\\\nconst text = \\\\\\"I love building and shipping new features for our users!\\\\\\";\\\\nconst responseFormat = \\\\\\"wav\\\\\\";\\\\n\\\\nasync function main() {\\\\n const response = await groq.audio.speech.create({\\\\n model: model,\\\\n voice: voice,\\\\n input: text,\\\\n response\_format: responseFormat\\\\n });\\\\n\\\\n const buffer = Buffer.from(await response.arrayBuffer());\\\\n await fs.promises.writeFile(speechFilePath, buffer);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"))\\\\n\\\\nspeech\_file\_path = \\\\\\"speech.wav\\\\\\"\\\\nmodel = \\\\\\"playai-tts\\\\\\"\\\\nvoice = \\\\\\"Fritz-PlayAI\\\\\\"\\\\ntext = \\\\\\"I love building and shipping new features for our users!\\\\\\"\\\\nresponse\_format = \\\\\\"wav\\\\\\"\\\\n\\\\nresponse = client.audio.speech.create(\\\\n model=model,\\\\n voice=voice,\\\\n input=text,\\\\n response\_format=response\_format\\\\n)\\\\n\\\\nresponse.write\_to\_file(speech\_file\_path)\\\\n\\"},\\"title\\":\\"Default\\"}\],\\"returns\\":\\"Returns an audio file in \`wav\` format.\\"}}},\\"/openai/v1/audio/transcriptions\\":{\\"post\\":{\\"operationId\\":\\"createTranscription\\",\\"requestBody\\":{\\"content\\":{\\"multipart/form-data\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateTranscriptionRequest\\"}}},\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateTranscriptionResponseJson\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Transcribes audio into the input language.\\",\\"tags\\":\[\\"Audio\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/audio/transcriptions \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: multipart/form-data\\\\\\" \\\\\\\\\\\\n -F file=\\\\\\"@./sample\_audio.m4a\\\\\\" \\\\\\\\\\\\n -F model=\\\\\\"whisper-large-v3\\\\\\"\\\\n\\",\\"js\\":\\"import fs from \\\\\\"fs\\\\\\";\\\\nimport Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq();\\\\nasync function main() {\\\\n const transcription = await groq.audio.transcriptions.create({\\\\n file: fs.createReadStream(\\\\\\"sample\_audio.m4a\\\\\\"),\\\\n model: \\\\\\"whisper-large-v3\\\\\\",\\\\n prompt: \\\\\\"Specify context or spelling\\\\\\", // Optional\\\\n response\_format: \\\\\\"json\\\\\\", // Optional\\\\n language: \\\\\\"en\\\\\\", // Optional\\\\n temperature: 0.0, // Optional\\\\n });\\\\n console.log(transcription.text);\\\\n}\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq()\\\\nfilename = os.path.dirname(\_\_file\_\_) + \\\\\\"/sample\_audio.m4a\\\\\\"\\\\n\\\\nwith open(filename, \\\\\\"rb\\\\\\") as file:\\\\n transcription = client.audio.transcriptions.create(\\\\n file=(filename, file.read()),\\\\n model=\\\\\\"whisper-large-v3\\\\\\",\\\\n prompt=\\\\\\"Specify context or spelling\\\\\\", # Optional\\\\n response\_format=\\\\\\"json\\\\\\", # Optional\\\\n language=\\\\\\"en\\\\\\", # Optional\\\\n temperature=0.0 # Optional\\\\n )\\\\n print(transcription.text)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"text\\\\\\": \\\\\\"Your transcribed text appears here...\\\\\\",\\\\n \\\\\\"x\_groq\\\\\\": {\\\\n \\\\\\"id\\\\\\": \\\\\\"req\_unique\_id\\\\\\"\\\\n }\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"Returns an audio transcription object.\\"}}},\\"/openai/v1/audio/translations\\":{\\"post\\":{\\"operationId\\":\\"createTranslation\\",\\"requestBody\\":{\\"content\\":{\\"multipart/form-data\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateTranslationRequest\\"}}},\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateTranslationResponseJson\\"}},\\"text/plain\\":{\\"schema\\":{\\"type\\":\\"string\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Translates audio into English.\\",\\"tags\\":\[\\"Audio\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/audio/translations \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: multipart/form-data\\\\\\" \\\\\\\\\\\\n -F file=\\\\\\"@./sample\_audio.m4a\\\\\\" \\\\\\\\\\\\n -F model=\\\\\\"whisper-large-v3\\\\\\"\\\\n\\",\\"js\\":\\"// Default\\\\nimport fs from \\\\\\"fs\\\\\\";\\\\nimport Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq();\\\\nasync function main() {\\\\n const translation = await groq.audio.translations.create({\\\\n file: fs.createReadStream(\\\\\\"sample\_audio.m4a\\\\\\"),\\\\n model: \\\\\\"whisper-large-v3\\\\\\",\\\\n prompt: \\\\\\"Specify context or spelling\\\\\\", // Optional\\\\n response\_format: \\\\\\"json\\\\\\", // Optional\\\\n temperature: 0.0, // Optional\\\\n });\\\\n console.log(translation.text);\\\\n}\\\\nmain();\\\\n\\",\\"py\\":\\"# Default\\\\nimport os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq()\\\\nfilename = os.path.dirname(\_\_file\_\_) + \\\\\\"/sample\_audio.m4a\\\\\\"\\\\n\\\\nwith open(filename, \\\\\\"rb\\\\\\") as file:\\\\n translation = client.audio.translations.create(\\\\n file=(filename, file.read()),\\\\n model=\\\\\\"whisper-large-v3\\\\\\",\\\\n prompt=\\\\\\"Specify context or spelling\\\\\\", # Optional\\\\n response\_format=\\\\\\"json\\\\\\", # Optional\\\\n temperature=0.0 # Optional\\\\n )\\\\n print(translation.text)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"text\\\\\\": \\\\\\"Your translated text appears here...\\\\\\",\\\\n \\\\\\"x\_groq\\\\\\": {\\\\n \\\\\\"id\\\\\\": \\\\\\"req\_unique\_id\\\\\\"\\\\n }\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"Returns an audio translation object.\\"}}},\\"/openai/v1/batches\\":{\\"get\\":{\\"operationId\\":\\"listBatches\\",\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/ListBatchesResponse\\"}}},\\"description\\":\\"Batch listed successfully.\\"}},\\"summary\\":\\"List your organization's batches.\\",\\"tags\\":\[\\"Batch\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/batches \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nasync function main() {\\\\n const batchList = await client.batches.list();\\\\n console.log(batchList.data);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"), # This is the default and can be omitted\\\\n)\\\\nbatch\_list = client.batches.list()\\\\nprint(batch\_list.data)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"object\\\\\\": \\\\\\"list\\\\\\",\\\\n \\\\\\"data\\\\\\": \[\\\\n {\\\\n \\\\\\"id\\\\\\": \\\\\\"batch\_01jh6xa7reempvjyh6n3yst2zw\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"batch\\\\\\",\\\\n \\\\\\"endpoint\\\\\\": \\\\\\"/v1/chat/completions\\\\\\",\\\\n \\\\\\"errors\\\\\\": null,\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"completion\_window\\\\\\": \\\\\\"24h\\\\\\",\\\\n \\\\\\"status\\\\\\": \\\\\\"validating\\\\\\",\\\\n \\\\\\"output\_file\_id\\\\\\": null,\\\\n \\\\\\"error\_file\_id\\\\\\": null,\\\\n \\\\\\"finalizing\_at\\\\\\": null,\\\\n \\\\\\"failed\_at\\\\\\": null,\\\\n \\\\\\"expired\_at\\\\\\": null,\\\\n \\\\\\"cancelled\_at\\\\\\": null,\\\\n \\\\\\"request\_counts\\\\\\": {\\\\n \\\\\\"total\\\\\\": 0,\\\\n \\\\\\"completed\\\\\\": 0,\\\\n \\\\\\"failed\\\\\\": 0\\\\n },\\\\n \\\\\\"metadata\\\\\\": null,\\\\n \\\\\\"created\_at\\\\\\": 1736472600,\\\\n \\\\\\"expires\_at\\\\\\": 1736559000,\\\\n \\\\\\"cancelling\_at\\\\\\": null,\\\\n \\\\\\"completed\_at\\\\\\": null,\\\\n \\\\\\"in\_progress\_at\\\\\\": null\\\\n }\\\\n \]\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A list of batches\\"}},\\"post\\":{\\"operationId\\":\\"createBatch\\",\\"requestBody\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"properties\\":{\\"completion\_window\\":{\\"description\\":\\"The time frame within which the batch should be processed. Durations from \`24h\` to \`7d\` are supported.\\",\\"type\\":\\"string\\"},\\"endpoint\\":{\\"description\\":\\"The endpoint to be used for all requests in the batch. Currently \`/v1/chat/completions\` is supported.\\",\\"enum\\":\[\\"/v1/chat/completions\\"\],\\"type\\":\\"string\\"},\\"input\_file\_id\\":{\\"description\\":\\"The ID of an uploaded file that contains requests for the new batch.\\\\n\\\\nSee \[upload file\](/docs/api-reference#files-upload) for how to upload a file.\\\\n\\\\nYour input file must be formatted as a \[JSONL file\](/docs/batch), and must be uploaded with the purpose \`batch\`. The file can be up to 100 MB in size.\\\\n\\",\\"type\\":\\"string\\"},\\"metadata\\":{\\"additionalProperties\\":{\\"type\\":\\"string\\"},\\"description\\":\\"Optional custom metadata for the batch.\\",\\"nullable\\":true,\\"type\\":\\"object\\"}},\\"required\\":\[\\"input\_file\_id\\",\\"endpoint\\",\\"completion\_window\\"\],\\"type\\":\\"object\\"}}},\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/Batch\\"}}},\\"description\\":\\"Batch created successfully.\\"}},\\"summary\\":\\"Creates and executes a batch from an uploaded file of requests. \[Learn more\](/docs/batch).\\",\\"tags\\":\[\\"Batch\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/batches \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n -d '{\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"endpoint\\\\\\": \\\\\\"/v1/chat/completions\\\\\\",\\\\n \\\\\\"completion\_window\\\\\\": \\\\\\"24h\\\\\\"\\\\n }'\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nasync function main() {\\\\n const batch = await client.batches.create({\\\\n completion\_window: \\\\\\"24h\\\\\\",\\\\n endpoint: \\\\\\"/v1/chat/completions\\\\\\",\\\\n input\_file\_id: \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n });\\\\n console.log(batch.id);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"), # This is the default and can be omitted\\\\n)\\\\nbatch = client.batches.create(\\\\n completion\_window=\\\\\\"24h\\\\\\",\\\\n endpoint=\\\\\\"/v1/chat/completions\\\\\\",\\\\n input\_file\_id=\\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n)\\\\nprint(batch.id)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"batch\_01jh6xa7reempvjyh6n3yst2zw\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"batch\\\\\\",\\\\n \\\\\\"endpoint\\\\\\": \\\\\\"/v1/chat/completions\\\\\\",\\\\n \\\\\\"errors\\\\\\": null,\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"completion\_window\\\\\\": \\\\\\"24h\\\\\\",\\\\n \\\\\\"status\\\\\\": \\\\\\"validating\\\\\\",\\\\n \\\\\\"output\_file\_id\\\\\\": null,\\\\n \\\\\\"error\_file\_id\\\\\\": null,\\\\n \\\\\\"finalizing\_at\\\\\\": null,\\\\n \\\\\\"failed\_at\\\\\\": null,\\\\n \\\\\\"expired\_at\\\\\\": null,\\\\n \\\\\\"cancelled\_at\\\\\\": null,\\\\n \\\\\\"request\_counts\\\\\\": {\\\\n \\\\\\"total\\\\\\": 0,\\\\n \\\\\\"completed\\\\\\": 0,\\\\n \\\\\\"failed\\\\\\": 0\\\\n },\\\\n \\\\\\"metadata\\\\\\": null,\\\\n \\\\\\"created\_at\\\\\\": 1736472600,\\\\n \\\\\\"expires\_at\\\\\\": 1736559000,\\\\n \\\\\\"cancelling\_at\\\\\\": null,\\\\n \\\\\\"completed\_at\\\\\\": null,\\\\n \\\\\\"in\_progress\_at\\\\\\": null\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A created batch object.\\"}}},\\"/openai/v1/batches/{batch\_id}\\":{\\"get\\":{\\"operationId\\":\\"retrieveBatch\\",\\"parameters\\":\[{\\"description\\":\\"The ID of the batch to retrieve.\\",\\"in\\":\\"path\\",\\"name\\":\\"batch\_id\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/Batch\\"}}},\\"description\\":\\"Batch retrieved successfully.\\"}},\\"summary\\":\\"Retrieves a batch.\\",\\"tags\\":\[\\"Batch\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/batches/batch\_01jh6xa7reempvjyh6n3yst2zw \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nasync function main() {\\\\n const batch = await client.batches.retrieve(\\\\\\"batch\_01jh6xa7reempvjyh6n3yst2zw\\\\\\");\\\\n console.log(batch.id);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"), # This is the default and can be omitted\\\\n)\\\\nbatch = client.batches.retrieve(\\\\n \\\\\\"batch\_01jh6xa7reempvjyh6n3yst2zw\\\\\\",\\\\n)\\\\nprint(batch.id)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"batch\_01jh6xa7reempvjyh6n3yst2zw\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"batch\\\\\\",\\\\n \\\\\\"endpoint\\\\\\": \\\\\\"/v1/chat/completions\\\\\\",\\\\n \\\\\\"errors\\\\\\": null,\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"completion\_window\\\\\\": \\\\\\"24h\\\\\\",\\\\n \\\\\\"status\\\\\\": \\\\\\"validating\\\\\\",\\\\n \\\\\\"output\_file\_id\\\\\\": null,\\\\n \\\\\\"error\_file\_id\\\\\\": null,\\\\n \\\\\\"finalizing\_at\\\\\\": null,\\\\n \\\\\\"failed\_at\\\\\\": null,\\\\n \\\\\\"expired\_at\\\\\\": null,\\\\n \\\\\\"cancelled\_at\\\\\\": null,\\\\n \\\\\\"request\_counts\\\\\\": {\\\\n \\\\\\"total\\\\\\": 0,\\\\n \\\\\\"completed\\\\\\": 0,\\\\n \\\\\\"failed\\\\\\": 0\\\\n },\\\\n \\\\\\"metadata\\\\\\": null,\\\\n \\\\\\"created\_at\\\\\\": 1736472600,\\\\n \\\\\\"expires\_at\\\\\\": 1736559000,\\\\n \\\\\\"cancelling\_at\\\\\\": null,\\\\n \\\\\\"completed\_at\\\\\\": null,\\\\n \\\\\\"in\_progress\_at\\\\\\": null\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A batch object.\\"}}},\\"/openai/v1/batches/{batch\_id}/cancel\\":{\\"post\\":{\\"operationId\\":\\"cancelBatch\\",\\"parameters\\":\[{\\"description\\":\\"The ID of the batch to cancel.\\",\\"in\\":\\"path\\",\\"name\\":\\"batch\_id\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/Batch\\"}}},\\"description\\":\\"Batch cancelled successfully.\\"}},\\"summary\\":\\"Cancels a batch.\\",\\"tags\\":\[\\"Batch\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl -X POST https://api.groq.com/openai/v1/batches/batch\_01jh6xa7reempvjyh6n3yst2zw/cancel \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nasync function main() {\\\\n const batch = await client.batches.cancel(\\\\\\"batch\_01jh6xa7reempvjyh6n3yst2zw\\\\\\");\\\\n console.log(batch.id);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"), # This is the default and can be omitted\\\\n)\\\\nbatch = client.batches.cancel(\\\\n \\\\\\"batch\_01jh6xa7reempvjyh6n3yst2zw\\\\\\",\\\\n)\\\\nprint(batch.id)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"batch\_01jh6xa7reempvjyh6n3yst2zw\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"batch\\\\\\",\\\\n \\\\\\"endpoint\\\\\\": \\\\\\"/v1/chat/completions\\\\\\",\\\\n \\\\\\"errors\\\\\\": null,\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"completion\_window\\\\\\": \\\\\\"24h\\\\\\",\\\\n \\\\\\"status\\\\\\": \\\\\\"cancelling\\\\\\",\\\\n \\\\\\"output\_file\_id\\\\\\": null,\\\\n \\\\\\"error\_file\_id\\\\\\": null,\\\\n \\\\\\"finalizing\_at\\\\\\": null,\\\\n \\\\\\"failed\_at\\\\\\": null,\\\\n \\\\\\"expired\_at\\\\\\": null,\\\\n \\\\\\"cancelled\_at\\\\\\": null,\\\\n \\\\\\"request\_counts\\\\\\": {\\\\n \\\\\\"total\\\\\\": 0,\\\\n \\\\\\"completed\\\\\\": 0,\\\\n \\\\\\"failed\\\\\\": 0\\\\n },\\\\n \\\\\\"metadata\\\\\\": null,\\\\n \\\\\\"created\_at\\\\\\": 1736472600,\\\\n \\\\\\"expires\_at\\\\\\": 1736559000,\\\\n \\\\\\"cancelling\_at\\\\\\": null,\\\\n \\\\\\"completed\_at\\\\\\": null,\\\\n \\\\\\"in\_progress\_at\\\\\\": null\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A batch object.\\"}}},\\"/openai/v1/chat/completions\\":{\\"post\\":{\\"operationId\\":\\"createChatCompletion\\",\\"requestBody\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateChatCompletionRequest\\"}}},\\"description\\":\\"The chat prompt and parameters\\",\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateChatCompletionResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Creates a model response for the given chat conversation.\\",\\"tags\\":\[\\"Chat\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/chat/completions -s \\\\\\\\\\\\n-H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n-H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n-d '{\\\\n \\\\\\"model\\\\\\": \\\\\\"llama-3.3-70b-versatile\\\\\\",\\\\n \\\\\\"messages\\\\\\": \[{\\\\n \\\\\\"role\\\\\\": \\\\\\"user\\\\\\",\\\\n \\\\\\"content\\\\\\": \\\\\\"Explain the importance of fast language models\\\\\\"\\\\n }\]\\\\n}'\\\\n\\",\\"js\\":\\"import Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq({ apiKey: process.env.GROQ\_API\_KEY });\\\\n\\\\nasync function main() {\\\\n const completion = await groq.chat.completions\\\\n .create({\\\\n messages: \[\\\\n {\\\\n role: \\\\\\"user\\\\\\",\\\\n content: \\\\\\"Explain the importance of fast language models\\\\\\",\\\\n },\\\\n \],\\\\n model: \\\\\\"llama-3.3-70b-versatile\\\\\\",\\\\n })\\\\n console.log(completion.choices\[0\].message.content);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\n\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n # This is the default and can be omitted\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"),\\\\n)\\\\n\\\\nchat\_completion = client.chat.completions.create(\\\\n messages=\[\\\\n {\\\\n \\\\\\"role\\\\\\": \\\\\\"system\\\\\\",\\\\n \\\\\\"content\\\\\\": \\\\\\"You are a helpful assistant.\\\\\\"\\\\n },\\\\n {\\\\n \\\\\\"role\\\\\\": \\\\\\"user\\\\\\",\\\\n \\\\\\"content\\\\\\": \\\\\\"Explain the importance of fast language models\\\\\\",\\\\n }\\\\n \],\\\\n model=\\\\\\"llama-3.3-70b-versatile\\\\\\",\\\\n)\\\\n\\\\nprint(chat\_completion.choices\[0\].message.content)\\\\n\\"},\\"response\\":\\"$1c\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"Returns a \[chat completion\](/docs/api-reference#chat-create) object, or a streamed sequence of \[chat completion chunk\](/docs/api-reference#chat-create) objects if the request is streamed.\\"}}},\\"/openai/v1/embeddings\\":{\\"post\\":{\\"operationId\\":\\"createEmbedding\\",\\"requestBody\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateEmbeddingRequest\\"}}},\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateEmbeddingResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Creates an embedding vector representing the input text.\\",\\"tags\\":\[\\"Embeddings\\"\]}},\\"/openai/v1/files\\":{\\"get\\":{\\"operationId\\":\\"listFiles\\",\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/ListFilesResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Returns a list of files.\\",\\"tags\\":\[\\"Files\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/files \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nasync function main() {\\\\n const fileList = await client.files.list();\\\\n console.log(fileList.data);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"), # This is the default and can be omitted\\\\n)\\\\nfile\_list = client.files.list()\\\\nprint(file\_list.data)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"object\\\\\\": \\\\\\"list\\\\\\",\\\\n \\\\\\"data\\\\\\": \[\\\\n {\\\\n \\\\\\"id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"file\\\\\\",\\\\n \\\\\\"bytes\\\\\\": 966,\\\\n \\\\\\"created\_at\\\\\\": 1736472501,\\\\n \\\\\\"filename\\\\\\": \\\\\\"batch\_file.jsonl\\\\\\",\\\\n \\\\\\"purpose\\\\\\": \\\\\\"batch\\\\\\"\\\\n }\\\\n \]\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A list of \[File\](/docs/api-reference#files-upload) objects.\\"}},\\"post\\":{\\"operationId\\":\\"uploadFile\\",\\"requestBody\\":{\\"content\\":{\\"multipart/form-data\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateFileRequest\\"}}},\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/File\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Upload a file that can be used across various endpoints.\\\\n\\\\nThe Batch API only supports \`.jsonl\` files up to 100 MB in size. The input also has a specific required \[format\](/docs/batch).\\\\n\\\\nPlease contact us if you need to increase these storage limits.\\\\n\\",\\"tags\\":\[\\"Files\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/files \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -F purpose=\\\\\\"batch\\\\\\" \\\\\\\\\\\\n -F \\\\\\"file=@batch\_file.jsonl\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nconst fileContent = '{\\\\\\"custom\_id\\\\\\": \\\\\\"request-1\\\\\\", \\\\\\"method\\\\\\": \\\\\\"POST\\\\\\", \\\\\\"url\\\\\\": \\\\\\"/v1/chat/completions\\\\\\", \\\\\\"body\\\\\\": {\\\\\\"model\\\\\\": \\\\\\"llama-3.1-8b-instant\\\\\\", \\\\\\"messages\\\\\\": \[{\\\\\\"role\\\\\\": \\\\\\"user\\\\\\", \\\\\\"content\\\\\\": \\\\\\"Explain the importance of fast language models\\\\\\"}\]}}\\\\\\\\n';\\\\n\\\\nasync function main() {\\\\n const blob = new Blob(\[fileContent\]);\\\\n const file = new File(\[blob\], 'batch.jsonl');\\\\n\\\\n const createdFile = await client.files.create({ file: file, purpose: 'batch' });\\\\n console.log(createdFile.id);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nimport requests # pip install requests first!\\\\n\\\\ndef upload\_file\_to\_groq(api\_key, file\_path):\\\\n url = \\\\\\"https://api.groq.com/openai/v1/files\\\\\\"\\\\n\\\\n headers = {\\\\n \\\\\\"Authorization\\\\\\": f\\\\\\"Bearer {api\_key}\\\\\\"\\\\n }\\\\n\\\\n # Prepare the file and form data\\\\n files = {\\\\n \\\\\\"file\\\\\\": (\\\\\\"batch\_file.jsonl\\\\\\", open(file\_path, \\\\\\"rb\\\\\\"))\\\\n }\\\\n\\\\n data = {\\\\n \\\\\\"purpose\\\\\\": \\\\\\"batch\\\\\\"\\\\n }\\\\n\\\\n # Make the POST request\\\\n response = requests.post(url, headers=headers, files=files, data=data)\\\\n\\\\n return response.json()\\\\n\\\\n# Usage example\\\\napi\_key = os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\")\\\\nfile\_path = \\\\\\"batch\_file.jsonl\\\\\\" # Path to your JSONL file\\\\n\\\\ntry:\\\\n result = upload\_file\_to\_groq(api\_key, file\_path)\\\\n print(result)\\\\nexcept Exception as e:\\\\n print(f\\\\\\"Error: {e}\\\\\\")\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"file\\\\\\",\\\\n \\\\\\"bytes\\\\\\": 966,\\\\n \\\\\\"created\_at\\\\\\": 1736472501,\\\\n \\\\\\"filename\\\\\\": \\\\\\"batch\_file.jsonl\\\\\\",\\\\n \\\\\\"purpose\\\\\\": \\\\\\"batch\\\\\\"\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"The uploaded File object.\\"}}},\\"/openai/v1/files/{file\_id}\\":{\\"delete\\":{\\"operationId\\":\\"deleteFile\\",\\"parameters\\":\[{\\"description\\":\\"The ID of the file to use for this request.\\",\\"in\\":\\"path\\",\\"name\\":\\"file\_id\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/DeleteFileResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Delete a file.\\",\\"tags\\":\[\\"Files\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl -X DELETE https://api.groq.com/openai/v1/files/file\_01jh6x76wtemjr74t1fh0faj5t \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nasync function main() {\\\\n const fileDelete = await client.files.delete(\\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\");\\\\n console.log(fileDelete);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"), # This is the default and can be omitted\\\\n)\\\\nfile\_delete = client.files.delete(\\\\n \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n)\\\\nprint(file\_delete)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"file\\\\\\",\\\\n \\\\\\"deleted\\\\\\": true\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A deleted file response object.\\"}},\\"get\\":{\\"operationId\\":\\"retrieveFile\\",\\"parameters\\":\[{\\"description\\":\\"The file to retrieve\\",\\"in\\":\\"path\\",\\"name\\":\\"file\_id\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/File\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Returns information about a file.\\",\\"tags\\":\[\\"Files\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/files/file\_01jh6x76wtemjr74t1fh0faj5t \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nasync function main() {\\\\n const file = await client.files.info('file\_01jh6x76wtemjr74t1fh0faj5t');\\\\n console.log(file);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"), # This is the default and can be omitted\\\\n)\\\\nfile = client.files.info(\\\\n \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n)\\\\nprint(file)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"file\\\\\\",\\\\n \\\\\\"bytes\\\\\\": 966,\\\\n \\\\\\"created\_at\\\\\\": 1736472501,\\\\n \\\\\\"filename\\\\\\": \\\\\\"batch\_file.jsonl\\\\\\",\\\\n \\\\\\"purpose\\\\\\": \\\\\\"batch\\\\\\"\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A file object.\\"}}},\\"/openai/v1/files/{file\_id}/content\\":{\\"get\\":{\\"operationId\\":\\"downloadFile\\",\\"parameters\\":\[{\\"description\\":\\"The ID of the file to use for this request.\\",\\"in\\":\\"path\\",\\"name\\":\\"file\_id\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/octet-stream\\":{\\"schema\\":{\\"format\\":\\"binary\\",\\"type\\":\\"string\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Returns the contents of the specified file.\\",\\"tags\\":\[\\"Files\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/files/file\_01jh6x76wtemjr74t1fh0faj5t/content \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\], // This is the default and can be omitted\\\\n});\\\\n\\\\nasync function main() {\\\\n const response = await client.files.content('file\_01jh6x76wtemjr74t1fh0faj5t');\\\\n console.log(response);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"), # This is the default and can be omitted\\\\n)\\\\nresponse = client.files.content(\\\\n \\\\\\"file\_01jh6x76wtemjr74t1fh0faj5t\\\\\\",\\\\n)\\\\nprint(response)\\\\n\\"},\\"title\\":\\"Default\\"}\],\\"returns\\":\\"The file content\\"}}},\\"/openai/v1/models\\":{\\"get\\":{\\"description\\":\\"get all available models\\",\\"operationId\\":\\"listModels\\",\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/ListModelsResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"List all available \[models\](https://console.groq.com/docs/models).\\",\\"tags\\":\[\\"Models\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/models \\\\\\\\\\\\n-H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq({ apiKey: process.env.GROQ\_API\_KEY });\\\\n\\\\nasync function main() {\\\\n const models = await groq.models.list();\\\\n console.log(models);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n # This is the default and can be omitted\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"),\\\\n)\\\\n\\\\nmodels = client.models.list()\\\\n\\\\nprint(models)\\\\n\\"},\\"response\\":\\"$1d\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A list of model objects.\\"}}},\\"/openai/v1/models/{model}\\":{\\"delete\\":{\\"description\\":\\"Delete a model\\",\\"operationId\\":\\"deleteModel\\",\\"parameters\\":\[{\\"description\\":\\"The model to delete\\",\\"in\\":\\"path\\",\\"name\\":\\"model\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/DeleteModelResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Delete model\\",\\"tags\\":\[\\"Models\\"\]},\\"get\\":{\\"description\\":\\"Get a specific model\\",\\"operationId\\":\\"retrieveModel\\",\\"parameters\\":\[{\\"description\\":\\"The model to get\\",\\"in\\":\\"path\\",\\"name\\":\\"model\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/Model\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Get detailed information about a \[model\](https://console.groq.com/docs/models).\\",\\"tags\\":\[\\"Models\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/models/llama-3.3-70b-versatile \\\\\\\\\\\\n-H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq({ apiKey: process.env.GROQ\_API\_KEY });\\\\n\\\\nasync function main() {\\\\n const model = await groq.models.retrieve(\\\\\\"llama-3.3-70b-versatile\\\\\\");\\\\n console.log(model);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n # This is the default and can be omitted\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"),\\\\n)\\\\n\\\\nmodel = client.models.retrieve(\\\\\\"llama-3.3-70b-versatile\\\\\\")\\\\n\\\\nprint(model)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"llama3-8b-8192\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"model\\\\\\",\\\\n \\\\\\"created\\\\\\": 1693721698,\\\\n \\\\\\"owned\_by\\\\\\": \\\\\\"Meta\\\\\\",\\\\n \\\\\\"active\\\\\\": true,\\\\n \\\\\\"context\_window\\\\\\": 8192,\\\\n \\\\\\"public\_apps\\\\\\": null,\\\\n \\\\\\"max\_completion\_tokens\\\\\\": 8192\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A model object.\\"}}},\\"/openai/v1/reranking\\":{\\"post\\":{\\"description\\":\\"Given a query and a list of documents, returns the documents ranked by their relevance to the query.\\\\nThe documents are scored and sorted in descending order of relevance.\\\\n\\",\\"operationId\\":\\"createReranking\\",\\"requestBody\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/RerankingRequest\\"}}},\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/RerankingResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Reranks documents based on their relevance to a query.\\",\\"tags\\":\[\\"Reranking\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/reranking \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n -d '{\\\\n \\\\\\"model\\\\\\": \\\\\\"qwen3-reranker-4b\\\\\\",\\\\n \\\\\\"query\\\\\\": \\\\\\"artificial intelligence\\\\\\",\\\\n \\\\\\"docs\\\\\\": \[\\\\n \\\\\\"Machine learning is a subset of AI\\\\\\",\\\\n \\\\\\"The weather is nice today\\\\\\",\\\\n \\\\\\"Deep learning uses neural networks\\\\\\"\\\\n \]\\\\n }'\\\\n\\",\\"js\\":\\"import Groq from 'groq-sdk';\\\\n\\\\nconst client = new Groq({\\\\n apiKey: process.env\['GROQ\_API\_KEY'\],\\\\n});\\\\n\\\\nasync function main() {\\\\n const reranking = await client.reranking.create({\\\\n model: 'qwen3-reranker-4b',\\\\n query: 'artificial intelligence',\\\\n docs: \[\\\\n 'Machine learning is a subset of AI',\\\\n 'The weather is nice today',\\\\n 'Deep learning uses neural networks'\\\\n \]\\\\n });\\\\n console.log(reranking.results);\\\\n}\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"))\\\\n\\\\nreranking = client.reranking.create(\\\\n model=\\\\\\"qwen3-reranker-4b\\\\\\",\\\\n query=\\\\\\"artificial intelligence\\\\\\",\\\\n docs=\[\\\\n \\\\\\"Machine learning is a subset of AI\\\\\\",\\\\n \\\\\\"The weather is nice today\\\\\\", \\\\n \\\\\\"Deep learning uses neural networks\\\\\\"\\\\n \]\\\\n)\\\\nprint(reranking.results)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"results\\\\\\": \[\\\\n {\\\\n \\\\\\"doc\\\\\\": \\\\\\"Machine learning is a subset of AI\\\\\\",\\\\n \\\\\\"score\\\\\\": 0.92\\\\n },\\\\n {\\\\n \\\\\\"doc\\\\\\": \\\\\\"Deep learning uses neural networks\\\\\\", \\\\n \\\\\\"score\\\\\\": 0.87\\\\n },\\\\n {\\\\n \\\\\\"doc\\\\\\": \\\\\\"The weather is nice today\\\\\\",\\\\n \\\\\\"score\\\\\\": 0.23\\\\n }\\\\n \]\\\\n}\\\\n\\",\\"title\\":\\"Basic Reranking\\"},{\\"request\\":{\\"json\\":\\"{\\\\n \\\\\\"model\\\\\\": \\\\\\"qwen3-reranker-4b\\\\\\",\\\\n \\\\\\"query\\\\\\": \\\\\\"climate change effects\\\\\\",\\\\n \\\\\\"docs\\\\\\": \[\\\\n \\\\\\"Global warming causes sea level rise\\\\\\",\\\\n \\\\\\"Electric cars reduce emissions\\\\\\",\\\\n \\\\\\"Renewable energy is growing fast\\\\\\"\\\\n \],\\\\n \\\\\\"instruction\\\\\\": \\\\\\"Find documents specifically about environmental impacts\\\\\\"\\\\n}\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"results\\\\\\": \[\\\\n {\\\\n \\\\\\"doc\\\\\\": \\\\\\"Global warming causes sea level rise\\\\\\",\\\\n \\\\\\"score\\\\\\": 0.95\\\\n },\\\\n {\\\\n \\\\\\"doc\\\\\\": \\\\\\"Electric cars reduce emissions\\\\\\",\\\\n \\\\\\"score\\\\\\": 0.78\\\\n },\\\\n {\\\\n \\\\\\"doc\\\\\\": \\\\\\"Renewable energy is growing fast\\\\\\",\\\\n \\\\\\"score\\\\\\": 0.65\\\\n }\\\\n \]\\\\n}\\\\n\\",\\"title\\":\\"Reranking with Custom Instruction\\"}\],\\"returns\\":\\"A list of documents sorted by relevance score in descending order. \\\\nScores range from 0.0 to 1.0, where higher scores indicate greater relevance to the query.\\\\n\\"}}},\\"/openai/v1/responses\\":{\\"post\\":{\\"operationId\\":\\"createResponse\\",\\"requestBody\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateResponseRequest\\"}}},\\"description\\":\\"The input prompt and parameters\\",\\"required\\":true},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateResponseResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Creates a model response for the given input.\\",\\"tags\\":\[\\"Responses\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/openai/v1/responses -s \\\\\\\\\\\\n-H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n-H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n-d '{\\\\n \\\\\\"model\\\\\\": \\\\\\"gpt-oss\\\\\\",\\\\n \\\\\\"input\\\\\\": \\\\\\"Tell me a three sentence bedtime story about a unicorn.\\\\\\"\\\\n}'\\\\n\\"},\\"response\\":\\"$1e\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"Returns a \[response\](/docs/api-reference#responses-create) object, or a streamed sequence of \[response events\](/docs/api-reference#responses-streaming) if the request is streamed.\\"}}},\\"/v1/fine\_tunings\\":{\\"get\\":{\\"operationId\\":\\"listFineTunings\\",\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/ListFineTuningsResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Lists all previously created fine tunings. This endpoint is in closed beta. \[Contact us\](https://groq.com/contact) for more information.\\",\\"tags\\":\[\\"Fine Tuning\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/v1/fine\_tunings -s \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq({ apiKey: process.env.GROQ\_API\_KEY });\\\\n\\\\nasync function main() {\\\\n const fineTunings = await groq.fine\_tunings.list();\\\\n console.log(fineTunings);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\n\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n # This is the default and can be omitted\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"),\\\\n)\\\\n\\\\nfine\_tunings = client.fine\_tunings.list()\\\\n\\\\nprint(fine\_tunings)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"object\\\\\\": \\\\\\"list\\\\\\",\\\\n \\\\\\"data\\\\\\": \[\\\\n {\\\\n \\\\\\"id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"name\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"base\_model\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"type\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"created\_at\\\\\\": 0,\\\\n \\\\\\"fine\_tuned\_model\\\\\\": \\\\\\"string\\\\\\"\\\\n }\\\\n \]\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"The list of fine tunes\\"}},\\"post\\":{\\"operationId\\":\\"createFineTuning\\",\\"requestBody\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/CreateFineTuningRequest\\"}}}},\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/ReadFineTuningResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Creates a new fine tuning for the already uploaded files This endpoint is in closed beta. \[Contact us\](https://groq.com/contact) for more information.\\",\\"tags\\":\[\\"Fine Tuning\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/v1/fine\_tunings -s \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\" \\\\\\\\\\\\n -d '{\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"\\u003cfile-id\\u003e\\\\\\",\\\\n \\\\\\"name\\\\\\": \\\\\\"test-1\\\\\\",\\\\n \\\\\\"type\\\\\\": \\\\\\"lora\\\\\\",\\\\n \\\\\\"base\_model\\\\\\": \\\\\\"llama-3.1-8b-instant\\\\\\"\\\\n }'\\\\n\\",\\"js\\":\\"import Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq({ apiKey: process.env.GROQ\_API\_KEY });\\\\n\\\\nasync function main() {\\\\n const fineTunings = await groq.fine\_tunings.create({\\\\n input\_file\_id: \\\\\\"\\u003cfile-id\\u003e\\\\\\",\\\\n name: \\\\\\"test-1\\\\\\",\\\\n type: \\\\\\"lora\\\\\\",\\\\n base\_model: \\\\\\"llama-3.1-8b-instant\\\\\\"\\\\n });\\\\n console.log(fineTunings);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\n\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n # This is the default and can be omitted\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"),\\\\n)\\\\n\\\\nfine\_tunings = client.fine\_tunings.create(\\\\n input\_file\_id=\\\\\\"\\u003cfile-id\\u003e\\\\\\",\\\\n name=\\\\\\"test-1\\\\\\",\\\\n type=\\\\\\"lora\\\\\\",\\\\n base\_model=\\\\\\"llama-3.1-8b-instant\\\\\\"\\\\n)\\\\n\\\\nprint(fine\_tunings)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"object\\\\\\",\\\\n \\\\\\"data\\\\\\": {\\\\n \\\\\\"id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"name\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"base\_model\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"type\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"created\_at\\\\\\": 0,\\\\n \\\\\\"fine\_tuned\_model\\\\\\": \\\\\\"string\\\\\\"\\\\n }\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"The newly created fine tune\\"}}},\\"/v1/fine\_tunings/{id}\\":{\\"delete\\":{\\"operationId\\":\\"deleteFineTuning\\",\\"parameters\\":\[{\\"in\\":\\"path\\",\\"name\\":\\"id\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/DeleteFineTuningResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Deletes an existing fine tuning by id This endpoint is in closed beta. \[Contact us\](https://groq.com/contact) for more information.\\",\\"tags\\":\[\\"Fine Tuning\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl -X DELETE https://api.groq.com/v1/fine\_tunings/:id -s \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq({ apiKey: process.env.GROQ\_API\_KEY });\\\\n\\\\nasync function main() {\\\\n await groq.fine\_tunings.delete({id: \\\\\\"\\u003cid\\u003e\\\\\\"});\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\n\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n # This is the default and can be omitted\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"),\\\\n)\\\\n\\\\nclient.fine\_tunings.delete(id=\\\\\\"\\u003cid\\u003e\\\\\\")\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"fine\_tuning\\\\\\",\\\\n \\\\\\"deleted\\\\\\": true\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A confirmation of the deleted fine tune\\"}},\\"get\\":{\\"operationId\\":\\"getFineTuning\\",\\"parameters\\":\[{\\"in\\":\\"path\\",\\"name\\":\\"id\\",\\"required\\":true,\\"schema\\":{\\"type\\":\\"string\\"}}\],\\"responses\\":{\\"200\\":{\\"content\\":{\\"application/json\\":{\\"schema\\":{\\"$ref\\":\\"#/components/schemas/ReadFineTuningResponse\\"}}},\\"description\\":\\"OK\\"}},\\"summary\\":\\"Retrieves an existing fine tuning by id This endpoint is in closed beta. \[Contact us\](https://groq.com/contact) for more information.\\",\\"tags\\":\[\\"Fine Tuning\\"\],\\"x-groq-metadata\\":{\\"examples\\":\[{\\"request\\":{\\"curl\\":\\"curl https://api.groq.com/v1/fine\_tunings/:id -s \\\\\\\\\\\\n -H \\\\\\"Content-Type: application/json\\\\\\" \\\\\\\\\\\\n -H \\\\\\"Authorization: Bearer $GROQ\_API\_KEY\\\\\\"\\\\n\\",\\"js\\":\\"import Groq from \\\\\\"groq-sdk\\\\\\";\\\\n\\\\nconst groq = new Groq({ apiKey: process.env.GROQ\_API\_KEY });\\\\n\\\\nasync function main() {\\\\n const fineTuning = await groq.fine\_tunings.get({id: \\\\\\"\\u003cid\\u003e\\\\\\"});\\\\n console.log(fineTuning);\\\\n}\\\\n\\\\nmain();\\\\n\\",\\"py\\":\\"import os\\\\n\\\\nfrom groq import Groq\\\\n\\\\nclient = Groq(\\\\n # This is the default and can be omitted\\\\n api\_key=os.environ.get(\\\\\\"GROQ\_API\_KEY\\\\\\"),\\\\n)\\\\n\\\\nfine\_tuning = client.fine\_tunings.get(id=\\\\\\"\\u003cid\\u003e\\\\\\")\\\\n\\\\nprint(fine\_tuning)\\\\n\\"},\\"response\\":\\"{\\\\n \\\\\\"id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"object\\\\\\": \\\\\\"object\\\\\\",\\\\n \\\\\\"data\\\\\\": {\\\\n \\\\\\"id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"name\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"base\_model\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"type\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"input\_file\_id\\\\\\": \\\\\\"string\\\\\\",\\\\n \\\\\\"created\_at\\\\\\": 0,\\\\n \\\\\\"fine\_tuned\_model\\\\\\": \\\\\\"string\\\\\\"\\\\n }\\\\n}\\\\n\\",\\"title\\":\\"Default\\"}\],\\"returns\\":\\"A fine tune metadata object\\"}}}},\\"security\\":\[{\\"api\_key\\":\[\]}\],\\"servers\\":\[{\\"url\\":\\"https://api.groq.com\\"}\],\\"x-groq-metadata\\":{\\"groups\\":\[{\\"description\\":\\"\\",\\"id\\":\\"chat\\",\\"sections\\":\[{\\"key\\":\\"createChatCompletion\\",\\"path\\":\\"create\\",\\"type\\":\\"endpoint\\"}\],\\"title\\":\\"Chat\\",\\"type\\":\\"endpoints\\"},{\\"description\\":\\"\\",\\"id\\":\\"responses\\",\\"sections\\":\[{\\"key\\":\\"createResponse\\",\\"path\\":\\"create\\",\\"type\\":\\"endpoint\\"}\],\\"title\\":\\"Responses (beta)\\",\\"type\\":\\"endpoints\\"},{\\"description\\":\\"\\",\\"id\\":\\"audio\\",\\"sections\\":\[{\\"key\\":\\"createTranscription\\",\\"path\\":\\"transcription\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"createTranslation\\",\\"path\\":\\"translation\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"createSpeech\\",\\"path\\":\\"speech\\",\\"type\\":\\"endpoint\\"}\],\\"title\\":\\"Audio\\",\\"type\\":\\"endpoints\\"},{\\"description\\":\\"\\",\\"id\\":\\"models\\",\\"sections\\":\[{\\"key\\":\\"listModels\\",\\"path\\":\\"list\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"retrieveModel\\",\\"path\\":\\"retrieve\\",\\"type\\":\\"endpoint\\"}\],\\"title\\":\\"Models\\",\\"type\\":\\"endpoints\\"},{\\"description\\":\\"\\",\\"id\\":\\"batches\\",\\"sections\\":\[{\\"key\\":\\"createBatch\\",\\"path\\":\\"create\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"retrieveBatch\\",\\"path\\":\\"retrieve\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"listBatches\\",\\"path\\":\\"list\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"cancelBatch\\",\\"path\\":\\"cancel\\",\\"type\\":\\"endpoint\\"}\],\\"title\\":\\"Batches\\",\\"type\\":\\"endpoints\\"},{\\"description\\":\\"\\",\\"id\\":\\"files\\",\\"sections\\":\[{\\"key\\":\\"uploadFile\\",\\"path\\":\\"upload\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"listFiles\\",\\"path\\":\\"list\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"deleteFile\\",\\"path\\":\\"delete\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"retrieveFile\\",\\"path\\":\\"retrieve\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"downloadFile\\",\\"path\\":\\"download\\",\\"type\\":\\"endpoint\\"}\],\\"title\\":\\"Files\\",\\"type\\":\\"endpoints\\"},{\\"description\\":\\"\\",\\"id\\":\\"fine-tuning\\",\\"sections\\":\[{\\"key\\":\\"listFineTunings\\",\\"path\\":\\"list\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"createFineTuning\\",\\"path\\":\\"create\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"getFineTuning\\",\\"path\\":\\"get\\",\\"type\\":\\"endpoint\\"},{\\"key\\":\\"deleteFineTuning\\",\\"path\\":\\"delete\\",\\"type\\":\\"endpoint\\"}\],\\"title\\":\\"Fine Tuning\\",\\"type\\":\\"endpoints\\"}\]}},\\"children\\":\[\\"$\\",\\"$L5\\",null,{\\"parallelRouterKey\\":\\"children\\",\\"error\\":\\"$undefined\\",\\"errorStyles\\":\\"$undefined\\",\\"errorScripts\\":\\"$undefined\\",\\"template\\":\[\\"$\\",\\"$L6\\",null,{}\],\\"templateStyles\\":\\"$undefined\\",\\"templateScripts\\":\\"$undefined\\",\\"notFound\\":\\"$undefined\\",\\"forbidden\\":\\"$undefined\\",\\"unauthorized\\":\\"$undefined\\"}\]}\]\\n"\])self.\_\_next\_f.push(\[1,"1f:I\[15112,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"1874\\",\\"static/chunks/1874-1af85b5fe730b1d1.js\\",\\"3354\\",\\"static/chunks/3354-1c66ad7116e53618.js\\",\\"6603\\",\\"static/chunks/6603-5a9bea7a37afd382.js\\",\\"1103\\",\\"static/chunks/1103-6ee8bf0e353f128c.js\\",\\"7764\\",\\"static/chunks/7764-9237aa9acd440644.js\\",\\"5193\\",\\"static/chunks/5193-5a50519070c43274.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"4215\\",\\"static/chunks/4215-210105fc188a4582.js\\",\\"5789\\",\\"static/chunks/5789-d4359a1055e0aacd.js\\",\\"4454\\",\\"static/chunks/4454-c3cbd02a8687a7b6.js\\",\\"7860\\",\\"static/chunks/7860-a6cd4bd13b0a2513.js\\",\\"1285\\",\\"static/chunks/app/(console)/docs/(mdx-pages)/rate-limits/page-b0be08a4163e38e4.js\\"\],\\"ContentSwitcher\\"\]\\n20:I\[15112,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"1874\\",\\"static/chunks/1874-1af85b5fe730b1d1.js\\",\\"3354\\",\\"static/chunks/3354-1c66ad7116e53618.js\\",\\"6603\\",\\"static/chunks/6603-5a9bea7a37afd382.js\\",\\"1103\\",\\"static/chunks/1103-6ee8bf0e353f128c.js\\",\\"7764\\",\\"static/chunks/7764-9237aa9acd440644.js\\",\\"5193\\",\\"static/chunks/5193-5a50519070c43274.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"4215\\",\\"static/chunks/4215-210105fc188a4582.js\\",\\"5789\\",\\"static/chunks/5789-d4359a1055e0aacd.js\\",\\"4454\\",\\"static/chunks/4454-c3cbd02a8687a7b6.js\\",\\"7860\\",\\"static/chunks/7860-a6cd4bd13b0a2513.js\\",\\"1285\\",\\"static/chunks/app/(console)/docs/(mdx-pages)/rate-limits/page-b0be08a4163e38e4.js\\"\],\\"ContentItem\\"\]\\n21:I\[99989,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"9170\\",\\"static/chunks/9170-d2c224567d46ff41.js\\",\\"1874\\",\\"static/chunks/1874-1af85b5fe730b1d1.js\\",\\"3354\\",\\"static/chunks/3354-1c66ad7116e53618.js\\",\\"6603\\",\\"st"\])self.\_\_next\_f.push(\[1,"atic/chunks/6603-5a9bea7a37afd382.js\\",\\"1103\\",\\"static/chunks/1103-6ee8bf0e353f128c.js\\",\\"7764\\",\\"static/chunks/7764-9237aa9acd440644.js\\",\\"5193\\",\\"static/chunks/5193-5a50519070c43274.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"4215\\",\\"static/chunks/4215-210105fc188a4582.js\\",\\"5789\\",\\"static/chunks/5789-d4359a1055e0aacd.js\\",\\"4454\\",\\"static/chunks/4454-c3cbd02a8687a7b6.js\\",\\"7860\\",\\"static/chunks/7860-a6cd4bd13b0a2513.js\\",\\"1285\\",\\"static/chunks/app/(console)/docs/(mdx-pages)/rate-limits/page-b0be08a4163e38e4.js\\"\],\\"CopyableDocsTable\\"\]\\n"\])self.\_\_next\_f.push(\[1,"c:\[\\"$\\",\\"div\\",null,{\\"children\\":\[\\"$\\",\\"$L1f\\",null,{\\"selectedContents\\":\[\\"free\\",\\"developer\\"\],\\"className\\":\\"py-4\\",\\"children\\":\[\[\\"$\\",\\"$L20\\",null,{\\"value\\":\\"free\\",\\"children\\":\[\\"$\\",\\"$L21\\",null,{\\"headers\\":\[{\\"title\\":\\"MODEL ID\\",\\"className\\":\\"min-w-\[300px\]\\"},\\"RPM\\",\\"RPD\\",\\"TPM\\",\\"TPD\\",\\"ASH\\",\\"ASD\\"\],\\"rows\\":\[\[\\"allam-2-7b\\",\\"30\\",\\"7K\\",\\"6K\\",\\"500K\\",\\"-\\",\\"-\\"\],\[\\"groq/compound\\",\\"30\\",\\"250\\",\\"70K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"groq/compound-mini\\",\\"30\\",\\"250\\",\\"70K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"llama-3.1-8b-instant\\",\\"30\\",\\"14.4K\\",\\"6K\\",\\"500K\\",\\"-\\",\\"-\\"\],\[\\"llama-3.3-70b-versatile\\",\\"30\\",\\"1K\\",\\"12K\\",\\"100K\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-4-maverick-17b-128e-instruct\\",\\"30\\",\\"1K\\",\\"6K\\",\\"500K\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-4-scout-17b-16e-instruct\\",\\"30\\",\\"1K\\",\\"30K\\",\\"500K\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-guard-4-12b\\",\\"30\\",\\"14.4K\\",\\"15K\\",\\"500K\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-prompt-guard-2-22m\\",\\"30\\",\\"14.4K\\",\\"15K\\",\\"500K\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-prompt-guard-2-86m\\",\\"30\\",\\"14.4K\\",\\"15K\\",\\"500K\\",\\"-\\",\\"-\\"\],\[\\"moonshotai/kimi-k2-instruct\\",\\"60\\",\\"1K\\",\\"10K\\",\\"300K\\",\\"-\\",\\"-\\"\],\[\\"moonshotai/kimi-k2-instruct-0905\\",\\"60\\",\\"1K\\",\\"10K\\",\\"300K\\",\\"-\\",\\"-\\"\],\[\\"openai/gpt-oss-120b\\",\\"30\\",\\"1K\\",\\"8K\\",\\"200K\\",\\"-\\",\\"-\\"\],\[\\"openai/gpt-oss-20b\\",\\"30\\",\\"1K\\",\\"8K\\",\\"200K\\",\\"-\\",\\"-\\"\],\[\\"openai/gpt-oss-safeguard-20b\\",\\"30\\",\\"1K\\",\\"8K\\",\\"200K\\",\\"-\\",\\"-\\"\],\[\\"playai-tts\\",\\"10\\",\\"100\\",\\"1.2K\\",\\"3.6K\\",\\"-\\",\\"-\\"\],\[\\"playai-tts-arabic\\",\\"10\\",\\"100\\",\\"1.2K\\",\\"3.6K\\",\\"-\\",\\"-\\"\],\[\\"qwen/qwen3-32b\\",\\"60\\",\\"1K\\",\\"6K\\",\\"500K\\",\\"-\\",\\"-\\"\],\[\\"whisper-large-v3\\",\\"20\\",\\"2K\\",\\"-\\",\\"-\\",\\"7.2K\\",\\"28.8K\\"\],\[\\"whisper-large-v3-turbo\\",\\"20\\",\\"2K\\",\\"-\\",\\"-\\",\\"7.2K\\",\\"28.8K\\"\]\],\\"copyColumnIndex\\":0}\]}\],\[\\"$\\",\\"$L20\\",null,{\\"value\\":\\"developer\\",\\"children\\":\[\\"$\\",\\"$L21\\",null,{\\"headers\\":\\"$c:props:children:props:children:0:props:children:props:headers\\",\\"rows\\":\[\[\\"allam-2-7b\\",\\"300\\",\\"60K\\",\\"60K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"groq/compound\\",\\"200\\",\\"20K\\",\\"200K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"groq/compound-mini\\",\\"200\\",\\"20K\\",\\"200K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"llama-3.1-8b-instant\\",\\"1K\\",\\"500K\\",\\"250K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"llama-3.3-70b-versatile\\",\\"1K\\",\\"500K\\",\\"300K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-4-maverick-17b-128e-instruct\\",\\"1K\\",\\"500K\\",\\"300K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-4-scout-17b-16e-instruct\\",\\"1K\\",\\"500K\\",\\"300K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-guard-4-12b\\",\\"100\\",\\"50K\\",\\"30K\\",\\"1M\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-prompt-guard-2-22m\\",\\"100\\",\\"50K\\",\\"30K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"meta-llama/llama-prompt-guard-2-86m\\",\\"100\\",\\"50K\\",\\"30K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"moonshotai/kimi-k2-instruct\\",\\"1K\\",\\"500K\\",\\"250K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"moonshotai/kimi-k2-instruct-0905\\",\\"1K\\",\\"500K\\",\\"250K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"openai/gpt-oss-120b\\",\\"1K\\",\\"500K\\",\\"250K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"openai/gpt-oss-20b\\",\\"1K\\",\\"500K\\",\\"250K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"openai/gpt-oss-safeguard-20b\\",\\"1K\\",\\"500K\\",\\"150K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"playai-tts\\",\\"250\\",\\"100K\\",\\"50K\\",\\"2M\\",\\"-\\",\\"-\\"\],\[\\"playai-tts-arabic\\",\\"250\\",\\"100K\\",\\"50K\\",\\"2M\\",\\"-\\",\\"-\\"\],\[\\"qwen/qwen3-32b\\",\\"1K\\",\\"500K\\",\\"300K\\",\\"-\\",\\"-\\",\\"-\\"\],\[\\"whisper-large-v3\\",\\"300\\",\\"200K\\",\\"-\\",\\"-\\",\\"200K\\",\\"4M\\"\],\[\\"whisper-large-v3-turbo\\",\\"400\\",\\"200K\\",\\"-\\",\\"-\\",\\"400K\\",\\"4M\\"\]\],\\"copyColumnIndex\\":0}\]}\]\]}\]}\]\\n"\])self.\_\_next\_f.push(\[1,"22:I\[57863,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"7487\\",\\"static/chunks/b3b06311-fa77c49096c4cc97.js\\",\\"7539\\",\\"static/chunks/1f447996-8974f1b7e13f2cb9.js\\",\\"8455\\",\\"static/chunks/d611311c-7b5735a4c92c90d9.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"7213\\",\\"static/chunks/7213-443f51519115da15.js\\",\\"7177\\",\\"static/chunks/app/layout-a8184e653bbd672b.js\\"\],\\"PostHogClientProvider\\"\]\\n23:I\[82615,\[\\"8779\\",\\"static/chunks/8285d696-4c2af9547952b91b.js\\",\\"7487\\",\\"static/chunks/b3b06311-fa77c49096c4cc97.js\\",\\"7539\\",\\"static/chunks/1f447996-8974f1b7e13f2cb9.js\\",\\"8455\\",\\"static/chunks/d611311c-7b5735a4c92c90d9.js\\",\\"3596\\",\\"static/chunks/3596-81e8162c1dd9940f.js\\",\\"888\\",\\"static/chunks/888-7cd7f38e76441637.js\\",\\"1466\\",\\"static/chunks/1466-ba3c67344a908bbe.js\\",\\"1727\\",\\"static/chunks/1727-8cb7c763d81e3223.js\\",\\"7213\\",\\"static/chunks/7213-443f51519115da15.js\\",\\"7177\\",\\"static/chunks/app/layout-a8184e653bbd672b.js\\"\],\\"ClientProviders\\"\]\\n"\])self.\_\_next\_f.push(\[1,"17:\[\\"$\\",\\"$L22\\",null,{\\"flags\\":{\\"survey-targeting-0ac5ca675d-custom\\":true,\\"home-hero-usage-redesign\\":\\"test\\",\\"model-limits\\":true,\\"cards-ordering\\":\\"test\\",\\"survey-targeting-ac30499a63\\":false,\\"billing-improved-upgrade-message\\":\\"test\\",\\"survey-targeting-7869e3894d-custom\\":true,\\"ff-enable-sso-selfserve\\":true,\\"billing\_limits\\":true,\\"server-side-mcp-preview\\":false,\\"projects\\":true,\\"survey-targeting-e28db87db8-custom\\":true,\\"top\_loader\\":false,\\"ff-ssrl-enabled\\":false,\\"orion-remote-config\\":true,\\"function-calling\\":false,\\"data-controls-chat-completions\\":true,\\"ff-invoice-retry\\":true,\\"saved-prompts\\":false,\\"survey-targeting-956d3e0f6c-custom\\":true,\\"abab-testing\\":true,\\"home-cards\\":\\"control\\",\\"data-controls\\":true,\\"ff-chat-scroll-trap\\":true},\\"userIdentity\\":{\\"id\\":\\"8f4f19d8-ff95-4970-b537-324d9fe7f8dc\\",\\"name\\":\\"$undefined\\",\\"email\\":\\"$undefined\\",\\"isIdentified\\":false},\\"children\\":\[\\"$\\",\\"$L23\\",null,{\\"layoutPreferences\\":{},\\"children\\":\[\\"$\\",\\"$L5\\",null,{\\"parallelRouterKey\\":\\"children\\",\\"error\\":\\"$undefined\\",\\"errorStyles\\":\\"$undefined\\",\\"errorScripts\\":\\"$undefined\\",\\"template\\":\[\\"$\\",\\"$L6\\",null,{}\],\\"templateStyles\\":\\"$undefined\\",\\"templateScripts\\":\\"$undefined\\",\\"notFound\\":\[\[\[\\"$\\",\\"title\\",null,{\\"children\\":\\"404: This page could not be found.\\"}\],\[\\"$\\",\\"div\\",null,{\\"style\\":\\"$0:f:0:1:2:children:1:props:children:1:props:slots:children:props:notFound:0:1:props:style\\",\\"children\\":\[\\"$\\",\\"div\\",null,{\\"children\\":\[\[\\"$\\",\\"style\\",null,{\\"dangerouslySetInnerHTML\\":{\\"\_\_html\\":\\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\\"}}\],\[\\"$\\",\\"h1\\",null,{\\"className\\":\\"next-error-h1\\",\\"style\\":\\"$0:f:0:1:2:children:1:props:children:1:props:slots:children:props:notFound:0:1:props:children:props:children:1:props:style\\",\\"children\\":404}\],\[\\"$\\",\\"div\\",null,{\\"style\\":\\"$0:f:0:1:2:children:1:props:children:1:props:slots:children:props:notFound:0:1:props:children:props:children:2:props:style\\",\\"children\\":\[\\"$\\",\\"h2\\",null,{\\"style\\":\\"$0:f:0:1:2:children:1:props:children:1:props:slots:children:props:notFound:0:1:props:children:props:children:2:props:children:props:style\\",\\"children\\":\\"This page could not be found.\\"}\]}\]\]}\]}\]\],\[\]\],\\"forbidden\\":\\"$undefined\\",\\"unauthorized\\":\\"$undefined\\"}\]}\]}\]\\n"\]) window\['dataLayer'\] = window\['dataLayer'\] || \[\]; function gtag(){window\['dataLayer'\].push(arguments);} gtag('js', new Date()); gtag('config', 'G-CQ9K0VPEEQ'); (function(w,l){ w\[l\]=w\[l\]||\[\]; w\[l\].push({'gtm.start': new Date().getTime(),event:'gtm.js'}); })(window,'dataLayer');